2024-01-03 00:37:41,309 - INFO - Log directory: ./libcity/log
2024-01-03 00:37:41,309 - INFO - Begin pretrain-pipeline, task=trajectory_embedding, model_name=LinearNextLoc, dataset_name=bj, exp_id=401518
2024-01-03 00:37:41,309 - INFO - {'task': 'trajectory_embedding', 'model': 'LinearNextLoc', 'dataset': 'bj', 'saved_model': True, 'train': True, 'gpu': True, 'gpu_id': 0, 'pretrain_path': 'libcity/cache/454824/model_cache/454824_BERTContrastiveLM_bj.pt', 'mlm_ratio': 0.6, 'n_layers': 6, 'd_model': 256, 'attn_heads': 8, 'max_epoch': 30, 'batch_size': 8, 'grad_accmu_steps': 1, 'learning_rate': 0.0002, 'roadnetwork': 'bj_roadmap_edge_bj_True_1_merge', 'geo_file': 'bj_roadmap_edge_bj_True_1_merge_withdegree', 'rel_file': 'bj_roadmap_edge_bj_True_1_merge_withdegree', 'merge': True, 'min_freq': 1, 'seq_len': 128, 'test_every': 50, 'temperature': 0.05, 'contra_loss_type': 'simclr', 'classify_label': 'vflag', 'type_ln': 'post', 'add_cls': True, 'add_time_in_day': True, 'add_day_in_week': True, 'add_pe': True, 'add_temporal_bias': True, 'temporal_bias_dim': 64, 'use_mins_interval': False, 'add_gat': True, 'gat_heads_per_layer': [8, 16, 1], 'gat_features_per_layer': [16, 16, 256], 'gat_dropout': 0.1, 'gat_K': 1, 'gat_avg_last': True, 'load_trans_prob': True, 'append_degree2gcn': True, 'normal_feature': False, 'pooling': 'cls', 'dataset_class': 'NextLocDataset', 'executor': 'NextLocExecutor', 'evaluator': 'RegressionEvaluator', 'num_workers': 0, 'vocab_path': None, 'mlp_ratio': 4, 'pretrain_road_emb': None, 'future_mask': False, 'load_node2vec': False, 'dropout': 0.1, 'drop_path': 0.3, 'attn_drop': 0.1, 'seed': 0, 'learner': 'adamw', 'lr_eta_min': 0, 'lr_warmup_epoch': 4, 'lr_warmup_init': 1e-06, 'lr_decay': True, 'lr_scheduler': 'cosinelr', 'lr_decay_ratio': 0.1, 't_in_epochs': True, 'clip_grad_norm': True, 'max_grad_norm': 5, 'use_early_stop': True, 'patience': 10, 'log_batch': 500, 'log_every': 1, 'l2_reg': None, 'bidir_adj_mx': False, 'freeze': False, 'metrics': ['MAE', 'RMSE', 'MAPE', 'R2', 'EVAR'], 'save_modes': ['csv', 'json'], 'device': device(type='cuda', index=0), 'exp_id': 401518}
2024-01-03 00:37:41,842 - INFO - Loading Vocab from raw_data/vocab_bj_True_1_merge.pkl
2024-01-03 00:37:41,849 - INFO - vocab_path=raw_data/vocab_bj_True_1_merge.pkl, usr_num=75, vocab_size=27485
2024-01-03 00:37:41,902 - INFO - Loaded file bj_roadmap_edge_bj_True_1_merge_withdegree.geo, num_nodes=27481
2024-01-03 00:37:43,282 - INFO - Loaded file bj_roadmap_edge_bj_True_1_merge_withdegree.rel, shape=(27481, 27481), edges=51763.0
2024-01-03 00:37:43,287 - INFO - node_features: (27481, 42)
2024-01-03 00:37:44,273 - INFO - node_features_encoded: torch.Size([27485, 42])
2024-01-03 00:37:44,426 - INFO - edge_index: torch.Size([2, 79209])
2024-01-03 00:37:44,430 - INFO - Trajectory loc-transfer prob shape=torch.Size([79209, 1])
2024-01-03 00:37:44,441 - INFO - Loading Dataset!
2024-01-03 00:37:44,751 - INFO - Processing dataset in TrajectoryProcessingDataset!
2024-01-03 00:51:15,035 - INFO - Processing dataset in TrajectoryProcessingDataset!
2024-01-03 00:55:44,661 - INFO - Processing dataset in TrajectoryProcessingDataset!
2024-01-03 00:57:38,590 - INFO - Size of dataset: 13138/4380/1959
2024-01-03 00:57:38,591 - INFO - Creating Dataloader!
2024-01-03 00:57:38,593 - INFO - Building Downstream LinearNextLoc model
2024-01-03 00:57:38,593 - INFO - Building BERTDownstream model
2024-01-03 00:57:39,529 - INFO - LinearNextLoc(
  (model): BERTDownstream(
    (bert): BERT(
      (embedding): BERTEmbedding(
        (token_embedding): GAT(
          (gat_net): Sequential(
            (0): GATLayerImp3(
              (linear_proj): Linear(in_features=42, out_features=128, bias=False)
              (linear_proj_tran_prob): Linear(in_features=1, out_features=128, bias=False)
              (skip_proj): Linear(in_features=42, out_features=128, bias=False)
              (leakyReLU): LeakyReLU(negative_slope=0.2)
              (softmax): Softmax(dim=-1)
              (activation): ELU(alpha=1.0)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (1): GATLayerImp3(
              (linear_proj): Linear(in_features=128, out_features=256, bias=False)
              (linear_proj_tran_prob): Linear(in_features=1, out_features=256, bias=False)
              (skip_proj): Linear(in_features=128, out_features=256, bias=False)
              (leakyReLU): LeakyReLU(negative_slope=0.2)
              (softmax): Softmax(dim=-1)
              (activation): ELU(alpha=1.0)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (2): GATLayerImp3(
              (linear_proj): Linear(in_features=256, out_features=256, bias=False)
              (linear_proj_tran_prob): Linear(in_features=1, out_features=256, bias=False)
              (skip_proj): Linear(in_features=256, out_features=256, bias=False)
              (leakyReLU): LeakyReLU(negative_slope=0.2)
              (softmax): Softmax(dim=-1)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
        )
        (position_embedding): PositionalEmbedding()
        (daytime_embedding): Embedding(1441, 256, padding_idx=0)
        (weekday_embedding): Embedding(8, 256, padding_idx=0)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (transformer_blocks): ModuleList(
        (0): TransformerBlock(
          (attention): MultiHeadedAttention(
            (linear_layers): ModuleList(
              (0): Linear(in_features=256, out_features=256, bias=True)
              (1): Linear(in_features=256, out_features=256, bias=True)
              (2): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout): Dropout(p=0.1, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0.1, inplace=False)
            (temporal_mat_bias_1): Linear(in_features=1, out_features=64, bias=True)
            (temporal_mat_bias_2): Linear(in_features=64, out_features=1, bias=True)
          )
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0.1, inplace=False)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (drop_path): Identity()
        )
        (1): TransformerBlock(
          (attention): MultiHeadedAttention(
            (linear_layers): ModuleList(
              (0): Linear(in_features=256, out_features=256, bias=True)
              (1): Linear(in_features=256, out_features=256, bias=True)
              (2): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout): Dropout(p=0.1, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0.1, inplace=False)
            (temporal_mat_bias_1): Linear(in_features=1, out_features=64, bias=True)
            (temporal_mat_bias_2): Linear(in_features=64, out_features=1, bias=True)
          )
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0.1, inplace=False)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (drop_path): DropPath()
        )
        (2): TransformerBlock(
          (attention): MultiHeadedAttention(
            (linear_layers): ModuleList(
              (0): Linear(in_features=256, out_features=256, bias=True)
              (1): Linear(in_features=256, out_features=256, bias=True)
              (2): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout): Dropout(p=0.1, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0.1, inplace=False)
            (temporal_mat_bias_1): Linear(in_features=1, out_features=64, bias=True)
            (temporal_mat_bias_2): Linear(in_features=64, out_features=1, bias=True)
          )
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0.1, inplace=False)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (drop_path): DropPath()
        )
        (3): TransformerBlock(
          (attention): MultiHeadedAttention(
            (linear_layers): ModuleList(
              (0): Linear(in_features=256, out_features=256, bias=True)
              (1): Linear(in_features=256, out_features=256, bias=True)
              (2): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout): Dropout(p=0.1, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0.1, inplace=False)
            (temporal_mat_bias_1): Linear(in_features=1, out_features=64, bias=True)
            (temporal_mat_bias_2): Linear(in_features=64, out_features=1, bias=True)
          )
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0.1, inplace=False)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (drop_path): DropPath()
        )
        (4): TransformerBlock(
          (attention): MultiHeadedAttention(
            (linear_layers): ModuleList(
              (0): Linear(in_features=256, out_features=256, bias=True)
              (1): Linear(in_features=256, out_features=256, bias=True)
              (2): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout): Dropout(p=0.1, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0.1, inplace=False)
            (temporal_mat_bias_1): Linear(in_features=1, out_features=64, bias=True)
            (temporal_mat_bias_2): Linear(in_features=64, out_features=1, bias=True)
          )
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0.1, inplace=False)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (drop_path): DropPath()
        )
        (5): TransformerBlock(
          (attention): MultiHeadedAttention(
            (linear_layers): ModuleList(
              (0): Linear(in_features=256, out_features=256, bias=True)
              (1): Linear(in_features=256, out_features=256, bias=True)
              (2): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout): Dropout(p=0.1, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0.1, inplace=False)
            (temporal_mat_bias_1): Linear(in_features=1, out_features=64, bias=True)
            (temporal_mat_bias_2): Linear(in_features=64, out_features=1, bias=True)
          )
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0.1, inplace=False)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (drop_path): DropPath()
        )
      )
    )
  )
  (linear): Linear(in_features=256, out_features=27485, bias=True)
  (softmax): LogSoftmax(dim=-1)
)
2024-01-03 00:57:39,532 - INFO - model.bert.embedding.token_embedding.gat_net.0.scoring_fn_target	torch.Size([1, 8, 16])	cuda:0	True
2024-01-03 00:57:39,533 - INFO - model.bert.embedding.token_embedding.gat_net.0.scoring_fn_source	torch.Size([1, 8, 16])	cuda:0	True
2024-01-03 00:57:39,533 - INFO - model.bert.embedding.token_embedding.gat_net.0.scoring_trans_prob	torch.Size([1, 8, 16])	cuda:0	True
2024-01-03 00:57:39,533 - INFO - model.bert.embedding.token_embedding.gat_net.0.bias	torch.Size([128])	cuda:0	True
2024-01-03 00:57:39,533 - INFO - model.bert.embedding.token_embedding.gat_net.0.linear_proj.weight	torch.Size([128, 42])	cuda:0	True
2024-01-03 00:57:39,533 - INFO - model.bert.embedding.token_embedding.gat_net.0.linear_proj_tran_prob.weight	torch.Size([128, 1])	cuda:0	True
2024-01-03 00:57:39,533 - INFO - model.bert.embedding.token_embedding.gat_net.0.skip_proj.weight	torch.Size([128, 42])	cuda:0	True
2024-01-03 00:57:39,533 - INFO - model.bert.embedding.token_embedding.gat_net.1.scoring_fn_target	torch.Size([1, 16, 16])	cuda:0	True
2024-01-03 00:57:39,533 - INFO - model.bert.embedding.token_embedding.gat_net.1.scoring_fn_source	torch.Size([1, 16, 16])	cuda:0	True
2024-01-03 00:57:39,534 - INFO - model.bert.embedding.token_embedding.gat_net.1.scoring_trans_prob	torch.Size([1, 16, 16])	cuda:0	True
2024-01-03 00:57:39,534 - INFO - model.bert.embedding.token_embedding.gat_net.1.bias	torch.Size([256])	cuda:0	True
2024-01-03 00:57:39,534 - INFO - model.bert.embedding.token_embedding.gat_net.1.linear_proj.weight	torch.Size([256, 128])	cuda:0	True
2024-01-03 00:57:39,534 - INFO - model.bert.embedding.token_embedding.gat_net.1.linear_proj_tran_prob.weight	torch.Size([256, 1])	cuda:0	True
2024-01-03 00:57:39,534 - INFO - model.bert.embedding.token_embedding.gat_net.1.skip_proj.weight	torch.Size([256, 128])	cuda:0	True
2024-01-03 00:57:39,534 - INFO - model.bert.embedding.token_embedding.gat_net.2.scoring_fn_target	torch.Size([1, 1, 256])	cuda:0	True
2024-01-03 00:57:39,534 - INFO - model.bert.embedding.token_embedding.gat_net.2.scoring_fn_source	torch.Size([1, 1, 256])	cuda:0	True
2024-01-03 00:57:39,534 - INFO - model.bert.embedding.token_embedding.gat_net.2.scoring_trans_prob	torch.Size([1, 1, 256])	cuda:0	True
2024-01-03 00:57:39,535 - INFO - model.bert.embedding.token_embedding.gat_net.2.bias	torch.Size([256])	cuda:0	True
2024-01-03 00:57:39,535 - INFO - model.bert.embedding.token_embedding.gat_net.2.linear_proj.weight	torch.Size([256, 256])	cuda:0	True
2024-01-03 00:57:39,535 - INFO - model.bert.embedding.token_embedding.gat_net.2.linear_proj_tran_prob.weight	torch.Size([256, 1])	cuda:0	True
2024-01-03 00:57:39,535 - INFO - model.bert.embedding.token_embedding.gat_net.2.skip_proj.weight	torch.Size([256, 256])	cuda:0	True
2024-01-03 00:57:39,535 - INFO - model.bert.embedding.daytime_embedding.weight	torch.Size([1441, 256])	cuda:0	True
2024-01-03 00:57:39,535 - INFO - model.bert.embedding.weekday_embedding.weight	torch.Size([8, 256])	cuda:0	True
2024-01-03 00:57:39,535 - INFO - model.bert.transformer_blocks.0.attention.linear_layers.0.weight	torch.Size([256, 256])	cuda:0	True
2024-01-03 00:57:39,535 - INFO - model.bert.transformer_blocks.0.attention.linear_layers.0.bias	torch.Size([256])	cuda:0	True
2024-01-03 00:57:39,536 - INFO - model.bert.transformer_blocks.0.attention.linear_layers.1.weight	torch.Size([256, 256])	cuda:0	True
2024-01-03 00:57:39,536 - INFO - model.bert.transformer_blocks.0.attention.linear_layers.1.bias	torch.Size([256])	cuda:0	True
2024-01-03 00:57:39,536 - INFO - model.bert.transformer_blocks.0.attention.linear_layers.2.weight	torch.Size([256, 256])	cuda:0	True
2024-01-03 00:57:39,536 - INFO - model.bert.transformer_blocks.0.attention.linear_layers.2.bias	torch.Size([256])	cuda:0	True
2024-01-03 00:57:39,536 - INFO - model.bert.transformer_blocks.0.attention.proj.weight	torch.Size([256, 256])	cuda:0	True
2024-01-03 00:57:39,536 - INFO - model.bert.transformer_blocks.0.attention.proj.bias	torch.Size([256])	cuda:0	True
2024-01-03 00:57:39,536 - INFO - model.bert.transformer_blocks.0.attention.temporal_mat_bias_1.weight	torch.Size([64, 1])	cuda:0	True
2024-01-03 00:57:39,536 - INFO - model.bert.transformer_blocks.0.attention.temporal_mat_bias_1.bias	torch.Size([64])	cuda:0	True
2024-01-03 00:57:39,537 - INFO - model.bert.transformer_blocks.0.attention.temporal_mat_bias_2.weight	torch.Size([1, 64])	cuda:0	True
2024-01-03 00:57:39,537 - INFO - model.bert.transformer_blocks.0.attention.temporal_mat_bias_2.bias	torch.Size([1])	cuda:0	True
2024-01-03 00:57:39,537 - INFO - model.bert.transformer_blocks.0.mlp.fc1.weight	torch.Size([1024, 256])	cuda:0	True
2024-01-03 00:57:39,537 - INFO - model.bert.transformer_blocks.0.mlp.fc1.bias	torch.Size([1024])	cuda:0	True
2024-01-03 00:57:39,537 - INFO - model.bert.transformer_blocks.0.mlp.fc2.weight	torch.Size([256, 1024])	cuda:0	True
2024-01-03 00:57:39,537 - INFO - model.bert.transformer_blocks.0.mlp.fc2.bias	torch.Size([256])	cuda:0	True
2024-01-03 00:57:39,537 - INFO - model.bert.transformer_blocks.0.norm1.weight	torch.Size([256])	cuda:0	True
2024-01-03 00:57:39,537 - INFO - model.bert.transformer_blocks.0.norm1.bias	torch.Size([256])	cuda:0	True
2024-01-03 00:57:39,538 - INFO - model.bert.transformer_blocks.0.norm2.weight	torch.Size([256])	cuda:0	True
2024-01-03 00:57:39,538 - INFO - model.bert.transformer_blocks.0.norm2.bias	torch.Size([256])	cuda:0	True
2024-01-03 00:57:39,538 - INFO - model.bert.transformer_blocks.1.attention.linear_layers.0.weight	torch.Size([256, 256])	cuda:0	True
2024-01-03 00:57:39,538 - INFO - model.bert.transformer_blocks.1.attention.linear_layers.0.bias	torch.Size([256])	cuda:0	True
2024-01-03 00:57:39,538 - INFO - model.bert.transformer_blocks.1.attention.linear_layers.1.weight	torch.Size([256, 256])	cuda:0	True
2024-01-03 00:57:39,538 - INFO - model.bert.transformer_blocks.1.attention.linear_layers.1.bias	torch.Size([256])	cuda:0	True
2024-01-03 00:57:39,538 - INFO - model.bert.transformer_blocks.1.attention.linear_layers.2.weight	torch.Size([256, 256])	cuda:0	True
2024-01-03 00:57:39,538 - INFO - model.bert.transformer_blocks.1.attention.linear_layers.2.bias	torch.Size([256])	cuda:0	True
2024-01-03 00:57:39,538 - INFO - model.bert.transformer_blocks.1.attention.proj.weight	torch.Size([256, 256])	cuda:0	True
2024-01-03 00:57:39,539 - INFO - model.bert.transformer_blocks.1.attention.proj.bias	torch.Size([256])	cuda:0	True
2024-01-03 00:57:39,539 - INFO - model.bert.transformer_blocks.1.attention.temporal_mat_bias_1.weight	torch.Size([64, 1])	cuda:0	True
2024-01-03 00:57:39,539 - INFO - model.bert.transformer_blocks.1.attention.temporal_mat_bias_1.bias	torch.Size([64])	cuda:0	True
2024-01-03 00:57:39,539 - INFO - model.bert.transformer_blocks.1.attention.temporal_mat_bias_2.weight	torch.Size([1, 64])	cuda:0	True
2024-01-03 00:57:39,539 - INFO - model.bert.transformer_blocks.1.attention.temporal_mat_bias_2.bias	torch.Size([1])	cuda:0	True
2024-01-03 00:57:39,539 - INFO - model.bert.transformer_blocks.1.mlp.fc1.weight	torch.Size([1024, 256])	cuda:0	True
2024-01-03 00:57:39,539 - INFO - model.bert.transformer_blocks.1.mlp.fc1.bias	torch.Size([1024])	cuda:0	True
2024-01-03 00:57:39,539 - INFO - model.bert.transformer_blocks.1.mlp.fc2.weight	torch.Size([256, 1024])	cuda:0	True
2024-01-03 00:57:39,539 - INFO - model.bert.transformer_blocks.1.mlp.fc2.bias	torch.Size([256])	cuda:0	True
2024-01-03 00:57:39,540 - INFO - model.bert.transformer_blocks.1.norm1.weight	torch.Size([256])	cuda:0	True
2024-01-03 00:57:39,540 - INFO - model.bert.transformer_blocks.1.norm1.bias	torch.Size([256])	cuda:0	True
2024-01-03 00:57:39,540 - INFO - model.bert.transformer_blocks.1.norm2.weight	torch.Size([256])	cuda:0	True
2024-01-03 00:57:39,540 - INFO - model.bert.transformer_blocks.1.norm2.bias	torch.Size([256])	cuda:0	True
2024-01-03 00:57:39,540 - INFO - model.bert.transformer_blocks.2.attention.linear_layers.0.weight	torch.Size([256, 256])	cuda:0	True
2024-01-03 00:57:39,540 - INFO - model.bert.transformer_blocks.2.attention.linear_layers.0.bias	torch.Size([256])	cuda:0	True
2024-01-03 00:57:39,540 - INFO - model.bert.transformer_blocks.2.attention.linear_layers.1.weight	torch.Size([256, 256])	cuda:0	True
2024-01-03 00:57:39,541 - INFO - model.bert.transformer_blocks.2.attention.linear_layers.1.bias	torch.Size([256])	cuda:0	True
2024-01-03 00:57:39,541 - INFO - model.bert.transformer_blocks.2.attention.linear_layers.2.weight	torch.Size([256, 256])	cuda:0	True
2024-01-03 00:57:39,541 - INFO - model.bert.transformer_blocks.2.attention.linear_layers.2.bias	torch.Size([256])	cuda:0	True
2024-01-03 00:57:39,541 - INFO - model.bert.transformer_blocks.2.attention.proj.weight	torch.Size([256, 256])	cuda:0	True
2024-01-03 00:57:39,541 - INFO - model.bert.transformer_blocks.2.attention.proj.bias	torch.Size([256])	cuda:0	True
2024-01-03 00:57:39,541 - INFO - model.bert.transformer_blocks.2.attention.temporal_mat_bias_1.weight	torch.Size([64, 1])	cuda:0	True
2024-01-03 00:57:39,541 - INFO - model.bert.transformer_blocks.2.attention.temporal_mat_bias_1.bias	torch.Size([64])	cuda:0	True
2024-01-03 00:57:39,542 - INFO - model.bert.transformer_blocks.2.attention.temporal_mat_bias_2.weight	torch.Size([1, 64])	cuda:0	True
2024-01-03 00:57:39,542 - INFO - model.bert.transformer_blocks.2.attention.temporal_mat_bias_2.bias	torch.Size([1])	cuda:0	True
2024-01-03 00:57:39,542 - INFO - model.bert.transformer_blocks.2.mlp.fc1.weight	torch.Size([1024, 256])	cuda:0	True
2024-01-03 00:57:39,542 - INFO - model.bert.transformer_blocks.2.mlp.fc1.bias	torch.Size([1024])	cuda:0	True
2024-01-03 00:57:39,542 - INFO - model.bert.transformer_blocks.2.mlp.fc2.weight	torch.Size([256, 1024])	cuda:0	True
2024-01-03 00:57:39,542 - INFO - model.bert.transformer_blocks.2.mlp.fc2.bias	torch.Size([256])	cuda:0	True
2024-01-03 00:57:39,542 - INFO - model.bert.transformer_blocks.2.norm1.weight	torch.Size([256])	cuda:0	True
2024-01-03 00:57:39,542 - INFO - model.bert.transformer_blocks.2.norm1.bias	torch.Size([256])	cuda:0	True
2024-01-03 00:57:39,542 - INFO - model.bert.transformer_blocks.2.norm2.weight	torch.Size([256])	cuda:0	True
2024-01-03 00:57:39,543 - INFO - model.bert.transformer_blocks.2.norm2.bias	torch.Size([256])	cuda:0	True
2024-01-03 00:57:39,543 - INFO - model.bert.transformer_blocks.3.attention.linear_layers.0.weight	torch.Size([256, 256])	cuda:0	True
2024-01-03 00:57:39,543 - INFO - model.bert.transformer_blocks.3.attention.linear_layers.0.bias	torch.Size([256])	cuda:0	True
2024-01-03 00:57:39,543 - INFO - model.bert.transformer_blocks.3.attention.linear_layers.1.weight	torch.Size([256, 256])	cuda:0	True
2024-01-03 00:57:39,543 - INFO - model.bert.transformer_blocks.3.attention.linear_layers.1.bias	torch.Size([256])	cuda:0	True
2024-01-03 00:57:39,543 - INFO - model.bert.transformer_blocks.3.attention.linear_layers.2.weight	torch.Size([256, 256])	cuda:0	True
2024-01-03 00:57:39,543 - INFO - model.bert.transformer_blocks.3.attention.linear_layers.2.bias	torch.Size([256])	cuda:0	True
2024-01-03 00:57:39,543 - INFO - model.bert.transformer_blocks.3.attention.proj.weight	torch.Size([256, 256])	cuda:0	True
2024-01-03 00:57:39,544 - INFO - model.bert.transformer_blocks.3.attention.proj.bias	torch.Size([256])	cuda:0	True
2024-01-03 00:57:39,544 - INFO - model.bert.transformer_blocks.3.attention.temporal_mat_bias_1.weight	torch.Size([64, 1])	cuda:0	True
2024-01-03 00:57:39,544 - INFO - model.bert.transformer_blocks.3.attention.temporal_mat_bias_1.bias	torch.Size([64])	cuda:0	True
2024-01-03 00:57:39,544 - INFO - model.bert.transformer_blocks.3.attention.temporal_mat_bias_2.weight	torch.Size([1, 64])	cuda:0	True
2024-01-03 00:57:39,544 - INFO - model.bert.transformer_blocks.3.attention.temporal_mat_bias_2.bias	torch.Size([1])	cuda:0	True
2024-01-03 00:57:39,544 - INFO - model.bert.transformer_blocks.3.mlp.fc1.weight	torch.Size([1024, 256])	cuda:0	True
2024-01-03 00:57:39,544 - INFO - model.bert.transformer_blocks.3.mlp.fc1.bias	torch.Size([1024])	cuda:0	True
2024-01-03 00:57:39,544 - INFO - model.bert.transformer_blocks.3.mlp.fc2.weight	torch.Size([256, 1024])	cuda:0	True
2024-01-03 00:57:39,545 - INFO - model.bert.transformer_blocks.3.mlp.fc2.bias	torch.Size([256])	cuda:0	True
2024-01-03 00:57:39,545 - INFO - model.bert.transformer_blocks.3.norm1.weight	torch.Size([256])	cuda:0	True
2024-01-03 00:57:39,545 - INFO - model.bert.transformer_blocks.3.norm1.bias	torch.Size([256])	cuda:0	True
2024-01-03 00:57:39,545 - INFO - model.bert.transformer_blocks.3.norm2.weight	torch.Size([256])	cuda:0	True
2024-01-03 00:57:39,545 - INFO - model.bert.transformer_blocks.3.norm2.bias	torch.Size([256])	cuda:0	True
2024-01-03 00:57:39,545 - INFO - model.bert.transformer_blocks.4.attention.linear_layers.0.weight	torch.Size([256, 256])	cuda:0	True
2024-01-03 00:57:39,545 - INFO - model.bert.transformer_blocks.4.attention.linear_layers.0.bias	torch.Size([256])	cuda:0	True
2024-01-03 00:57:39,545 - INFO - model.bert.transformer_blocks.4.attention.linear_layers.1.weight	torch.Size([256, 256])	cuda:0	True
2024-01-03 00:57:39,545 - INFO - model.bert.transformer_blocks.4.attention.linear_layers.1.bias	torch.Size([256])	cuda:0	True
2024-01-03 00:57:39,546 - INFO - model.bert.transformer_blocks.4.attention.linear_layers.2.weight	torch.Size([256, 256])	cuda:0	True
2024-01-03 00:57:39,546 - INFO - model.bert.transformer_blocks.4.attention.linear_layers.2.bias	torch.Size([256])	cuda:0	True
2024-01-03 00:57:39,546 - INFO - model.bert.transformer_blocks.4.attention.proj.weight	torch.Size([256, 256])	cuda:0	True
2024-01-03 00:57:39,546 - INFO - model.bert.transformer_blocks.4.attention.proj.bias	torch.Size([256])	cuda:0	True
2024-01-03 00:57:39,546 - INFO - model.bert.transformer_blocks.4.attention.temporal_mat_bias_1.weight	torch.Size([64, 1])	cuda:0	True
2024-01-03 00:57:39,546 - INFO - model.bert.transformer_blocks.4.attention.temporal_mat_bias_1.bias	torch.Size([64])	cuda:0	True
2024-01-03 00:57:39,546 - INFO - model.bert.transformer_blocks.4.attention.temporal_mat_bias_2.weight	torch.Size([1, 64])	cuda:0	True
2024-01-03 00:57:39,547 - INFO - model.bert.transformer_blocks.4.attention.temporal_mat_bias_2.bias	torch.Size([1])	cuda:0	True
2024-01-03 00:57:39,547 - INFO - model.bert.transformer_blocks.4.mlp.fc1.weight	torch.Size([1024, 256])	cuda:0	True
2024-01-03 00:57:39,547 - INFO - model.bert.transformer_blocks.4.mlp.fc1.bias	torch.Size([1024])	cuda:0	True
2024-01-03 00:57:39,547 - INFO - model.bert.transformer_blocks.4.mlp.fc2.weight	torch.Size([256, 1024])	cuda:0	True
2024-01-03 00:57:39,547 - INFO - model.bert.transformer_blocks.4.mlp.fc2.bias	torch.Size([256])	cuda:0	True
2024-01-03 00:57:39,547 - INFO - model.bert.transformer_blocks.4.norm1.weight	torch.Size([256])	cuda:0	True
2024-01-03 00:57:39,547 - INFO - model.bert.transformer_blocks.4.norm1.bias	torch.Size([256])	cuda:0	True
2024-01-03 00:57:39,547 - INFO - model.bert.transformer_blocks.4.norm2.weight	torch.Size([256])	cuda:0	True
2024-01-03 00:57:39,547 - INFO - model.bert.transformer_blocks.4.norm2.bias	torch.Size([256])	cuda:0	True
2024-01-03 00:57:39,548 - INFO - model.bert.transformer_blocks.5.attention.linear_layers.0.weight	torch.Size([256, 256])	cuda:0	True
2024-01-03 00:57:39,548 - INFO - model.bert.transformer_blocks.5.attention.linear_layers.0.bias	torch.Size([256])	cuda:0	True
2024-01-03 00:57:39,548 - INFO - model.bert.transformer_blocks.5.attention.linear_layers.1.weight	torch.Size([256, 256])	cuda:0	True
2024-01-03 00:57:39,548 - INFO - model.bert.transformer_blocks.5.attention.linear_layers.1.bias	torch.Size([256])	cuda:0	True
2024-01-03 00:57:39,548 - INFO - model.bert.transformer_blocks.5.attention.linear_layers.2.weight	torch.Size([256, 256])	cuda:0	True
2024-01-03 00:57:39,548 - INFO - model.bert.transformer_blocks.5.attention.linear_layers.2.bias	torch.Size([256])	cuda:0	True
2024-01-03 00:57:39,548 - INFO - model.bert.transformer_blocks.5.attention.proj.weight	torch.Size([256, 256])	cuda:0	True
2024-01-03 00:57:39,548 - INFO - model.bert.transformer_blocks.5.attention.proj.bias	torch.Size([256])	cuda:0	True
2024-01-03 00:57:39,549 - INFO - model.bert.transformer_blocks.5.attention.temporal_mat_bias_1.weight	torch.Size([64, 1])	cuda:0	True
2024-01-03 00:57:39,549 - INFO - model.bert.transformer_blocks.5.attention.temporal_mat_bias_1.bias	torch.Size([64])	cuda:0	True
2024-01-03 00:57:39,549 - INFO - model.bert.transformer_blocks.5.attention.temporal_mat_bias_2.weight	torch.Size([1, 64])	cuda:0	True
2024-01-03 00:57:39,549 - INFO - model.bert.transformer_blocks.5.attention.temporal_mat_bias_2.bias	torch.Size([1])	cuda:0	True
2024-01-03 00:57:39,549 - INFO - model.bert.transformer_blocks.5.mlp.fc1.weight	torch.Size([1024, 256])	cuda:0	True
2024-01-03 00:57:39,549 - INFO - model.bert.transformer_blocks.5.mlp.fc1.bias	torch.Size([1024])	cuda:0	True
2024-01-03 00:57:39,549 - INFO - model.bert.transformer_blocks.5.mlp.fc2.weight	torch.Size([256, 1024])	cuda:0	True
2024-01-03 00:57:39,549 - INFO - model.bert.transformer_blocks.5.mlp.fc2.bias	torch.Size([256])	cuda:0	True
2024-01-03 00:57:39,549 - INFO - model.bert.transformer_blocks.5.norm1.weight	torch.Size([256])	cuda:0	True
2024-01-03 00:57:39,550 - INFO - model.bert.transformer_blocks.5.norm1.bias	torch.Size([256])	cuda:0	True
2024-01-03 00:57:39,550 - INFO - model.bert.transformer_blocks.5.norm2.weight	torch.Size([256])	cuda:0	True
2024-01-03 00:57:39,550 - INFO - model.bert.transformer_blocks.5.norm2.bias	torch.Size([256])	cuda:0	True
2024-01-03 00:57:39,550 - INFO - linear.weight	torch.Size([27485, 256])	cuda:0	True
2024-01-03 00:57:39,550 - INFO - linear.bias	torch.Size([27485])	cuda:0	True
2024-01-03 00:57:39,551 - INFO - Total parameter numbers: 12384867
2024-01-03 00:57:39,551 - INFO - You select `adamw` optimizer.
2024-01-03 00:57:39,552 - INFO - You select `cosinelr` lr_scheduler.
2024-01-03 00:57:39,717 - INFO - Load Pretrained-Model from libcity/cache/454824/model_cache/454824_BERTContrastiveLM_bj.pt
2024-01-03 00:57:39,759 - INFO - Start training ...
2024-01-03 00:57:39,760 - INFO - Num_batches: train=1643, eval=548
2024-01-03 00:57:41,305 - INFO - {'mode': 'Train', 'epoch': 0, 'iter': 0, 'lr': 1e-06, 'loss': 10.393874168395996, 'acc(%)': 0.0}
2024-01-03 00:58:29,584 - INFO - {'mode': 'Train', 'epoch': 0, 'iter': 500, 'lr': 1e-06, 'loss': 11.068001747131348, 'acc(%)': 0.0}
2024-01-03 00:59:18,080 - INFO - {'mode': 'Train', 'epoch': 0, 'iter': 1000, 'lr': 1e-06, 'loss': 10.293131828308105, 'acc(%)': 0.012487512487512488}
2024-01-03 01:00:06,721 - INFO - {'mode': 'Train', 'epoch': 0, 'iter': 1500, 'lr': 1e-06, 'loss': 11.130398750305176, 'acc(%)': 0.024983344437041973}
2024-01-03 01:00:20,530 - INFO - Train: expid = 401518, Epoch = 0, avg_loss = 10.880446034636304, total_acc = 0.030446034404018874%.
2024-01-03 01:00:20,532 - INFO - epoch complete!
2024-01-03 01:00:20,532 - INFO - evaluating now!
2024-01-03 01:00:20,563 - INFO - {'mode': 'Eval', 'epoch': 0, 'iter': 0, 'lr': 1e-06, 'loss': 10.526867866516113, 'acc(%)': 0.0}
2024-01-03 01:00:34,658 - INFO - {'mode': 'Eval', 'epoch': 0, 'iter': 500, 'lr': 1e-06, 'loss': 10.942462921142578, 'acc(%)': 0.049900199600798396}
2024-01-03 01:00:35,987 - INFO - Eval: expid = 401518, Epoch = 0, avg_loss = 10.794947621593737, total_acc = 0.045662100456621%.
2024-01-03 01:00:35,988 - INFO - Epoch [0/30] (1643)  train_loss: 10.8804, train_acc: 0.03%, val_loss: 10.7949, val_acc: 0.05%, lr: 0.000051, 176.23s
2024-01-03 01:00:36,353 - INFO - Saved model at 0
2024-01-03 01:00:36,353 - INFO - Val loss decrease from inf to 10.7949, saving to ./libcity/cache/401518/model_cache/LinearNextLoc_bj_epoch0.tar
2024-01-03 01:00:36,452 - INFO - {'mode': 'Train', 'epoch': 1, 'iter': 0, 'lr': 5.075e-05, 'loss': 10.623636245727539, 'acc(%)': 0.0}
2024-01-03 01:01:25,754 - INFO - {'mode': 'Train', 'epoch': 1, 'iter': 500, 'lr': 5.075e-05, 'loss': 8.44314956665039, 'acc(%)': 2.944111776447106}
2024-01-03 01:02:15,548 - INFO - {'mode': 'Train', 'epoch': 1, 'iter': 1000, 'lr': 5.075e-05, 'loss': 8.814558982849121, 'acc(%)': 4.657842157842158}
2024-01-03 01:03:05,578 - INFO - {'mode': 'Train', 'epoch': 1, 'iter': 1500, 'lr': 5.075e-05, 'loss': 8.804141998291016, 'acc(%)': 5.213191205862758}
2024-01-03 01:03:19,857 - INFO - Train: expid = 401518, Epoch = 1, avg_loss = 8.903275498009686, total_acc = 5.404171106713351%.
2024-01-03 01:03:19,858 - INFO - epoch complete!
2024-01-03 01:03:19,858 - INFO - evaluating now!
2024-01-03 01:03:19,889 - INFO - {'mode': 'Eval', 'epoch': 1, 'iter': 0, 'lr': 5.075e-05, 'loss': 8.464033126831055, 'acc(%)': 12.5}
2024-01-03 01:03:34,429 - INFO - {'mode': 'Eval', 'epoch': 1, 'iter': 500, 'lr': 5.075e-05, 'loss': 11.02801513671875, 'acc(%)': 6.362275449101797}
2024-01-03 01:03:35,788 - INFO - Eval: expid = 401518, Epoch = 1, avg_loss = 9.565762259426727, total_acc = 6.438356164383562%.
2024-01-03 01:03:35,788 - INFO - Epoch [1/30] (3286)  train_loss: 8.9033, train_acc: 5.40%, val_loss: 9.5658, val_acc: 6.44%, lr: 0.000101, 179.44s
2024-01-03 01:03:36,145 - INFO - Saved model at 1
2024-01-03 01:03:36,145 - INFO - Val loss decrease from 10.7949 to 9.5658, saving to ./libcity/cache/401518/model_cache/LinearNextLoc_bj_epoch1.tar
2024-01-03 01:03:36,247 - INFO - {'mode': 'Train', 'epoch': 2, 'iter': 0, 'lr': 0.0001005, 'loss': 8.861778259277344, 'acc(%)': 0.0}
2024-01-03 01:04:26,995 - INFO - {'mode': 'Train', 'epoch': 2, 'iter': 500, 'lr': 0.0001005, 'loss': 7.635046482086182, 'acc(%)': 7.210578842315369}
2024-01-03 01:05:20,773 - INFO - {'mode': 'Train', 'epoch': 2, 'iter': 1000, 'lr': 0.0001005, 'loss': 9.360401153564453, 'acc(%)': 6.993006993006993}
2024-01-03 01:06:13,418 - INFO - {'mode': 'Train', 'epoch': 2, 'iter': 1500, 'lr': 0.0001005, 'loss': 7.391360282897949, 'acc(%)': 7.111925383077948}
2024-01-03 01:06:28,626 - INFO - Train: expid = 401518, Epoch = 2, avg_loss = 7.835497049385961, total_acc = 7.1928756279494594%.
2024-01-03 01:06:28,627 - INFO - epoch complete!
2024-01-03 01:06:28,627 - INFO - evaluating now!
2024-01-03 01:06:28,660 - INFO - {'mode': 'Eval', 'epoch': 2, 'iter': 0, 'lr': 0.0001005, 'loss': 7.044180870056152, 'acc(%)': 12.5}
2024-01-03 01:06:43,874 - INFO - {'mode': 'Eval', 'epoch': 2, 'iter': 500, 'lr': 0.0001005, 'loss': 9.633228302001953, 'acc(%)': 9.006986027944112}
2024-01-03 01:06:45,285 - INFO - Eval: expid = 401518, Epoch = 2, avg_loss = 9.017651502617962, total_acc = 9.06392694063927%.
2024-01-03 01:06:45,285 - INFO - Epoch [2/30] (4929)  train_loss: 7.8355, train_acc: 7.19%, val_loss: 9.0177, val_acc: 9.06%, lr: 0.000150, 189.14s
2024-01-03 01:06:45,657 - INFO - Saved model at 2
2024-01-03 01:06:45,657 - INFO - Val loss decrease from 9.5658 to 9.0177, saving to ./libcity/cache/401518/model_cache/LinearNextLoc_bj_epoch2.tar
2024-01-03 01:06:45,760 - INFO - {'mode': 'Train', 'epoch': 3, 'iter': 0, 'lr': 0.00015025000000000002, 'loss': 7.073948860168457, 'acc(%)': 0.0}
2024-01-03 01:07:38,655 - INFO - {'mode': 'Train', 'epoch': 3, 'iter': 500, 'lr': 0.00015025000000000002, 'loss': 6.996488094329834, 'acc(%)': 8.483033932135728}
2024-01-03 01:08:32,720 - INFO - {'mode': 'Train', 'epoch': 3, 'iter': 1000, 'lr': 0.00015025000000000002, 'loss': 7.419269561767578, 'acc(%)': 8.379120879120878}
2024-01-03 01:09:26,064 - INFO - {'mode': 'Train', 'epoch': 3, 'iter': 1500, 'lr': 0.00015025000000000002, 'loss': 7.4632062911987305, 'acc(%)': 8.869087275149901}
2024-01-03 01:09:42,416 - INFO - Train: expid = 401518, Epoch = 3, avg_loss = 6.811923587280496, total_acc = 9.027249200791596%.
2024-01-03 01:09:42,416 - INFO - epoch complete!
2024-01-03 01:09:42,417 - INFO - evaluating now!
2024-01-03 01:09:42,448 - INFO - {'mode': 'Eval', 'epoch': 3, 'iter': 0, 'lr': 0.00015025000000000002, 'loss': 7.9926252365112305, 'acc(%)': 12.5}
2024-01-03 01:10:00,339 - INFO - {'mode': 'Eval', 'epoch': 3, 'iter': 500, 'lr': 0.00015025000000000002, 'loss': 6.73759651184082, 'acc(%)': 10.803393213572853}
2024-01-03 01:10:02,020 - INFO - Eval: expid = 401518, Epoch = 3, avg_loss = 8.628250881526023, total_acc = 10.45662100456621%.
2024-01-03 01:10:02,020 - INFO - Epoch [3/30] (6572)  train_loss: 6.8119, train_acc: 9.03%, val_loss: 8.6283, val_acc: 10.46%, lr: 0.000191, 196.36s
2024-01-03 01:10:02,411 - INFO - Saved model at 3
2024-01-03 01:10:02,411 - INFO - Val loss decrease from 9.0177 to 8.6283, saving to ./libcity/cache/401518/model_cache/LinearNextLoc_bj_epoch3.tar
2024-01-03 01:10:02,535 - INFO - {'mode': 'Train', 'epoch': 4, 'iter': 0, 'lr': 0.0001913545457642601, 'loss': 4.726256847381592, 'acc(%)': 25.0}
2024-01-03 01:11:00,197 - INFO - {'mode': 'Train', 'epoch': 4, 'iter': 500, 'lr': 0.0001913545457642601, 'loss': 7.181049346923828, 'acc(%)': 10.978043912175648}
2024-01-03 01:11:55,666 - INFO - {'mode': 'Train', 'epoch': 4, 'iter': 1000, 'lr': 0.0001913545457642601, 'loss': 7.115065574645996, 'acc(%)': 11.113886113886114}
2024-01-03 01:12:51,564 - INFO - {'mode': 'Train', 'epoch': 4, 'iter': 1500, 'lr': 0.0001913545457642601, 'loss': 5.055838584899902, 'acc(%)': 11.14257161892072}
2024-01-03 01:13:08,593 - INFO - Train: expid = 401518, Epoch = 4, avg_loss = 5.941474804505853, total_acc = 11.166083117673924%.
2024-01-03 01:13:08,594 - INFO - epoch complete!
2024-01-03 01:13:08,594 - INFO - evaluating now!
2024-01-03 01:13:08,632 - INFO - {'mode': 'Eval', 'epoch': 4, 'iter': 0, 'lr': 0.0001913545457642601, 'loss': 9.00358772277832, 'acc(%)': 12.5}
2024-01-03 01:13:24,274 - INFO - {'mode': 'Eval', 'epoch': 4, 'iter': 500, 'lr': 0.0001913545457642601, 'loss': 12.862190246582031, 'acc(%)': 10.978043912175648}
2024-01-03 01:13:25,684 - INFO - Eval: expid = 401518, Epoch = 4, avg_loss = 8.444729573759314, total_acc = 10.95890410958904%.
2024-01-03 01:13:25,684 - INFO - Epoch [4/30] (8215)  train_loss: 5.9415, train_acc: 11.17%, val_loss: 8.4447, val_acc: 10.96%, lr: 0.000187, 203.27s
2024-01-03 01:13:26,045 - INFO - Saved model at 4
2024-01-03 01:13:26,045 - INFO - Val loss decrease from 8.6283 to 8.4447, saving to ./libcity/cache/401518/model_cache/LinearNextLoc_bj_epoch4.tar
2024-01-03 01:13:26,154 - INFO - {'mode': 'Train', 'epoch': 5, 'iter': 0, 'lr': 0.00018660254037844388, 'loss': 5.442719459533691, 'acc(%)': 0.0}
2024-01-03 01:14:17,283 - INFO - {'mode': 'Train', 'epoch': 5, 'iter': 500, 'lr': 0.00018660254037844388, 'loss': 5.906412601470947, 'acc(%)': 15.6187624750499}
2024-01-03 01:15:08,240 - INFO - {'mode': 'Train', 'epoch': 5, 'iter': 1000, 'lr': 0.00018660254037844388, 'loss': 5.453440189361572, 'acc(%)': 15.15984015984016}
2024-01-03 01:15:59,220 - INFO - {'mode': 'Train', 'epoch': 5, 'iter': 1500, 'lr': 0.00018660254037844388, 'loss': 4.868834495544434, 'acc(%)': 15.006662225183213}
2024-01-03 01:16:13,650 - INFO - Train: expid = 401518, Epoch = 5, avg_loss = 5.064211955297699, total_acc = 14.987060435378291%.
2024-01-03 01:16:13,650 - INFO - epoch complete!
2024-01-03 01:16:13,650 - INFO - evaluating now!
2024-01-03 01:16:13,680 - INFO - {'mode': 'Eval', 'epoch': 5, 'iter': 0, 'lr': 0.00018660254037844388, 'loss': 6.446930408477783, 'acc(%)': 25.0}
2024-01-03 01:16:28,430 - INFO - {'mode': 'Eval', 'epoch': 5, 'iter': 500, 'lr': 0.00018660254037844388, 'loss': 11.953680992126465, 'acc(%)': 12.77445109780439}
2024-01-03 01:16:29,820 - INFO - Eval: expid = 401518, Epoch = 5, avg_loss = 8.427236457284726, total_acc = 12.420091324200913%.
2024-01-03 01:16:29,820 - INFO - Epoch [5/30] (9858)  train_loss: 5.0642, train_acc: 14.99%, val_loss: 8.4272, val_acc: 12.42%, lr: 0.000181, 183.78s
2024-01-03 01:16:30,191 - INFO - Saved model at 5
2024-01-03 01:16:30,191 - INFO - Val loss decrease from 8.4447 to 8.4272, saving to ./libcity/cache/401518/model_cache/LinearNextLoc_bj_epoch5.tar
2024-01-03 01:16:30,290 - INFO - {'mode': 'Train', 'epoch': 6, 'iter': 0, 'lr': 0.00018090169943749476, 'loss': 3.248915910720825, 'acc(%)': 50.0}
2024-01-03 01:17:21,575 - INFO - {'mode': 'Train', 'epoch': 6, 'iter': 500, 'lr': 0.00018090169943749476, 'loss': 3.346041679382324, 'acc(%)': 20.459081836327346}
2024-01-03 01:18:12,490 - INFO - {'mode': 'Train', 'epoch': 6, 'iter': 1000, 'lr': 0.00018090169943749476, 'loss': 4.706019401550293, 'acc(%)': 19.642857142857142}
2024-01-03 01:19:03,499 - INFO - {'mode': 'Train', 'epoch': 6, 'iter': 1500, 'lr': 0.00018090169943749476, 'loss': 3.5049901008605957, 'acc(%)': 19.337108594270486}
2024-01-03 01:19:17,981 - INFO - Train: expid = 401518, Epoch = 6, avg_loss = 4.338093922116075, total_acc = 19.066829045516823%.
2024-01-03 01:19:17,981 - INFO - epoch complete!
2024-01-03 01:19:17,981 - INFO - evaluating now!
2024-01-03 01:19:18,011 - INFO - {'mode': 'Eval', 'epoch': 6, 'iter': 0, 'lr': 0.00018090169943749476, 'loss': 10.124143600463867, 'acc(%)': 12.5}
2024-01-03 01:19:32,774 - INFO - {'mode': 'Eval', 'epoch': 6, 'iter': 500, 'lr': 0.00018090169943749476, 'loss': 8.904586791992188, 'acc(%)': 13.697604790419163}
2024-01-03 01:19:34,154 - INFO - Eval: expid = 401518, Epoch = 6, avg_loss = 8.519418708374511, total_acc = 13.561643835616438%.
2024-01-03 01:19:34,154 - INFO - Epoch [6/30] (11501)  train_loss: 4.3381, train_acc: 19.07%, val_loss: 8.5194, val_acc: 13.56%, lr: 0.000174, 183.96s
2024-01-03 01:19:34,264 - INFO - {'mode': 'Train', 'epoch': 7, 'iter': 0, 'lr': 0.00017431448254773944, 'loss': 3.8963193893432617, 'acc(%)': 25.0}
2024-01-03 01:20:25,242 - INFO - {'mode': 'Train', 'epoch': 7, 'iter': 500, 'lr': 0.00017431448254773944, 'loss': 3.628204822540283, 'acc(%)': 29.990019960079838}
2024-01-03 01:21:16,358 - INFO - {'mode': 'Train', 'epoch': 7, 'iter': 1000, 'lr': 0.00017431448254773944, 'loss': 2.7748777866363525, 'acc(%)': 27.385114885114888}
2024-01-03 01:22:07,324 - INFO - {'mode': 'Train', 'epoch': 7, 'iter': 1500, 'lr': 0.00017431448254773944, 'loss': 4.3106584548950195, 'acc(%)': 26.740506329113924}
2024-01-03 01:22:21,825 - INFO - Train: expid = 401518, Epoch = 7, avg_loss = 3.6679489991856884, total_acc = 26.518495965900442%.
2024-01-03 01:22:21,825 - INFO - epoch complete!
2024-01-03 01:22:21,825 - INFO - evaluating now!
2024-01-03 01:22:21,855 - INFO - {'mode': 'Eval', 'epoch': 7, 'iter': 0, 'lr': 0.00017431448254773944, 'loss': 2.646286964416504, 'acc(%)': 75.0}
2024-01-03 01:22:36,577 - INFO - {'mode': 'Eval', 'epoch': 7, 'iter': 500, 'lr': 0.00017431448254773944, 'loss': 8.0366792678833, 'acc(%)': 14.496007984031936}
2024-01-03 01:22:37,957 - INFO - Eval: expid = 401518, Epoch = 7, avg_loss = 8.4726460674582, total_acc = 14.520547945205479%.
2024-01-03 01:22:37,957 - INFO - Epoch [7/30] (13144)  train_loss: 3.6679, train_acc: 26.52%, val_loss: 8.4726, val_acc: 14.52%, lr: 0.000167, 183.80s
2024-01-03 01:22:38,067 - INFO - {'mode': 'Train', 'epoch': 8, 'iter': 0, 'lr': 0.00016691306063588583, 'loss': 2.3035504817962646, 'acc(%)': 50.0}
2024-01-03 01:23:28,981 - INFO - {'mode': 'Train', 'epoch': 8, 'iter': 500, 'lr': 0.00016691306063588583, 'loss': 2.116091728210449, 'acc(%)': 40.993013972055884}
2024-01-03 01:24:20,054 - INFO - {'mode': 'Train', 'epoch': 8, 'iter': 1000, 'lr': 0.00016691306063588583, 'loss': 2.9651269912719727, 'acc(%)': 38.2992007992008}
2024-01-03 01:25:10,897 - INFO - {'mode': 'Train', 'epoch': 8, 'iter': 1500, 'lr': 0.00016691306063588583, 'loss': 3.733975410461426, 'acc(%)': 36.64223850766156}
2024-01-03 01:25:25,306 - INFO - Train: expid = 401518, Epoch = 8, avg_loss = 3.016424551676499, total_acc = 36.13944283757041%.
2024-01-03 01:25:25,306 - INFO - epoch complete!
2024-01-03 01:25:25,306 - INFO - evaluating now!
2024-01-03 01:25:25,336 - INFO - {'mode': 'Eval', 'epoch': 8, 'iter': 0, 'lr': 0.00016691306063588583, 'loss': 10.842436790466309, 'acc(%)': 25.0}
2024-01-03 01:25:40,016 - INFO - {'mode': 'Eval', 'epoch': 8, 'iter': 500, 'lr': 0.00016691306063588583, 'loss': 9.251383781433105, 'acc(%)': 14.920159680638722}
2024-01-03 01:25:41,388 - INFO - Eval: expid = 401518, Epoch = 8, avg_loss = 8.594389045837262, total_acc = 15.205479452054796%.
2024-01-03 01:25:41,388 - INFO - Epoch [8/30] (14787)  train_loss: 3.0164, train_acc: 36.14%, val_loss: 8.5944, val_acc: 15.21%, lr: 0.000159, 183.43s
2024-01-03 01:25:41,498 - INFO - {'mode': 'Train', 'epoch': 9, 'iter': 0, 'lr': 0.00015877852522924732, 'loss': 2.1677188873291016, 'acc(%)': 62.5}
2024-01-03 01:26:32,406 - INFO - {'mode': 'Train', 'epoch': 9, 'iter': 500, 'lr': 0.00015877852522924732, 'loss': 2.4319844245910645, 'acc(%)': 54.765469061876246}
2024-01-03 01:27:23,144 - INFO - {'mode': 'Train', 'epoch': 9, 'iter': 1000, 'lr': 0.00015877852522924732, 'loss': 3.0613315105438232, 'acc(%)': 50.836663336663335}
2024-01-03 01:28:14,349 - INFO - {'mode': 'Train', 'epoch': 9, 'iter': 1500, 'lr': 0.00015877852522924732, 'loss': 2.6874427795410156, 'acc(%)': 48.40106595602931}
2024-01-03 01:28:28,781 - INFO - Train: expid = 401518, Epoch = 9, avg_loss = 2.440760021177778, total_acc = 47.66326685949155%.
2024-01-03 01:28:28,781 - INFO - epoch complete!
2024-01-03 01:28:28,781 - INFO - evaluating now!
2024-01-03 01:28:28,811 - INFO - {'mode': 'Eval', 'epoch': 9, 'iter': 0, 'lr': 0.00015877852522924732, 'loss': 9.79002571105957, 'acc(%)': 12.5}
2024-01-03 01:28:43,521 - INFO - {'mode': 'Eval', 'epoch': 9, 'iter': 500, 'lr': 0.00015877852522924732, 'loss': 9.982636451721191, 'acc(%)': 16.392215568862277}
2024-01-03 01:28:44,901 - INFO - Eval: expid = 401518, Epoch = 9, avg_loss = 8.790708194366873, total_acc = 16.484018264840184%.
2024-01-03 01:28:44,901 - INFO - Epoch [9/30] (16430)  train_loss: 2.4408, train_acc: 47.66%, val_loss: 8.7907, val_acc: 16.48%, lr: 0.000150, 183.51s
2024-01-03 01:28:45,001 - INFO - {'mode': 'Train', 'epoch': 10, 'iter': 0, 'lr': 0.00015000000000000001, 'loss': 1.56076180934906, 'acc(%)': 75.0}
2024-01-03 01:29:35,894 - INFO - {'mode': 'Train', 'epoch': 10, 'iter': 500, 'lr': 0.00015000000000000001, 'loss': 2.239290475845337, 'acc(%)': 64.19660678642714}
2024-01-03 01:30:26,769 - INFO - {'mode': 'Train', 'epoch': 10, 'iter': 1000, 'lr': 0.00015000000000000001, 'loss': 1.9802312850952148, 'acc(%)': 61.98801198801199}
2024-01-03 01:31:17,674 - INFO - {'mode': 'Train', 'epoch': 10, 'iter': 1500, 'lr': 0.00015000000000000001, 'loss': 1.7746155261993408, 'acc(%)': 59.86009327115257}
2024-01-03 01:31:32,084 - INFO - Train: expid = 401518, Epoch = 10, avg_loss = 1.9205795161926251, total_acc = 59.36976708783681%.
2024-01-03 01:31:32,084 - INFO - epoch complete!
2024-01-03 01:31:32,084 - INFO - evaluating now!
2024-01-03 01:31:32,114 - INFO - {'mode': 'Eval', 'epoch': 10, 'iter': 0, 'lr': 0.00015000000000000001, 'loss': 12.115407943725586, 'acc(%)': 25.0}
2024-01-03 01:31:46,795 - INFO - {'mode': 'Eval', 'epoch': 10, 'iter': 500, 'lr': 0.00015000000000000001, 'loss': 8.5782470703125, 'acc(%)': 16.192614770459084}
2024-01-03 01:31:48,175 - INFO - Eval: expid = 401518, Epoch = 10, avg_loss = 8.899320603287928, total_acc = 16.506849315068493%.
2024-01-03 01:31:48,175 - INFO - Epoch [10/30] (18073)  train_loss: 1.9206, train_acc: 59.37%, val_loss: 8.8993, val_acc: 16.51%, lr: 0.000141, 183.27s
2024-01-03 01:31:48,285 - INFO - {'mode': 'Train', 'epoch': 11, 'iter': 0, 'lr': 0.00014067366430758004, 'loss': 1.2635084390640259, 'acc(%)': 75.0}
2024-01-03 01:32:39,100 - INFO - {'mode': 'Train', 'epoch': 11, 'iter': 500, 'lr': 0.00014067366430758004, 'loss': 1.2732926607131958, 'acc(%)': 75.94810379241517}
2024-01-03 01:33:30,289 - INFO - {'mode': 'Train', 'epoch': 11, 'iter': 1000, 'lr': 0.00014067366430758004, 'loss': 1.3131966590881348, 'acc(%)': 72.72727272727273}
2024-01-03 01:34:21,193 - INFO - {'mode': 'Train', 'epoch': 11, 'iter': 1500, 'lr': 0.00014067366430758004, 'loss': 1.5712592601776123, 'acc(%)': 70.08660892738175}
2024-01-03 01:34:35,603 - INFO - Train: expid = 401518, Epoch = 11, avg_loss = 1.4808406036254713, total_acc = 69.40173542396103%.
2024-01-03 01:34:35,603 - INFO - epoch complete!
2024-01-03 01:34:35,603 - INFO - evaluating now!
2024-01-03 01:34:35,633 - INFO - {'mode': 'Eval', 'epoch': 11, 'iter': 0, 'lr': 0.00014067366430758004, 'loss': 10.050569534301758, 'acc(%)': 12.5}
2024-01-03 01:34:50,355 - INFO - {'mode': 'Eval', 'epoch': 11, 'iter': 500, 'lr': 0.00014067366430758004, 'loss': 10.210236549377441, 'acc(%)': 16.716566866267467}
2024-01-03 01:34:51,735 - INFO - Eval: expid = 401518, Epoch = 11, avg_loss = 9.152158210702138, total_acc = 16.872146118721464%.
2024-01-03 01:34:51,735 - INFO - Epoch [11/30] (19716)  train_loss: 1.4808, train_acc: 69.40%, val_loss: 9.1522, val_acc: 16.87%, lr: 0.000131, 183.56s
2024-01-03 01:34:51,835 - INFO - {'mode': 'Train', 'epoch': 12, 'iter': 0, 'lr': 0.00013090169943749476, 'loss': 1.2791802883148193, 'acc(%)': 87.5}
2024-01-03 01:35:42,707 - INFO - {'mode': 'Train', 'epoch': 12, 'iter': 500, 'lr': 0.00013090169943749476, 'loss': 0.7543402910232544, 'acc(%)': 81.4620758483034}
2024-01-03 01:36:33,553 - INFO - {'mode': 'Train', 'epoch': 12, 'iter': 1000, 'lr': 0.00013090169943749476, 'loss': 1.1639541387557983, 'acc(%)': 79.64535464535464}
2024-01-03 01:37:24,441 - INFO - {'mode': 'Train', 'epoch': 12, 'iter': 1500, 'lr': 0.00013090169943749476, 'loss': 0.7388074994087219, 'acc(%)': 78.5143237841439}
2024-01-03 01:37:38,881 - INFO - Train: expid = 401518, Epoch = 12, avg_loss = 1.116833463281706, total_acc = 78.04079768610139%.
2024-01-03 01:37:38,881 - INFO - epoch complete!
2024-01-03 01:37:38,881 - INFO - evaluating now!
2024-01-03 01:37:38,921 - INFO - {'mode': 'Eval', 'epoch': 12, 'iter': 0, 'lr': 0.00013090169943749476, 'loss': 8.51513671875, 'acc(%)': 25.0}
2024-01-03 01:37:53,574 - INFO - {'mode': 'Eval', 'epoch': 12, 'iter': 500, 'lr': 0.00013090169943749476, 'loss': 10.681659698486328, 'acc(%)': 17.090818363273453}
2024-01-03 01:37:54,954 - INFO - Eval: expid = 401518, Epoch = 12, avg_loss = 9.228701310615017, total_acc = 17.031963470319635%.
2024-01-03 01:37:54,954 - INFO - Epoch [12/30] (21359)  train_loss: 1.1168, train_acc: 78.04%, val_loss: 9.2287, val_acc: 17.03%, lr: 0.000121, 183.22s
2024-01-03 01:37:55,064 - INFO - {'mode': 'Train', 'epoch': 13, 'iter': 0, 'lr': 0.00012079116908177593, 'loss': 0.37123072147369385, 'acc(%)': 100.0}
2024-01-03 01:38:46,016 - INFO - {'mode': 'Train', 'epoch': 13, 'iter': 500, 'lr': 0.00012079116908177593, 'loss': 1.687157154083252, 'acc(%)': 87.02594810379242}
2024-01-03 01:39:36,789 - INFO - {'mode': 'Train', 'epoch': 13, 'iter': 1000, 'lr': 0.00012079116908177593, 'loss': 0.6285892724990845, 'acc(%)': 85.3896103896104}
2024-01-03 01:40:27,497 - INFO - {'mode': 'Train', 'epoch': 13, 'iter': 1500, 'lr': 0.00012079116908177593, 'loss': 0.8454186916351318, 'acc(%)': 83.86075949367088}
2024-01-03 01:40:41,908 - INFO - Train: expid = 401518, Epoch = 13, avg_loss = 0.8560020973519441, total_acc = 83.28512711219363%.
2024-01-03 01:40:41,908 - INFO - epoch complete!
2024-01-03 01:40:41,908 - INFO - evaluating now!
2024-01-03 01:40:41,938 - INFO - {'mode': 'Eval', 'epoch': 13, 'iter': 0, 'lr': 0.00012079116908177593, 'loss': 10.993206977844238, 'acc(%)': 0.0}
2024-01-03 01:40:56,722 - INFO - {'mode': 'Eval', 'epoch': 13, 'iter': 500, 'lr': 0.00012079116908177593, 'loss': 9.687313079833984, 'acc(%)': 16.24251497005988}
2024-01-03 01:40:58,122 - INFO - Eval: expid = 401518, Epoch = 13, avg_loss = 9.559020347246841, total_acc = 16.5296803652968%.
2024-01-03 01:40:58,122 - INFO - Epoch [13/30] (23002)  train_loss: 0.8560, train_acc: 83.29%, val_loss: 9.5590, val_acc: 16.53%, lr: 0.000110, 183.17s
2024-01-03 01:40:58,222 - INFO - {'mode': 'Train', 'epoch': 14, 'iter': 0, 'lr': 0.00011045284632676536, 'loss': 0.572772741317749, 'acc(%)': 87.5}
2024-01-03 01:41:48,992 - INFO - {'mode': 'Train', 'epoch': 14, 'iter': 500, 'lr': 0.00011045284632676536, 'loss': 0.3471088111400604, 'acc(%)': 89.64570858283433}
2024-01-03 01:42:39,674 - INFO - {'mode': 'Train', 'epoch': 14, 'iter': 1000, 'lr': 0.00011045284632676536, 'loss': 0.9387889504432678, 'acc(%)': 88.82367632367632}
2024-01-03 01:43:30,580 - INFO - {'mode': 'Train', 'epoch': 14, 'iter': 1500, 'lr': 0.00011045284632676536, 'loss': 1.4411506652832031, 'acc(%)': 87.82478347768155}
2024-01-03 01:43:44,996 - INFO - Train: expid = 401518, Epoch = 14, avg_loss = 0.6598951819008717, total_acc = 87.52473740295326%.
2024-01-03 01:43:44,996 - INFO - epoch complete!
2024-01-03 01:43:44,996 - INFO - evaluating now!
2024-01-03 01:43:45,026 - INFO - {'mode': 'Eval', 'epoch': 14, 'iter': 0, 'lr': 0.00011045284632676536, 'loss': 11.174174308776855, 'acc(%)': 25.0}
2024-01-03 01:43:59,756 - INFO - {'mode': 'Eval', 'epoch': 14, 'iter': 500, 'lr': 0.00011045284632676536, 'loss': 14.317146301269531, 'acc(%)': 17.415169660678643}
2024-01-03 01:44:01,136 - INFO - Eval: expid = 401518, Epoch = 14, avg_loss = 9.70357647072779, total_acc = 17.168949771689498%.
2024-01-03 01:44:01,136 - INFO - Epoch [14/30] (24645)  train_loss: 0.6599, train_acc: 87.52%, val_loss: 9.7036, val_acc: 17.17%, lr: 0.000100, 183.01s
2024-01-03 01:44:01,246 - INFO - {'mode': 'Train', 'epoch': 15, 'iter': 0, 'lr': 0.00010000000000000003, 'loss': 0.2958754897117615, 'acc(%)': 100.0}
2024-01-03 01:44:52,054 - INFO - {'mode': 'Train', 'epoch': 15, 'iter': 500, 'lr': 0.00010000000000000003, 'loss': 0.8285320997238159, 'acc(%)': 92.86427145708582}
2024-01-03 01:45:42,877 - INFO - {'mode': 'Train', 'epoch': 15, 'iter': 1000, 'lr': 0.00010000000000000003, 'loss': 0.2342483401298523, 'acc(%)': 91.6958041958042}
2024-01-03 01:46:33,698 - INFO - {'mode': 'Train', 'epoch': 15, 'iter': 1500, 'lr': 0.00010000000000000003, 'loss': 0.9388202428817749, 'acc(%)': 90.72285143237842}
2024-01-03 01:46:48,171 - INFO - Train: expid = 401518, Epoch = 15, avg_loss = 0.5149264360539426, total_acc = 90.5008372659461%.
2024-01-03 01:46:48,171 - INFO - epoch complete!
2024-01-03 01:46:48,171 - INFO - evaluating now!
2024-01-03 01:46:48,201 - INFO - {'mode': 'Eval', 'epoch': 15, 'iter': 0, 'lr': 0.00010000000000000003, 'loss': 11.97681999206543, 'acc(%)': 0.0}
2024-01-03 01:47:02,913 - INFO - {'mode': 'Eval', 'epoch': 15, 'iter': 500, 'lr': 0.00010000000000000003, 'loss': 11.80970287322998, 'acc(%)': 17.465069860279442}
2024-01-03 01:47:04,283 - INFO - Eval: expid = 401518, Epoch = 15, avg_loss = 9.65650547214839, total_acc = 17.374429223744293%.
2024-01-03 01:47:04,293 - INFO - Epoch [15/30] (26288)  train_loss: 0.5149, train_acc: 90.50%, val_loss: 9.6565, val_acc: 17.37%, lr: 0.000090, 183.16s
2024-01-03 01:47:04,293 - WARNING - Early stopping at epoch: 15
2024-01-03 01:47:04,293 - INFO - Trained totally 16 epochs, average train time is 168.758s, average eval time is 16.386s, average train acc is 40.33%, average eval acc is 13.16%
2024-01-03 01:47:04,443 - INFO - Loaded model at 5
2024-01-03 01:47:04,533 - INFO - Save png at ./libcity/cache/401518/401518_loss.png
2024-01-03 01:47:04,593 - INFO - Save png at ./libcity/cache/401518/401518_lr.png
2024-01-03 01:47:04,663 - INFO - Save png at ./libcity/cache/401518/401518_acc.png
2024-01-03 01:47:05,013 - INFO - Saved model at ./libcity/cache/401518/model_cache/401518_LinearNextLoc_bj.pt
2024-01-03 01:47:05,013 - INFO - Start evaluating ...
2024-01-03 01:47:05,043 - INFO - {'mode': 'Test', 'epoch': 0, 'iter': 0, 'lr': 0.00018090169943749476, 'loss': 8.836466789245605, 'acc(%)': 12.5}
2024-01-03 01:47:12,362 - INFO - Test: expid = 401518, Epoch = 0, avg_loss = 8.07818942145464, total_acc = 8.320571720265441%.
2024-01-03 01:47:12,362 - INFO - Test time 7.348794937133789s.
