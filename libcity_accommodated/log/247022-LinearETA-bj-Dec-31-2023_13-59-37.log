2023-12-31 13:59:37,132 - INFO - Log directory: ./libcity/log
2023-12-31 13:59:37,132 - INFO - Begin pretrain-pipeline, task=trajectory_embedding, model_name=LinearETA, dataset_name=bj, exp_id=247022
2023-12-31 13:59:37,132 - INFO - {'task': 'trajectory_embedding', 'model': 'LinearETA', 'dataset': 'bj', 'saved_model': True, 'train': True, 'gpu': True, 'gpu_id': 0, 'pretrain_path': 'libcity/cache/135178/model_cache/135178_BERTContrastiveLM_bj.pt', 'n_layers': 6, 'd_model': 256, 'attn_heads': 8, 'max_epoch': 15, 'batch_size': 8, 'grad_accmu_steps': 1, 'learning_rate': 0.0002, 'roadnetwork': 'bj_roadmap_edge_bj_True_1_merge', 'geo_file': 'bj_roadmap_edge_bj_True_1_merge_withdegree', 'rel_file': 'bj_roadmap_edge_bj_True_1_merge_withdegree', 'merge': True, 'min_freq': 1, 'seq_len': 128, 'test_every': 50, 'temperature': 0.05, 'contra_loss_type': 'simclr', 'classify_label': 'vflag', 'type_ln': 'post', 'add_cls': True, 'add_time_in_day': True, 'add_day_in_week': True, 'add_pe': True, 'add_temporal_bias': True, 'temporal_bias_dim': 64, 'use_mins_interval': False, 'add_gat': True, 'gat_heads_per_layer': [8, 16, 1], 'gat_features_per_layer': [16, 16, 256], 'gat_dropout': 0.1, 'gat_K': 1, 'gat_avg_last': True, 'load_trans_prob': True, 'append_degree2gcn': True, 'normal_feature': False, 'pooling': 'cls', 'dataset_class': 'ETADataset', 'executor': 'ETAExecutor', 'evaluator': 'RegressionEvaluator', 'num_workers': 0, 'vocab_path': None, 'mlp_ratio': 4, 'pretrain_road_emb': None, 'future_mask': False, 'load_node2vec': False, 'dropout': 0.1, 'drop_path': 0.3, 'attn_drop': 0.1, 'seed': 0, 'learner': 'adamw', 'lr_eta_min': 0, 'lr_warmup_epoch': 4, 'lr_warmup_init': 1e-06, 'lr_decay': True, 'lr_scheduler': 'cosinelr', 'lr_decay_ratio': 0.1, 't_in_epochs': True, 'clip_grad_norm': True, 'max_grad_norm': 5, 'use_early_stop': True, 'patience': 10, 'log_batch': 500, 'log_every': 1, 'l2_reg': None, 'bidir_adj_mx': False, 'freeze': False, 'metrics': ['MAE', 'RMSE', 'MAPE', 'R2', 'EVAR'], 'save_modes': ['csv', 'json'], 'device': device(type='cuda', index=0), 'exp_id': 247022}
2023-12-31 13:59:37,636 - INFO - Loading Vocab from raw_data/vocab_bj_True_1_merge.pkl
2023-12-31 13:59:37,638 - INFO - vocab_path=raw_data/vocab_bj_True_1_merge.pkl, usr_num=3, vocab_size=4558
2023-12-31 13:59:37,648 - INFO - Loaded file bj_roadmap_edge_bj_True_1_merge_withdegree.geo, num_nodes=4554
2023-12-31 13:59:37,694 - INFO - Loaded file bj_roadmap_edge_bj_True_1_merge_withdegree.rel, shape=(4554, 4554), edges=5186.0
2023-12-31 13:59:37,698 - INFO - node_features: (4554, 33)
2023-12-31 13:59:38,594 - INFO - node_features_encoded: torch.Size([4558, 33])
2023-12-31 13:59:38,607 - INFO - edge_index: torch.Size([2, 9744])
2023-12-31 13:59:38,607 - INFO - Trajectory loc-transfer prob shape=torch.Size([9744, 1])
2023-12-31 13:59:38,608 - INFO - Loading Dataset!
2023-12-31 13:59:38,610 - INFO - Load dataset from raw_data/bj/cache_bj_train_True_True_1.pkl
2023-12-31 13:59:38,611 - INFO - Load dataset from raw_data/bj/cache_bj_eval_True_True_1.pkl
2023-12-31 13:59:38,611 - INFO - Load dataset from raw_data/bj/cache_bj_test_True_True_1.pkl
2023-12-31 13:59:38,612 - INFO - Size of dataset: 118/40/40
2023-12-31 13:59:38,612 - INFO - Creating Dataloader!
2023-12-31 13:59:38,614 - INFO - Building Downstream LinearETA model
2023-12-31 13:59:38,614 - INFO - Building BERTDownstream model
2023-12-31 13:59:39,210 - INFO - LinearETA(
  (model): BERTDownstream(
    (bert): BERT(
      (embedding): BERTEmbedding(
        (token_embedding): GAT(
          (gat_net): Sequential(
            (0): GATLayerImp3(
              (linear_proj): Linear(in_features=33, out_features=128, bias=False)
              (linear_proj_tran_prob): Linear(in_features=1, out_features=128, bias=False)
              (skip_proj): Linear(in_features=33, out_features=128, bias=False)
              (leakyReLU): LeakyReLU(negative_slope=0.2)
              (softmax): Softmax(dim=-1)
              (activation): ELU(alpha=1.0)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (1): GATLayerImp3(
              (linear_proj): Linear(in_features=128, out_features=256, bias=False)
              (linear_proj_tran_prob): Linear(in_features=1, out_features=256, bias=False)
              (skip_proj): Linear(in_features=128, out_features=256, bias=False)
              (leakyReLU): LeakyReLU(negative_slope=0.2)
              (softmax): Softmax(dim=-1)
              (activation): ELU(alpha=1.0)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (2): GATLayerImp3(
              (linear_proj): Linear(in_features=256, out_features=256, bias=False)
              (linear_proj_tran_prob): Linear(in_features=1, out_features=256, bias=False)
              (skip_proj): Linear(in_features=256, out_features=256, bias=False)
              (leakyReLU): LeakyReLU(negative_slope=0.2)
              (softmax): Softmax(dim=-1)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
        )
        (position_embedding): PositionalEmbedding()
        (daytime_embedding): Embedding(1441, 256, padding_idx=0)
        (weekday_embedding): Embedding(8, 256, padding_idx=0)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (transformer_blocks): ModuleList(
        (0): TransformerBlock(
          (attention): MultiHeadedAttention(
            (linear_layers): ModuleList(
              (0): Linear(in_features=256, out_features=256, bias=True)
              (1): Linear(in_features=256, out_features=256, bias=True)
              (2): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout): Dropout(p=0.1, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0.1, inplace=False)
            (temporal_mat_bias_1): Linear(in_features=1, out_features=64, bias=True)
            (temporal_mat_bias_2): Linear(in_features=64, out_features=1, bias=True)
          )
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0.1, inplace=False)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (drop_path): Identity()
        )
        (1): TransformerBlock(
          (attention): MultiHeadedAttention(
            (linear_layers): ModuleList(
              (0): Linear(in_features=256, out_features=256, bias=True)
              (1): Linear(in_features=256, out_features=256, bias=True)
              (2): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout): Dropout(p=0.1, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0.1, inplace=False)
            (temporal_mat_bias_1): Linear(in_features=1, out_features=64, bias=True)
            (temporal_mat_bias_2): Linear(in_features=64, out_features=1, bias=True)
          )
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0.1, inplace=False)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (drop_path): DropPath()
        )
        (2): TransformerBlock(
          (attention): MultiHeadedAttention(
            (linear_layers): ModuleList(
              (0): Linear(in_features=256, out_features=256, bias=True)
              (1): Linear(in_features=256, out_features=256, bias=True)
              (2): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout): Dropout(p=0.1, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0.1, inplace=False)
            (temporal_mat_bias_1): Linear(in_features=1, out_features=64, bias=True)
            (temporal_mat_bias_2): Linear(in_features=64, out_features=1, bias=True)
          )
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0.1, inplace=False)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (drop_path): DropPath()
        )
        (3): TransformerBlock(
          (attention): MultiHeadedAttention(
            (linear_layers): ModuleList(
              (0): Linear(in_features=256, out_features=256, bias=True)
              (1): Linear(in_features=256, out_features=256, bias=True)
              (2): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout): Dropout(p=0.1, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0.1, inplace=False)
            (temporal_mat_bias_1): Linear(in_features=1, out_features=64, bias=True)
            (temporal_mat_bias_2): Linear(in_features=64, out_features=1, bias=True)
          )
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0.1, inplace=False)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (drop_path): DropPath()
        )
        (4): TransformerBlock(
          (attention): MultiHeadedAttention(
            (linear_layers): ModuleList(
              (0): Linear(in_features=256, out_features=256, bias=True)
              (1): Linear(in_features=256, out_features=256, bias=True)
              (2): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout): Dropout(p=0.1, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0.1, inplace=False)
            (temporal_mat_bias_1): Linear(in_features=1, out_features=64, bias=True)
            (temporal_mat_bias_2): Linear(in_features=64, out_features=1, bias=True)
          )
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0.1, inplace=False)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (drop_path): DropPath()
        )
        (5): TransformerBlock(
          (attention): MultiHeadedAttention(
            (linear_layers): ModuleList(
              (0): Linear(in_features=256, out_features=256, bias=True)
              (1): Linear(in_features=256, out_features=256, bias=True)
              (2): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout): Dropout(p=0.1, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0.1, inplace=False)
            (temporal_mat_bias_1): Linear(in_features=1, out_features=64, bias=True)
            (temporal_mat_bias_2): Linear(in_features=64, out_features=1, bias=True)
          )
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0.1, inplace=False)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (drop_path): DropPath()
        )
      )
    )
  )
  (linear): Linear(in_features=256, out_features=1, bias=True)
)
2023-12-31 13:59:39,214 - INFO - model.bert.embedding.token_embedding.gat_net.0.scoring_fn_target	torch.Size([1, 8, 16])	cuda:0	True
2023-12-31 13:59:39,214 - INFO - model.bert.embedding.token_embedding.gat_net.0.scoring_fn_source	torch.Size([1, 8, 16])	cuda:0	True
2023-12-31 13:59:39,214 - INFO - model.bert.embedding.token_embedding.gat_net.0.scoring_trans_prob	torch.Size([1, 8, 16])	cuda:0	True
2023-12-31 13:59:39,214 - INFO - model.bert.embedding.token_embedding.gat_net.0.bias	torch.Size([128])	cuda:0	True
2023-12-31 13:59:39,214 - INFO - model.bert.embedding.token_embedding.gat_net.0.linear_proj.weight	torch.Size([128, 33])	cuda:0	True
2023-12-31 13:59:39,214 - INFO - model.bert.embedding.token_embedding.gat_net.0.linear_proj_tran_prob.weight	torch.Size([128, 1])	cuda:0	True
2023-12-31 13:59:39,214 - INFO - model.bert.embedding.token_embedding.gat_net.0.skip_proj.weight	torch.Size([128, 33])	cuda:0	True
2023-12-31 13:59:39,215 - INFO - model.bert.embedding.token_embedding.gat_net.1.scoring_fn_target	torch.Size([1, 16, 16])	cuda:0	True
2023-12-31 13:59:39,215 - INFO - model.bert.embedding.token_embedding.gat_net.1.scoring_fn_source	torch.Size([1, 16, 16])	cuda:0	True
2023-12-31 13:59:39,215 - INFO - model.bert.embedding.token_embedding.gat_net.1.scoring_trans_prob	torch.Size([1, 16, 16])	cuda:0	True
2023-12-31 13:59:39,215 - INFO - model.bert.embedding.token_embedding.gat_net.1.bias	torch.Size([256])	cuda:0	True
2023-12-31 13:59:39,215 - INFO - model.bert.embedding.token_embedding.gat_net.1.linear_proj.weight	torch.Size([256, 128])	cuda:0	True
2023-12-31 13:59:39,215 - INFO - model.bert.embedding.token_embedding.gat_net.1.linear_proj_tran_prob.weight	torch.Size([256, 1])	cuda:0	True
2023-12-31 13:59:39,215 - INFO - model.bert.embedding.token_embedding.gat_net.1.skip_proj.weight	torch.Size([256, 128])	cuda:0	True
2023-12-31 13:59:39,215 - INFO - model.bert.embedding.token_embedding.gat_net.2.scoring_fn_target	torch.Size([1, 1, 256])	cuda:0	True
2023-12-31 13:59:39,216 - INFO - model.bert.embedding.token_embedding.gat_net.2.scoring_fn_source	torch.Size([1, 1, 256])	cuda:0	True
2023-12-31 13:59:39,216 - INFO - model.bert.embedding.token_embedding.gat_net.2.scoring_trans_prob	torch.Size([1, 1, 256])	cuda:0	True
2023-12-31 13:59:39,216 - INFO - model.bert.embedding.token_embedding.gat_net.2.bias	torch.Size([256])	cuda:0	True
2023-12-31 13:59:39,216 - INFO - model.bert.embedding.token_embedding.gat_net.2.linear_proj.weight	torch.Size([256, 256])	cuda:0	True
2023-12-31 13:59:39,216 - INFO - model.bert.embedding.token_embedding.gat_net.2.linear_proj_tran_prob.weight	torch.Size([256, 1])	cuda:0	True
2023-12-31 13:59:39,216 - INFO - model.bert.embedding.token_embedding.gat_net.2.skip_proj.weight	torch.Size([256, 256])	cuda:0	True
2023-12-31 13:59:39,217 - INFO - model.bert.embedding.daytime_embedding.weight	torch.Size([1441, 256])	cuda:0	True
2023-12-31 13:59:39,217 - INFO - model.bert.embedding.weekday_embedding.weight	torch.Size([8, 256])	cuda:0	True
2023-12-31 13:59:39,217 - INFO - model.bert.transformer_blocks.0.attention.linear_layers.0.weight	torch.Size([256, 256])	cuda:0	True
2023-12-31 13:59:39,217 - INFO - model.bert.transformer_blocks.0.attention.linear_layers.0.bias	torch.Size([256])	cuda:0	True
2023-12-31 13:59:39,217 - INFO - model.bert.transformer_blocks.0.attention.linear_layers.1.weight	torch.Size([256, 256])	cuda:0	True
2023-12-31 13:59:39,217 - INFO - model.bert.transformer_blocks.0.attention.linear_layers.1.bias	torch.Size([256])	cuda:0	True
2023-12-31 13:59:39,217 - INFO - model.bert.transformer_blocks.0.attention.linear_layers.2.weight	torch.Size([256, 256])	cuda:0	True
2023-12-31 13:59:39,217 - INFO - model.bert.transformer_blocks.0.attention.linear_layers.2.bias	torch.Size([256])	cuda:0	True
2023-12-31 13:59:39,217 - INFO - model.bert.transformer_blocks.0.attention.proj.weight	torch.Size([256, 256])	cuda:0	True
2023-12-31 13:59:39,218 - INFO - model.bert.transformer_blocks.0.attention.proj.bias	torch.Size([256])	cuda:0	True
2023-12-31 13:59:39,218 - INFO - model.bert.transformer_blocks.0.attention.temporal_mat_bias_1.weight	torch.Size([64, 1])	cuda:0	True
2023-12-31 13:59:39,218 - INFO - model.bert.transformer_blocks.0.attention.temporal_mat_bias_1.bias	torch.Size([64])	cuda:0	True
2023-12-31 13:59:39,218 - INFO - model.bert.transformer_blocks.0.attention.temporal_mat_bias_2.weight	torch.Size([1, 64])	cuda:0	True
2023-12-31 13:59:39,218 - INFO - model.bert.transformer_blocks.0.attention.temporal_mat_bias_2.bias	torch.Size([1])	cuda:0	True
2023-12-31 13:59:39,218 - INFO - model.bert.transformer_blocks.0.mlp.fc1.weight	torch.Size([1024, 256])	cuda:0	True
2023-12-31 13:59:39,218 - INFO - model.bert.transformer_blocks.0.mlp.fc1.bias	torch.Size([1024])	cuda:0	True
2023-12-31 13:59:39,218 - INFO - model.bert.transformer_blocks.0.mlp.fc2.weight	torch.Size([256, 1024])	cuda:0	True
2023-12-31 13:59:39,218 - INFO - model.bert.transformer_blocks.0.mlp.fc2.bias	torch.Size([256])	cuda:0	True
2023-12-31 13:59:39,219 - INFO - model.bert.transformer_blocks.0.norm1.weight	torch.Size([256])	cuda:0	True
2023-12-31 13:59:39,219 - INFO - model.bert.transformer_blocks.0.norm1.bias	torch.Size([256])	cuda:0	True
2023-12-31 13:59:39,219 - INFO - model.bert.transformer_blocks.0.norm2.weight	torch.Size([256])	cuda:0	True
2023-12-31 13:59:39,219 - INFO - model.bert.transformer_blocks.0.norm2.bias	torch.Size([256])	cuda:0	True
2023-12-31 13:59:39,219 - INFO - model.bert.transformer_blocks.1.attention.linear_layers.0.weight	torch.Size([256, 256])	cuda:0	True
2023-12-31 13:59:39,219 - INFO - model.bert.transformer_blocks.1.attention.linear_layers.0.bias	torch.Size([256])	cuda:0	True
2023-12-31 13:59:39,219 - INFO - model.bert.transformer_blocks.1.attention.linear_layers.1.weight	torch.Size([256, 256])	cuda:0	True
2023-12-31 13:59:39,219 - INFO - model.bert.transformer_blocks.1.attention.linear_layers.1.bias	torch.Size([256])	cuda:0	True
2023-12-31 13:59:39,219 - INFO - model.bert.transformer_blocks.1.attention.linear_layers.2.weight	torch.Size([256, 256])	cuda:0	True
2023-12-31 13:59:39,220 - INFO - model.bert.transformer_blocks.1.attention.linear_layers.2.bias	torch.Size([256])	cuda:0	True
2023-12-31 13:59:39,220 - INFO - model.bert.transformer_blocks.1.attention.proj.weight	torch.Size([256, 256])	cuda:0	True
2023-12-31 13:59:39,220 - INFO - model.bert.transformer_blocks.1.attention.proj.bias	torch.Size([256])	cuda:0	True
2023-12-31 13:59:39,220 - INFO - model.bert.transformer_blocks.1.attention.temporal_mat_bias_1.weight	torch.Size([64, 1])	cuda:0	True
2023-12-31 13:59:39,220 - INFO - model.bert.transformer_blocks.1.attention.temporal_mat_bias_1.bias	torch.Size([64])	cuda:0	True
2023-12-31 13:59:39,220 - INFO - model.bert.transformer_blocks.1.attention.temporal_mat_bias_2.weight	torch.Size([1, 64])	cuda:0	True
2023-12-31 13:59:39,220 - INFO - model.bert.transformer_blocks.1.attention.temporal_mat_bias_2.bias	torch.Size([1])	cuda:0	True
2023-12-31 13:59:39,220 - INFO - model.bert.transformer_blocks.1.mlp.fc1.weight	torch.Size([1024, 256])	cuda:0	True
2023-12-31 13:59:39,220 - INFO - model.bert.transformer_blocks.1.mlp.fc1.bias	torch.Size([1024])	cuda:0	True
2023-12-31 13:59:39,221 - INFO - model.bert.transformer_blocks.1.mlp.fc2.weight	torch.Size([256, 1024])	cuda:0	True
2023-12-31 13:59:39,221 - INFO - model.bert.transformer_blocks.1.mlp.fc2.bias	torch.Size([256])	cuda:0	True
2023-12-31 13:59:39,221 - INFO - model.bert.transformer_blocks.1.norm1.weight	torch.Size([256])	cuda:0	True
2023-12-31 13:59:39,221 - INFO - model.bert.transformer_blocks.1.norm1.bias	torch.Size([256])	cuda:0	True
2023-12-31 13:59:39,221 - INFO - model.bert.transformer_blocks.1.norm2.weight	torch.Size([256])	cuda:0	True
2023-12-31 13:59:39,221 - INFO - model.bert.transformer_blocks.1.norm2.bias	torch.Size([256])	cuda:0	True
2023-12-31 13:59:39,221 - INFO - model.bert.transformer_blocks.2.attention.linear_layers.0.weight	torch.Size([256, 256])	cuda:0	True
2023-12-31 13:59:39,221 - INFO - model.bert.transformer_blocks.2.attention.linear_layers.0.bias	torch.Size([256])	cuda:0	True
2023-12-31 13:59:39,222 - INFO - model.bert.transformer_blocks.2.attention.linear_layers.1.weight	torch.Size([256, 256])	cuda:0	True
2023-12-31 13:59:39,222 - INFO - model.bert.transformer_blocks.2.attention.linear_layers.1.bias	torch.Size([256])	cuda:0	True
2023-12-31 13:59:39,222 - INFO - model.bert.transformer_blocks.2.attention.linear_layers.2.weight	torch.Size([256, 256])	cuda:0	True
2023-12-31 13:59:39,222 - INFO - model.bert.transformer_blocks.2.attention.linear_layers.2.bias	torch.Size([256])	cuda:0	True
2023-12-31 13:59:39,222 - INFO - model.bert.transformer_blocks.2.attention.proj.weight	torch.Size([256, 256])	cuda:0	True
2023-12-31 13:59:39,222 - INFO - model.bert.transformer_blocks.2.attention.proj.bias	torch.Size([256])	cuda:0	True
2023-12-31 13:59:39,222 - INFO - model.bert.transformer_blocks.2.attention.temporal_mat_bias_1.weight	torch.Size([64, 1])	cuda:0	True
2023-12-31 13:59:39,222 - INFO - model.bert.transformer_blocks.2.attention.temporal_mat_bias_1.bias	torch.Size([64])	cuda:0	True
2023-12-31 13:59:39,222 - INFO - model.bert.transformer_blocks.2.attention.temporal_mat_bias_2.weight	torch.Size([1, 64])	cuda:0	True
2023-12-31 13:59:39,223 - INFO - model.bert.transformer_blocks.2.attention.temporal_mat_bias_2.bias	torch.Size([1])	cuda:0	True
2023-12-31 13:59:39,223 - INFO - model.bert.transformer_blocks.2.mlp.fc1.weight	torch.Size([1024, 256])	cuda:0	True
2023-12-31 13:59:39,223 - INFO - model.bert.transformer_blocks.2.mlp.fc1.bias	torch.Size([1024])	cuda:0	True
2023-12-31 13:59:39,223 - INFO - model.bert.transformer_blocks.2.mlp.fc2.weight	torch.Size([256, 1024])	cuda:0	True
2023-12-31 13:59:39,223 - INFO - model.bert.transformer_blocks.2.mlp.fc2.bias	torch.Size([256])	cuda:0	True
2023-12-31 13:59:39,223 - INFO - model.bert.transformer_blocks.2.norm1.weight	torch.Size([256])	cuda:0	True
2023-12-31 13:59:39,223 - INFO - model.bert.transformer_blocks.2.norm1.bias	torch.Size([256])	cuda:0	True
2023-12-31 13:59:39,223 - INFO - model.bert.transformer_blocks.2.norm2.weight	torch.Size([256])	cuda:0	True
2023-12-31 13:59:39,223 - INFO - model.bert.transformer_blocks.2.norm2.bias	torch.Size([256])	cuda:0	True
2023-12-31 13:59:39,224 - INFO - model.bert.transformer_blocks.3.attention.linear_layers.0.weight	torch.Size([256, 256])	cuda:0	True
2023-12-31 13:59:39,224 - INFO - model.bert.transformer_blocks.3.attention.linear_layers.0.bias	torch.Size([256])	cuda:0	True
2023-12-31 13:59:39,224 - INFO - model.bert.transformer_blocks.3.attention.linear_layers.1.weight	torch.Size([256, 256])	cuda:0	True
2023-12-31 13:59:39,224 - INFO - model.bert.transformer_blocks.3.attention.linear_layers.1.bias	torch.Size([256])	cuda:0	True
2023-12-31 13:59:39,224 - INFO - model.bert.transformer_blocks.3.attention.linear_layers.2.weight	torch.Size([256, 256])	cuda:0	True
2023-12-31 13:59:39,224 - INFO - model.bert.transformer_blocks.3.attention.linear_layers.2.bias	torch.Size([256])	cuda:0	True
2023-12-31 13:59:39,224 - INFO - model.bert.transformer_blocks.3.attention.proj.weight	torch.Size([256, 256])	cuda:0	True
2023-12-31 13:59:39,224 - INFO - model.bert.transformer_blocks.3.attention.proj.bias	torch.Size([256])	cuda:0	True
2023-12-31 13:59:39,224 - INFO - model.bert.transformer_blocks.3.attention.temporal_mat_bias_1.weight	torch.Size([64, 1])	cuda:0	True
2023-12-31 13:59:39,224 - INFO - model.bert.transformer_blocks.3.attention.temporal_mat_bias_1.bias	torch.Size([64])	cuda:0	True
2023-12-31 13:59:39,225 - INFO - model.bert.transformer_blocks.3.attention.temporal_mat_bias_2.weight	torch.Size([1, 64])	cuda:0	True
2023-12-31 13:59:39,225 - INFO - model.bert.transformer_blocks.3.attention.temporal_mat_bias_2.bias	torch.Size([1])	cuda:0	True
2023-12-31 13:59:39,225 - INFO - model.bert.transformer_blocks.3.mlp.fc1.weight	torch.Size([1024, 256])	cuda:0	True
2023-12-31 13:59:39,225 - INFO - model.bert.transformer_blocks.3.mlp.fc1.bias	torch.Size([1024])	cuda:0	True
2023-12-31 13:59:39,225 - INFO - model.bert.transformer_blocks.3.mlp.fc2.weight	torch.Size([256, 1024])	cuda:0	True
2023-12-31 13:59:39,225 - INFO - model.bert.transformer_blocks.3.mlp.fc2.bias	torch.Size([256])	cuda:0	True
2023-12-31 13:59:39,225 - INFO - model.bert.transformer_blocks.3.norm1.weight	torch.Size([256])	cuda:0	True
2023-12-31 13:59:39,225 - INFO - model.bert.transformer_blocks.3.norm1.bias	torch.Size([256])	cuda:0	True
2023-12-31 13:59:39,226 - INFO - model.bert.transformer_blocks.3.norm2.weight	torch.Size([256])	cuda:0	True
2023-12-31 13:59:39,226 - INFO - model.bert.transformer_blocks.3.norm2.bias	torch.Size([256])	cuda:0	True
2023-12-31 13:59:39,226 - INFO - model.bert.transformer_blocks.4.attention.linear_layers.0.weight	torch.Size([256, 256])	cuda:0	True
2023-12-31 13:59:39,226 - INFO - model.bert.transformer_blocks.4.attention.linear_layers.0.bias	torch.Size([256])	cuda:0	True
2023-12-31 13:59:39,226 - INFO - model.bert.transformer_blocks.4.attention.linear_layers.1.weight	torch.Size([256, 256])	cuda:0	True
2023-12-31 13:59:39,226 - INFO - model.bert.transformer_blocks.4.attention.linear_layers.1.bias	torch.Size([256])	cuda:0	True
2023-12-31 13:59:39,226 - INFO - model.bert.transformer_blocks.4.attention.linear_layers.2.weight	torch.Size([256, 256])	cuda:0	True
2023-12-31 13:59:39,226 - INFO - model.bert.transformer_blocks.4.attention.linear_layers.2.bias	torch.Size([256])	cuda:0	True
2023-12-31 13:59:39,226 - INFO - model.bert.transformer_blocks.4.attention.proj.weight	torch.Size([256, 256])	cuda:0	True
2023-12-31 13:59:39,226 - INFO - model.bert.transformer_blocks.4.attention.proj.bias	torch.Size([256])	cuda:0	True
2023-12-31 13:59:39,227 - INFO - model.bert.transformer_blocks.4.attention.temporal_mat_bias_1.weight	torch.Size([64, 1])	cuda:0	True
2023-12-31 13:59:39,227 - INFO - model.bert.transformer_blocks.4.attention.temporal_mat_bias_1.bias	torch.Size([64])	cuda:0	True
2023-12-31 13:59:39,227 - INFO - model.bert.transformer_blocks.4.attention.temporal_mat_bias_2.weight	torch.Size([1, 64])	cuda:0	True
2023-12-31 13:59:39,227 - INFO - model.bert.transformer_blocks.4.attention.temporal_mat_bias_2.bias	torch.Size([1])	cuda:0	True
2023-12-31 13:59:39,227 - INFO - model.bert.transformer_blocks.4.mlp.fc1.weight	torch.Size([1024, 256])	cuda:0	True
2023-12-31 13:59:39,227 - INFO - model.bert.transformer_blocks.4.mlp.fc1.bias	torch.Size([1024])	cuda:0	True
2023-12-31 13:59:39,227 - INFO - model.bert.transformer_blocks.4.mlp.fc2.weight	torch.Size([256, 1024])	cuda:0	True
2023-12-31 13:59:39,228 - INFO - model.bert.transformer_blocks.4.mlp.fc2.bias	torch.Size([256])	cuda:0	True
2023-12-31 13:59:39,228 - INFO - model.bert.transformer_blocks.4.norm1.weight	torch.Size([256])	cuda:0	True
2023-12-31 13:59:39,228 - INFO - model.bert.transformer_blocks.4.norm1.bias	torch.Size([256])	cuda:0	True
2023-12-31 13:59:39,228 - INFO - model.bert.transformer_blocks.4.norm2.weight	torch.Size([256])	cuda:0	True
2023-12-31 13:59:39,228 - INFO - model.bert.transformer_blocks.4.norm2.bias	torch.Size([256])	cuda:0	True
2023-12-31 13:59:39,228 - INFO - model.bert.transformer_blocks.5.attention.linear_layers.0.weight	torch.Size([256, 256])	cuda:0	True
2023-12-31 13:59:39,228 - INFO - model.bert.transformer_blocks.5.attention.linear_layers.0.bias	torch.Size([256])	cuda:0	True
2023-12-31 13:59:39,228 - INFO - model.bert.transformer_blocks.5.attention.linear_layers.1.weight	torch.Size([256, 256])	cuda:0	True
2023-12-31 13:59:39,228 - INFO - model.bert.transformer_blocks.5.attention.linear_layers.1.bias	torch.Size([256])	cuda:0	True
2023-12-31 13:59:39,229 - INFO - model.bert.transformer_blocks.5.attention.linear_layers.2.weight	torch.Size([256, 256])	cuda:0	True
2023-12-31 13:59:39,229 - INFO - model.bert.transformer_blocks.5.attention.linear_layers.2.bias	torch.Size([256])	cuda:0	True
2023-12-31 13:59:39,229 - INFO - model.bert.transformer_blocks.5.attention.proj.weight	torch.Size([256, 256])	cuda:0	True
2023-12-31 13:59:39,229 - INFO - model.bert.transformer_blocks.5.attention.proj.bias	torch.Size([256])	cuda:0	True
2023-12-31 13:59:39,229 - INFO - model.bert.transformer_blocks.5.attention.temporal_mat_bias_1.weight	torch.Size([64, 1])	cuda:0	True
2023-12-31 13:59:39,229 - INFO - model.bert.transformer_blocks.5.attention.temporal_mat_bias_1.bias	torch.Size([64])	cuda:0	True
2023-12-31 13:59:39,229 - INFO - model.bert.transformer_blocks.5.attention.temporal_mat_bias_2.weight	torch.Size([1, 64])	cuda:0	True
2023-12-31 13:59:39,229 - INFO - model.bert.transformer_blocks.5.attention.temporal_mat_bias_2.bias	torch.Size([1])	cuda:0	True
2023-12-31 13:59:39,229 - INFO - model.bert.transformer_blocks.5.mlp.fc1.weight	torch.Size([1024, 256])	cuda:0	True
2023-12-31 13:59:39,230 - INFO - model.bert.transformer_blocks.5.mlp.fc1.bias	torch.Size([1024])	cuda:0	True
2023-12-31 13:59:39,230 - INFO - model.bert.transformer_blocks.5.mlp.fc2.weight	torch.Size([256, 1024])	cuda:0	True
2023-12-31 13:59:39,230 - INFO - model.bert.transformer_blocks.5.mlp.fc2.bias	torch.Size([256])	cuda:0	True
2023-12-31 13:59:39,230 - INFO - model.bert.transformer_blocks.5.norm1.weight	torch.Size([256])	cuda:0	True
2023-12-31 13:59:39,230 - INFO - model.bert.transformer_blocks.5.norm1.bias	torch.Size([256])	cuda:0	True
2023-12-31 13:59:39,230 - INFO - model.bert.transformer_blocks.5.norm2.weight	torch.Size([256])	cuda:0	True
2023-12-31 13:59:39,230 - INFO - model.bert.transformer_blocks.5.norm2.bias	torch.Size([256])	cuda:0	True
2023-12-31 13:59:39,230 - INFO - linear.weight	torch.Size([1, 256])	cuda:0	True
2023-12-31 13:59:39,230 - INFO - linear.bias	torch.Size([1])	cuda:0	True
2023-12-31 13:59:39,231 - INFO - Total parameter numbers: 5319175
2023-12-31 13:59:39,231 - INFO - You select `adamw` optimizer.
2023-12-31 13:59:39,232 - INFO - You select `cosinelr` lr_scheduler.
2023-12-31 13:59:39,272 - INFO - Load Pretrained-Model from libcity/cache/135178/model_cache/135178_BERTContrastiveLM_bj.pt
2023-12-31 13:59:39,290 - INFO - Start training ...
2023-12-31 13:59:39,290 - INFO - Num_batches: train=15, eval=5
2023-12-31 13:59:39,883 - INFO - {'mode': 'Train', 'epoch': 0, 'iter': 0, 'lr': 1e-06, 'loss': 326.4132080078125}
2023-12-31 13:59:40,686 - INFO - Train: expid = 247022, Epoch = 0, avg_loss = 268.79534084514034.
2023-12-31 13:59:40,687 - INFO - epoch complete!
2023-12-31 13:59:40,687 - INFO - evaluating now!
2023-12-31 13:59:40,703 - INFO - {'mode': 'Eval', 'epoch': 0, 'iter': 0, 'lr': 1e-06, 'loss': 207.45425415039062}
2023-12-31 13:59:40,762 - INFO - Eval: expid = 247022, Epoch = 0, avg_loss = 216.24722290039062.
2023-12-31 13:59:40,762 - INFO - Epoch [0/15] (15)  train_loss: 268.7953, val_loss: 216.2472, lr: 0.000051, 1.47s
2023-12-31 13:59:40,941 - INFO - Saved model at 0
2023-12-31 13:59:40,941 - INFO - Val loss decrease from inf to 216.2472, saving to ./libcity/cache/247022/model_cache/LinearETA_bj_epoch0.tar
2023-12-31 13:59:40,998 - INFO - {'mode': 'Train', 'epoch': 1, 'iter': 0, 'lr': 5.075e-05, 'loss': 147.91873168945312}
2023-12-31 13:59:41,809 - INFO - Train: expid = 247022, Epoch = 1, avg_loss = 176.45208235918466.
2023-12-31 13:59:41,809 - INFO - epoch complete!
2023-12-31 13:59:41,809 - INFO - evaluating now!
2023-12-31 13:59:41,825 - INFO - {'mode': 'Eval', 'epoch': 1, 'iter': 0, 'lr': 5.075e-05, 'loss': 140.9573211669922}
2023-12-31 13:59:41,885 - INFO - Eval: expid = 247022, Epoch = 1, avg_loss = 82.25690956115723.
2023-12-31 13:59:41,886 - INFO - Epoch [1/15] (30)  train_loss: 176.4521, val_loss: 82.2569, lr: 0.000101, 0.94s
2023-12-31 13:59:42,056 - INFO - Saved model at 1
2023-12-31 13:59:42,056 - INFO - Val loss decrease from 216.2472 to 82.2569, saving to ./libcity/cache/247022/model_cache/LinearETA_bj_epoch1.tar
2023-12-31 13:59:42,114 - INFO - {'mode': 'Train', 'epoch': 2, 'iter': 0, 'lr': 0.0001005, 'loss': 233.36236572265625}
2023-12-31 13:59:42,924 - INFO - Train: expid = 247022, Epoch = 2, avg_loss = 117.56828101206634.
2023-12-31 13:59:42,924 - INFO - epoch complete!
2023-12-31 13:59:42,924 - INFO - evaluating now!
2023-12-31 13:59:42,940 - INFO - {'mode': 'Eval', 'epoch': 2, 'iter': 0, 'lr': 0.0001005, 'loss': 29.50184440612793}
2023-12-31 13:59:43,000 - INFO - Eval: expid = 247022, Epoch = 2, avg_loss = 71.53115348815918.
2023-12-31 13:59:43,000 - INFO - Epoch [2/15] (45)  train_loss: 117.5683, val_loss: 71.5312, lr: 0.000150, 0.94s
2023-12-31 13:59:43,170 - INFO - Saved model at 2
2023-12-31 13:59:43,170 - INFO - Val loss decrease from 82.2569 to 71.5312, saving to ./libcity/cache/247022/model_cache/LinearETA_bj_epoch2.tar
2023-12-31 13:59:43,240 - INFO - {'mode': 'Train', 'epoch': 3, 'iter': 0, 'lr': 0.00015025000000000002, 'loss': 88.22313690185547}
2023-12-31 13:59:44,048 - INFO - Train: expid = 247022, Epoch = 3, avg_loss = 107.58276347790735.
2023-12-31 13:59:44,049 - INFO - epoch complete!
2023-12-31 13:59:44,049 - INFO - evaluating now!
2023-12-31 13:59:44,065 - INFO - {'mode': 'Eval', 'epoch': 3, 'iter': 0, 'lr': 0.00015025000000000002, 'loss': 61.47201156616211}
2023-12-31 13:59:44,125 - INFO - Eval: expid = 247022, Epoch = 3, avg_loss = 65.05961151123047.
2023-12-31 13:59:44,125 - INFO - Epoch [3/15] (60)  train_loss: 107.5828, val_loss: 65.0596, lr: 0.000167, 0.96s
2023-12-31 13:59:44,296 - INFO - Saved model at 3
2023-12-31 13:59:44,297 - INFO - Val loss decrease from 71.5312 to 65.0596, saving to ./libcity/cache/247022/model_cache/LinearETA_bj_epoch3.tar
2023-12-31 13:59:44,354 - INFO - {'mode': 'Train', 'epoch': 4, 'iter': 0, 'lr': 0.00016691306063588583, 'loss': 11.704365730285645}
2023-12-31 13:59:45,162 - INFO - Train: expid = 247022, Epoch = 4, avg_loss = 97.87093204562947.
2023-12-31 13:59:45,162 - INFO - epoch complete!
2023-12-31 13:59:45,163 - INFO - evaluating now!
2023-12-31 13:59:45,179 - INFO - {'mode': 'Eval', 'epoch': 4, 'iter': 0, 'lr': 0.00016691306063588583, 'loss': 28.82798194885254}
2023-12-31 13:59:45,240 - INFO - Eval: expid = 247022, Epoch = 4, avg_loss = 60.77004337310791.
2023-12-31 13:59:45,240 - INFO - Epoch [4/15] (75)  train_loss: 97.8709, val_loss: 60.7700, lr: 0.000150, 0.94s
2023-12-31 13:59:45,414 - INFO - Saved model at 4
2023-12-31 13:59:45,414 - INFO - Val loss decrease from 65.0596 to 60.7700, saving to ./libcity/cache/247022/model_cache/LinearETA_bj_epoch4.tar
2023-12-31 13:59:45,471 - INFO - {'mode': 'Train', 'epoch': 5, 'iter': 0, 'lr': 0.00015000000000000001, 'loss': 61.42287063598633}
2023-12-31 13:59:46,262 - INFO - Train: expid = 247022, Epoch = 5, avg_loss = 92.77941325559455.
2023-12-31 13:59:46,262 - INFO - epoch complete!
2023-12-31 13:59:46,262 - INFO - evaluating now!
2023-12-31 13:59:46,278 - INFO - {'mode': 'Eval', 'epoch': 5, 'iter': 0, 'lr': 0.00015000000000000001, 'loss': 12.421684265136719}
2023-12-31 13:59:46,337 - INFO - Eval: expid = 247022, Epoch = 5, avg_loss = 54.42309417724609.
2023-12-31 13:59:46,338 - INFO - Epoch [5/15] (90)  train_loss: 92.7794, val_loss: 54.4231, lr: 0.000131, 0.92s
2023-12-31 13:59:46,506 - INFO - Saved model at 5
2023-12-31 13:59:46,506 - INFO - Val loss decrease from 60.7700 to 54.4231, saving to ./libcity/cache/247022/model_cache/LinearETA_bj_epoch5.tar
2023-12-31 13:59:46,564 - INFO - {'mode': 'Train', 'epoch': 6, 'iter': 0, 'lr': 0.00013090169943749476, 'loss': 40.17106628417969}
2023-12-31 13:59:47,348 - INFO - Train: expid = 247022, Epoch = 6, avg_loss = 87.59424552270922.
2023-12-31 13:59:47,349 - INFO - epoch complete!
2023-12-31 13:59:47,349 - INFO - evaluating now!
2023-12-31 13:59:47,365 - INFO - {'mode': 'Eval', 'epoch': 6, 'iter': 0, 'lr': 0.00013090169943749476, 'loss': 16.48307991027832}
2023-12-31 13:59:47,425 - INFO - Eval: expid = 247022, Epoch = 6, avg_loss = 50.904991149902344.
2023-12-31 13:59:47,425 - INFO - Epoch [6/15] (105)  train_loss: 87.5942, val_loss: 50.9050, lr: 0.000110, 0.92s
2023-12-31 13:59:47,591 - INFO - Saved model at 6
2023-12-31 13:59:47,591 - INFO - Val loss decrease from 54.4231 to 50.9050, saving to ./libcity/cache/247022/model_cache/LinearETA_bj_epoch6.tar
2023-12-31 13:59:47,649 - INFO - {'mode': 'Train', 'epoch': 7, 'iter': 0, 'lr': 0.00011045284632676536, 'loss': 276.5498046875}
2023-12-31 13:59:48,460 - INFO - Train: expid = 247022, Epoch = 7, avg_loss = 84.29898541660633.
2023-12-31 13:59:48,460 - INFO - epoch complete!
2023-12-31 13:59:48,460 - INFO - evaluating now!
2023-12-31 13:59:48,477 - INFO - {'mode': 'Eval', 'epoch': 7, 'iter': 0, 'lr': 0.00011045284632676536, 'loss': 39.97931671142578}
2023-12-31 13:59:48,537 - INFO - Eval: expid = 247022, Epoch = 7, avg_loss = 48.828483200073244.
2023-12-31 13:59:48,537 - INFO - Epoch [7/15] (120)  train_loss: 84.2990, val_loss: 48.8285, lr: 0.000090, 0.95s
2023-12-31 13:59:48,709 - INFO - Saved model at 7
2023-12-31 13:59:48,709 - INFO - Val loss decrease from 50.9050 to 48.8285, saving to ./libcity/cache/247022/model_cache/LinearETA_bj_epoch7.tar
2023-12-31 13:59:48,766 - INFO - {'mode': 'Train', 'epoch': 8, 'iter': 0, 'lr': 8.954715367323468e-05, 'loss': 97.7427978515625}
2023-12-31 13:59:49,563 - INFO - Train: expid = 247022, Epoch = 8, avg_loss = 81.88243542687367.
2023-12-31 13:59:49,564 - INFO - epoch complete!
2023-12-31 13:59:49,564 - INFO - evaluating now!
2023-12-31 13:59:49,580 - INFO - {'mode': 'Eval', 'epoch': 8, 'iter': 0, 'lr': 8.954715367323468e-05, 'loss': 21.78339195251465}
2023-12-31 13:59:49,640 - INFO - Eval: expid = 247022, Epoch = 8, avg_loss = 47.10766792297363.
2023-12-31 13:59:49,641 - INFO - Epoch [8/15] (135)  train_loss: 81.8824, val_loss: 47.1077, lr: 0.000069, 0.93s
2023-12-31 13:59:49,810 - INFO - Saved model at 8
2023-12-31 13:59:49,810 - INFO - Val loss decrease from 48.8285 to 47.1077, saving to ./libcity/cache/247022/model_cache/LinearETA_bj_epoch8.tar
2023-12-31 13:59:49,873 - INFO - {'mode': 'Train', 'epoch': 9, 'iter': 0, 'lr': 6.909830056250527e-05, 'loss': 71.84503936767578}
2023-12-31 13:59:50,657 - INFO - Train: expid = 247022, Epoch = 9, avg_loss = 80.01698264429125.
2023-12-31 13:59:50,658 - INFO - epoch complete!
2023-12-31 13:59:50,658 - INFO - evaluating now!
2023-12-31 13:59:50,674 - INFO - {'mode': 'Eval', 'epoch': 9, 'iter': 0, 'lr': 6.909830056250527e-05, 'loss': 104.13580322265625}
2023-12-31 13:59:50,736 - INFO - Eval: expid = 247022, Epoch = 9, avg_loss = 45.98889007568359.
2023-12-31 13:59:50,737 - INFO - Epoch [9/15] (150)  train_loss: 80.0170, val_loss: 45.9889, lr: 0.000050, 0.93s
2023-12-31 13:59:50,912 - INFO - Saved model at 9
2023-12-31 13:59:50,912 - INFO - Val loss decrease from 47.1077 to 45.9889, saving to ./libcity/cache/247022/model_cache/LinearETA_bj_epoch9.tar
2023-12-31 13:59:50,974 - INFO - {'mode': 'Train', 'epoch': 10, 'iter': 0, 'lr': 5.000000000000002e-05, 'loss': 7.690018653869629}
2023-12-31 13:59:51,778 - INFO - Train: expid = 247022, Epoch = 10, avg_loss = 77.76129221512099.
2023-12-31 13:59:51,778 - INFO - epoch complete!
2023-12-31 13:59:51,779 - INFO - evaluating now!
2023-12-31 13:59:51,795 - INFO - {'mode': 'Eval', 'epoch': 10, 'iter': 0, 'lr': 5.000000000000002e-05, 'loss': 36.982093811035156}
2023-12-31 13:59:51,856 - INFO - Eval: expid = 247022, Epoch = 10, avg_loss = 44.89889907836914.
2023-12-31 13:59:51,856 - INFO - Epoch [10/15] (165)  train_loss: 77.7613, val_loss: 44.8989, lr: 0.000033, 0.94s
2023-12-31 13:59:52,023 - INFO - Saved model at 10
2023-12-31 13:59:52,023 - INFO - Val loss decrease from 45.9889 to 44.8989, saving to ./libcity/cache/247022/model_cache/LinearETA_bj_epoch10.tar
2023-12-31 13:59:52,086 - INFO - {'mode': 'Train', 'epoch': 11, 'iter': 0, 'lr': 3.308693936411421e-05, 'loss': 89.04163360595703}
2023-12-31 13:59:52,895 - INFO - Train: expid = 247022, Epoch = 11, avg_loss = 75.78845822609077.
2023-12-31 13:59:52,895 - INFO - epoch complete!
2023-12-31 13:59:52,895 - INFO - evaluating now!
2023-12-31 13:59:52,911 - INFO - {'mode': 'Eval', 'epoch': 11, 'iter': 0, 'lr': 3.308693936411421e-05, 'loss': 38.373416900634766}
2023-12-31 13:59:52,971 - INFO - Eval: expid = 247022, Epoch = 11, avg_loss = 44.571501922607425.
2023-12-31 13:59:52,971 - INFO - Epoch [11/15] (180)  train_loss: 75.7885, val_loss: 44.5715, lr: 0.000019, 0.95s
2023-12-31 13:59:53,139 - INFO - Saved model at 11
2023-12-31 13:59:53,139 - INFO - Val loss decrease from 44.8989 to 44.5715, saving to ./libcity/cache/247022/model_cache/LinearETA_bj_epoch11.tar
2023-12-31 13:59:53,200 - INFO - {'mode': 'Train', 'epoch': 12, 'iter': 0, 'lr': 1.9098300562505266e-05, 'loss': 14.143499374389648}
2023-12-31 13:59:54,003 - INFO - Train: expid = 247022, Epoch = 12, avg_loss = 75.21523433620646.
2023-12-31 13:59:54,004 - INFO - epoch complete!
2023-12-31 13:59:54,004 - INFO - evaluating now!
2023-12-31 13:59:54,021 - INFO - {'mode': 'Eval', 'epoch': 12, 'iter': 0, 'lr': 1.9098300562505266e-05, 'loss': 16.060853958129883}
2023-12-31 13:59:54,081 - INFO - Eval: expid = 247022, Epoch = 12, avg_loss = 44.06418571472168.
2023-12-31 13:59:54,081 - INFO - Epoch [12/15] (195)  train_loss: 75.2152, val_loss: 44.0642, lr: 0.000009, 0.94s
2023-12-31 13:59:54,286 - INFO - Saved model at 12
2023-12-31 13:59:54,286 - INFO - Val loss decrease from 44.5715 to 44.0642, saving to ./libcity/cache/247022/model_cache/LinearETA_bj_epoch12.tar
2023-12-31 13:59:54,344 - INFO - {'mode': 'Train', 'epoch': 13, 'iter': 0, 'lr': 8.645454235739903e-06, 'loss': 68.6108627319336}
2023-12-31 13:59:55,146 - INFO - Train: expid = 247022, Epoch = 13, avg_loss = 74.71118868811656.
2023-12-31 13:59:55,146 - INFO - epoch complete!
2023-12-31 13:59:55,146 - INFO - evaluating now!
2023-12-31 13:59:55,162 - INFO - {'mode': 'Eval', 'epoch': 13, 'iter': 0, 'lr': 8.645454235739903e-06, 'loss': 6.7373809814453125}
2023-12-31 13:59:55,223 - INFO - Eval: expid = 247022, Epoch = 13, avg_loss = 43.9340259552002.
2023-12-31 13:59:55,223 - INFO - Epoch [13/15] (210)  train_loss: 74.7112, val_loss: 43.9340, lr: 0.000002, 0.94s
2023-12-31 13:59:55,391 - INFO - Saved model at 13
2023-12-31 13:59:55,391 - INFO - Val loss decrease from 44.0642 to 43.9340, saving to ./libcity/cache/247022/model_cache/LinearETA_bj_epoch13.tar
2023-12-31 13:59:55,449 - INFO - {'mode': 'Train', 'epoch': 14, 'iter': 0, 'lr': 2.1852399266194314e-06, 'loss': 54.50277328491211}
2023-12-31 13:59:56,271 - INFO - Train: expid = 247022, Epoch = 14, avg_loss = 74.0906267004498.
2023-12-31 13:59:56,271 - INFO - epoch complete!
2023-12-31 13:59:56,272 - INFO - evaluating now!
2023-12-31 13:59:56,288 - INFO - {'mode': 'Eval', 'epoch': 14, 'iter': 0, 'lr': 2.1852399266194314e-06, 'loss': 19.096065521240234}
2023-12-31 13:59:56,347 - INFO - Eval: expid = 247022, Epoch = 14, avg_loss = 43.85722122192383.
2023-12-31 13:59:56,347 - INFO - Epoch [14/15] (225)  train_loss: 74.0906, val_loss: 43.8572, lr: 0.000020, 0.96s
2023-12-31 13:59:56,519 - INFO - Saved model at 14
2023-12-31 13:59:56,519 - INFO - Val loss decrease from 43.9340 to 43.8572, saving to ./libcity/cache/247022/model_cache/LinearETA_bj_epoch14.tar
2023-12-31 13:59:56,519 - INFO - Trained totally 15 epochs, average train time is 0.898s, average eval time is 0.076s
2023-12-31 13:59:56,601 - INFO - Loaded model at 14
2023-12-31 13:59:56,667 - INFO - Save png at ./libcity/cache/247022/247022_loss.png
2023-12-31 13:59:56,727 - INFO - Save png at ./libcity/cache/247022/247022_lr.png
2023-12-31 13:59:56,855 - INFO - Saved model at ./libcity/cache/247022/model_cache/247022_LinearETA_bj.pt
2023-12-31 13:59:56,855 - INFO - Start evaluating ...
2023-12-31 13:59:56,874 - INFO - {'mode': 'Test', 'epoch': 0, 'iter': 0, 'lr': 2e-05, 'loss': 23.122224807739258}
2023-12-31 13:59:56,943 - INFO - Test: expid = 247022, Epoch = 0, avg_loss = 47.457957649230956.
2023-12-31 13:59:56,944 - INFO - Evaluate result is {"MAE": 4.781984901428222, "RMSE": 6.144947814941406, "MAPE": 0.4433067560195923, "R2": 0.23961296648416744, "EVAR": 0.3447351694107056}
2023-12-31 13:59:56,945 - INFO - Evaluate result is saved at ./libcity/cache/247022/evaluate_cache\247022_2023_12_31_13_59_56_LinearETA_bj.json
2023-12-31 13:59:56,945 - INFO - 
{
 "MAE": 4.781984901428222,
 "RMSE": 6.144947814941406,
 "MAPE": 0.4433067560195923,
 "R2": 0.23961296648416744,
 "EVAR": 0.3447351694107056
}
2023-12-31 13:59:56,948 - INFO - Evaluate result is saved at ./libcity/cache/247022/evaluate_cache\247022_2023_12_31_13_59_56_LinearETA_bj.csv
2023-12-31 13:59:56,953 - INFO - 
        MAE      RMSE      MAPE        R2      EVAR
1  4.781985  6.144948  0.443307  0.239613  0.344735
2023-12-31 13:59:56,953 - INFO - Test time 0.09732246398925781s.
