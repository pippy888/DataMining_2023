2024-01-02 18:13:20,301 - INFO - Log directory: ./libcity/log
2024-01-02 18:13:20,301 - INFO - Begin pretrain-pipeline, task=trajectory_embedding, model_name=LinearNextLoc, dataset_name=bj, exp_id=230271
2024-01-02 18:13:20,301 - INFO - {'task': 'trajectory_embedding', 'model': 'LinearNextLoc', 'dataset': 'bj', 'saved_model': True, 'train': True, 'gpu': True, 'gpu_id': 0, 'pretrain_path': 'libcity/cache/454824/model_cache/454824_BERTContrastiveLM_bj.pt', 'mlm_ratio': 0.6, 'n_layers': 6, 'd_model': 256, 'attn_heads': 8, 'max_epoch': 30, 'batch_size': 8, 'grad_accmu_steps': 1, 'learning_rate': 0.0002, 'roadnetwork': 'bj_roadmap_edge_bj_True_1_merge', 'geo_file': 'bj_roadmap_edge_bj_True_1_merge_withdegree', 'rel_file': 'bj_roadmap_edge_bj_True_1_merge_withdegree', 'merge': True, 'min_freq': 1, 'seq_len': 128, 'test_every': 50, 'temperature': 0.05, 'contra_loss_type': 'simclr', 'classify_label': 'vflag', 'type_ln': 'post', 'add_cls': True, 'add_time_in_day': True, 'add_day_in_week': True, 'add_pe': True, 'add_temporal_bias': True, 'temporal_bias_dim': 64, 'use_mins_interval': False, 'add_gat': True, 'gat_heads_per_layer': [8, 16, 1], 'gat_features_per_layer': [16, 16, 256], 'gat_dropout': 0.1, 'gat_K': 1, 'gat_avg_last': True, 'load_trans_prob': True, 'append_degree2gcn': True, 'normal_feature': False, 'pooling': 'cls', 'dataset_class': 'NextLocDataset', 'executor': 'NextLocExecutor', 'evaluator': 'RegressionEvaluator', 'num_workers': 0, 'vocab_path': None, 'mlp_ratio': 4, 'pretrain_road_emb': None, 'future_mask': False, 'load_node2vec': False, 'dropout': 0.1, 'drop_path': 0.3, 'attn_drop': 0.1, 'seed': 0, 'learner': 'adamw', 'lr_eta_min': 0, 'lr_warmup_epoch': 4, 'lr_warmup_init': 1e-06, 'lr_decay': True, 'lr_scheduler': 'cosinelr', 'lr_decay_ratio': 0.1, 't_in_epochs': True, 'clip_grad_norm': True, 'max_grad_norm': 5, 'use_early_stop': True, 'patience': 10, 'log_batch': 500, 'log_every': 1, 'l2_reg': None, 'bidir_adj_mx': False, 'freeze': False, 'metrics': ['MAE', 'RMSE', 'MAPE', 'R2', 'EVAR'], 'save_modes': ['csv', 'json'], 'device': device(type='cuda', index=0), 'exp_id': 230271}
2024-01-02 18:13:20,815 - INFO - Loading Vocab from raw_data/vocab_bj_True_1_merge.pkl
2024-01-02 18:13:20,822 - INFO - vocab_path=raw_data/vocab_bj_True_1_merge.pkl, usr_num=75, vocab_size=27485
2024-01-02 18:13:20,872 - INFO - Loaded file bj_roadmap_edge_bj_True_1_merge_withdegree.geo, num_nodes=27481
2024-01-02 18:13:22,270 - INFO - Loaded file bj_roadmap_edge_bj_True_1_merge_withdegree.rel, shape=(27481, 27481), edges=51763.0
2024-01-02 18:13:22,275 - INFO - node_features: (27481, 42)
2024-01-02 18:13:23,290 - INFO - node_features_encoded: torch.Size([27485, 42])
2024-01-02 18:13:23,439 - INFO - edge_index: torch.Size([2, 79209])
2024-01-02 18:13:23,443 - INFO - Trajectory loc-transfer prob shape=torch.Size([79209, 1])
2024-01-02 18:13:23,454 - INFO - Loading Dataset!
2024-01-02 18:13:23,755 - INFO - Processing dataset in TrajectoryProcessingDataset!
2024-01-02 18:26:53,124 - INFO - Processing dataset in TrajectoryProcessingDataset!
2024-01-02 18:31:24,027 - INFO - Processing dataset in TrajectoryProcessingDataset!
2024-01-02 18:33:17,286 - INFO - Size of dataset: 13138/4380/1938
2024-01-02 18:33:17,286 - INFO - Creating Dataloader!
2024-01-02 18:33:17,288 - INFO - Building Downstream LinearNextLoc model
2024-01-02 18:33:17,289 - INFO - Building BERTDownstream model
2024-01-02 18:33:18,108 - INFO - LinearNextLoc(
  (model): BERTDownstream(
    (bert): BERT(
      (embedding): BERTEmbedding(
        (token_embedding): GAT(
          (gat_net): Sequential(
            (0): GATLayerImp3(
              (linear_proj): Linear(in_features=42, out_features=128, bias=False)
              (linear_proj_tran_prob): Linear(in_features=1, out_features=128, bias=False)
              (skip_proj): Linear(in_features=42, out_features=128, bias=False)
              (leakyReLU): LeakyReLU(negative_slope=0.2)
              (softmax): Softmax(dim=-1)
              (activation): ELU(alpha=1.0)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (1): GATLayerImp3(
              (linear_proj): Linear(in_features=128, out_features=256, bias=False)
              (linear_proj_tran_prob): Linear(in_features=1, out_features=256, bias=False)
              (skip_proj): Linear(in_features=128, out_features=256, bias=False)
              (leakyReLU): LeakyReLU(negative_slope=0.2)
              (softmax): Softmax(dim=-1)
              (activation): ELU(alpha=1.0)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (2): GATLayerImp3(
              (linear_proj): Linear(in_features=256, out_features=256, bias=False)
              (linear_proj_tran_prob): Linear(in_features=1, out_features=256, bias=False)
              (skip_proj): Linear(in_features=256, out_features=256, bias=False)
              (leakyReLU): LeakyReLU(negative_slope=0.2)
              (softmax): Softmax(dim=-1)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
        )
        (position_embedding): PositionalEmbedding()
        (daytime_embedding): Embedding(1441, 256, padding_idx=0)
        (weekday_embedding): Embedding(8, 256, padding_idx=0)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (transformer_blocks): ModuleList(
        (0): TransformerBlock(
          (attention): MultiHeadedAttention(
            (linear_layers): ModuleList(
              (0): Linear(in_features=256, out_features=256, bias=True)
              (1): Linear(in_features=256, out_features=256, bias=True)
              (2): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout): Dropout(p=0.1, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0.1, inplace=False)
            (temporal_mat_bias_1): Linear(in_features=1, out_features=64, bias=True)
            (temporal_mat_bias_2): Linear(in_features=64, out_features=1, bias=True)
          )
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0.1, inplace=False)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (drop_path): Identity()
        )
        (1): TransformerBlock(
          (attention): MultiHeadedAttention(
            (linear_layers): ModuleList(
              (0): Linear(in_features=256, out_features=256, bias=True)
              (1): Linear(in_features=256, out_features=256, bias=True)
              (2): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout): Dropout(p=0.1, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0.1, inplace=False)
            (temporal_mat_bias_1): Linear(in_features=1, out_features=64, bias=True)
            (temporal_mat_bias_2): Linear(in_features=64, out_features=1, bias=True)
          )
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0.1, inplace=False)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (drop_path): DropPath()
        )
        (2): TransformerBlock(
          (attention): MultiHeadedAttention(
            (linear_layers): ModuleList(
              (0): Linear(in_features=256, out_features=256, bias=True)
              (1): Linear(in_features=256, out_features=256, bias=True)
              (2): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout): Dropout(p=0.1, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0.1, inplace=False)
            (temporal_mat_bias_1): Linear(in_features=1, out_features=64, bias=True)
            (temporal_mat_bias_2): Linear(in_features=64, out_features=1, bias=True)
          )
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0.1, inplace=False)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (drop_path): DropPath()
        )
        (3): TransformerBlock(
          (attention): MultiHeadedAttention(
            (linear_layers): ModuleList(
              (0): Linear(in_features=256, out_features=256, bias=True)
              (1): Linear(in_features=256, out_features=256, bias=True)
              (2): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout): Dropout(p=0.1, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0.1, inplace=False)
            (temporal_mat_bias_1): Linear(in_features=1, out_features=64, bias=True)
            (temporal_mat_bias_2): Linear(in_features=64, out_features=1, bias=True)
          )
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0.1, inplace=False)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (drop_path): DropPath()
        )
        (4): TransformerBlock(
          (attention): MultiHeadedAttention(
            (linear_layers): ModuleList(
              (0): Linear(in_features=256, out_features=256, bias=True)
              (1): Linear(in_features=256, out_features=256, bias=True)
              (2): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout): Dropout(p=0.1, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0.1, inplace=False)
            (temporal_mat_bias_1): Linear(in_features=1, out_features=64, bias=True)
            (temporal_mat_bias_2): Linear(in_features=64, out_features=1, bias=True)
          )
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0.1, inplace=False)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (drop_path): DropPath()
        )
        (5): TransformerBlock(
          (attention): MultiHeadedAttention(
            (linear_layers): ModuleList(
              (0): Linear(in_features=256, out_features=256, bias=True)
              (1): Linear(in_features=256, out_features=256, bias=True)
              (2): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout): Dropout(p=0.1, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0.1, inplace=False)
            (temporal_mat_bias_1): Linear(in_features=1, out_features=64, bias=True)
            (temporal_mat_bias_2): Linear(in_features=64, out_features=1, bias=True)
          )
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0.1, inplace=False)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (drop_path): DropPath()
        )
      )
    )
  )
  (linear): Linear(in_features=256, out_features=27485, bias=True)
  (softmax): LogSoftmax(dim=-1)
)
2024-01-02 18:33:18,111 - INFO - model.bert.embedding.token_embedding.gat_net.0.scoring_fn_target	torch.Size([1, 8, 16])	cuda:0	True
2024-01-02 18:33:18,112 - INFO - model.bert.embedding.token_embedding.gat_net.0.scoring_fn_source	torch.Size([1, 8, 16])	cuda:0	True
2024-01-02 18:33:18,112 - INFO - model.bert.embedding.token_embedding.gat_net.0.scoring_trans_prob	torch.Size([1, 8, 16])	cuda:0	True
2024-01-02 18:33:18,112 - INFO - model.bert.embedding.token_embedding.gat_net.0.bias	torch.Size([128])	cuda:0	True
2024-01-02 18:33:18,112 - INFO - model.bert.embedding.token_embedding.gat_net.0.linear_proj.weight	torch.Size([128, 42])	cuda:0	True
2024-01-02 18:33:18,112 - INFO - model.bert.embedding.token_embedding.gat_net.0.linear_proj_tran_prob.weight	torch.Size([128, 1])	cuda:0	True
2024-01-02 18:33:18,112 - INFO - model.bert.embedding.token_embedding.gat_net.0.skip_proj.weight	torch.Size([128, 42])	cuda:0	True
2024-01-02 18:33:18,112 - INFO - model.bert.embedding.token_embedding.gat_net.1.scoring_fn_target	torch.Size([1, 16, 16])	cuda:0	True
2024-01-02 18:33:18,113 - INFO - model.bert.embedding.token_embedding.gat_net.1.scoring_fn_source	torch.Size([1, 16, 16])	cuda:0	True
2024-01-02 18:33:18,113 - INFO - model.bert.embedding.token_embedding.gat_net.1.scoring_trans_prob	torch.Size([1, 16, 16])	cuda:0	True
2024-01-02 18:33:18,113 - INFO - model.bert.embedding.token_embedding.gat_net.1.bias	torch.Size([256])	cuda:0	True
2024-01-02 18:33:18,113 - INFO - model.bert.embedding.token_embedding.gat_net.1.linear_proj.weight	torch.Size([256, 128])	cuda:0	True
2024-01-02 18:33:18,113 - INFO - model.bert.embedding.token_embedding.gat_net.1.linear_proj_tran_prob.weight	torch.Size([256, 1])	cuda:0	True
2024-01-02 18:33:18,113 - INFO - model.bert.embedding.token_embedding.gat_net.1.skip_proj.weight	torch.Size([256, 128])	cuda:0	True
2024-01-02 18:33:18,113 - INFO - model.bert.embedding.token_embedding.gat_net.2.scoring_fn_target	torch.Size([1, 1, 256])	cuda:0	True
2024-01-02 18:33:18,114 - INFO - model.bert.embedding.token_embedding.gat_net.2.scoring_fn_source	torch.Size([1, 1, 256])	cuda:0	True
2024-01-02 18:33:18,114 - INFO - model.bert.embedding.token_embedding.gat_net.2.scoring_trans_prob	torch.Size([1, 1, 256])	cuda:0	True
2024-01-02 18:33:18,114 - INFO - model.bert.embedding.token_embedding.gat_net.2.bias	torch.Size([256])	cuda:0	True
2024-01-02 18:33:18,114 - INFO - model.bert.embedding.token_embedding.gat_net.2.linear_proj.weight	torch.Size([256, 256])	cuda:0	True
2024-01-02 18:33:18,114 - INFO - model.bert.embedding.token_embedding.gat_net.2.linear_proj_tran_prob.weight	torch.Size([256, 1])	cuda:0	True
2024-01-02 18:33:18,114 - INFO - model.bert.embedding.token_embedding.gat_net.2.skip_proj.weight	torch.Size([256, 256])	cuda:0	True
2024-01-02 18:33:18,114 - INFO - model.bert.embedding.daytime_embedding.weight	torch.Size([1441, 256])	cuda:0	True
2024-01-02 18:33:18,115 - INFO - model.bert.embedding.weekday_embedding.weight	torch.Size([8, 256])	cuda:0	True
2024-01-02 18:33:18,115 - INFO - model.bert.transformer_blocks.0.attention.linear_layers.0.weight	torch.Size([256, 256])	cuda:0	True
2024-01-02 18:33:18,115 - INFO - model.bert.transformer_blocks.0.attention.linear_layers.0.bias	torch.Size([256])	cuda:0	True
2024-01-02 18:33:18,115 - INFO - model.bert.transformer_blocks.0.attention.linear_layers.1.weight	torch.Size([256, 256])	cuda:0	True
2024-01-02 18:33:18,115 - INFO - model.bert.transformer_blocks.0.attention.linear_layers.1.bias	torch.Size([256])	cuda:0	True
2024-01-02 18:33:18,115 - INFO - model.bert.transformer_blocks.0.attention.linear_layers.2.weight	torch.Size([256, 256])	cuda:0	True
2024-01-02 18:33:18,115 - INFO - model.bert.transformer_blocks.0.attention.linear_layers.2.bias	torch.Size([256])	cuda:0	True
2024-01-02 18:33:18,115 - INFO - model.bert.transformer_blocks.0.attention.proj.weight	torch.Size([256, 256])	cuda:0	True
2024-01-02 18:33:18,115 - INFO - model.bert.transformer_blocks.0.attention.proj.bias	torch.Size([256])	cuda:0	True
2024-01-02 18:33:18,116 - INFO - model.bert.transformer_blocks.0.attention.temporal_mat_bias_1.weight	torch.Size([64, 1])	cuda:0	True
2024-01-02 18:33:18,116 - INFO - model.bert.transformer_blocks.0.attention.temporal_mat_bias_1.bias	torch.Size([64])	cuda:0	True
2024-01-02 18:33:18,116 - INFO - model.bert.transformer_blocks.0.attention.temporal_mat_bias_2.weight	torch.Size([1, 64])	cuda:0	True
2024-01-02 18:33:18,116 - INFO - model.bert.transformer_blocks.0.attention.temporal_mat_bias_2.bias	torch.Size([1])	cuda:0	True
2024-01-02 18:33:18,116 - INFO - model.bert.transformer_blocks.0.mlp.fc1.weight	torch.Size([1024, 256])	cuda:0	True
2024-01-02 18:33:18,116 - INFO - model.bert.transformer_blocks.0.mlp.fc1.bias	torch.Size([1024])	cuda:0	True
2024-01-02 18:33:18,116 - INFO - model.bert.transformer_blocks.0.mlp.fc2.weight	torch.Size([256, 1024])	cuda:0	True
2024-01-02 18:33:18,116 - INFO - model.bert.transformer_blocks.0.mlp.fc2.bias	torch.Size([256])	cuda:0	True
2024-01-02 18:33:18,117 - INFO - model.bert.transformer_blocks.0.norm1.weight	torch.Size([256])	cuda:0	True
2024-01-02 18:33:18,117 - INFO - model.bert.transformer_blocks.0.norm1.bias	torch.Size([256])	cuda:0	True
2024-01-02 18:33:18,117 - INFO - model.bert.transformer_blocks.0.norm2.weight	torch.Size([256])	cuda:0	True
2024-01-02 18:33:18,117 - INFO - model.bert.transformer_blocks.0.norm2.bias	torch.Size([256])	cuda:0	True
2024-01-02 18:33:18,117 - INFO - model.bert.transformer_blocks.1.attention.linear_layers.0.weight	torch.Size([256, 256])	cuda:0	True
2024-01-02 18:33:18,117 - INFO - model.bert.transformer_blocks.1.attention.linear_layers.0.bias	torch.Size([256])	cuda:0	True
2024-01-02 18:33:18,117 - INFO - model.bert.transformer_blocks.1.attention.linear_layers.1.weight	torch.Size([256, 256])	cuda:0	True
2024-01-02 18:33:18,117 - INFO - model.bert.transformer_blocks.1.attention.linear_layers.1.bias	torch.Size([256])	cuda:0	True
2024-01-02 18:33:18,117 - INFO - model.bert.transformer_blocks.1.attention.linear_layers.2.weight	torch.Size([256, 256])	cuda:0	True
2024-01-02 18:33:18,118 - INFO - model.bert.transformer_blocks.1.attention.linear_layers.2.bias	torch.Size([256])	cuda:0	True
2024-01-02 18:33:18,118 - INFO - model.bert.transformer_blocks.1.attention.proj.weight	torch.Size([256, 256])	cuda:0	True
2024-01-02 18:33:18,118 - INFO - model.bert.transformer_blocks.1.attention.proj.bias	torch.Size([256])	cuda:0	True
2024-01-02 18:33:18,118 - INFO - model.bert.transformer_blocks.1.attention.temporal_mat_bias_1.weight	torch.Size([64, 1])	cuda:0	True
2024-01-02 18:33:18,118 - INFO - model.bert.transformer_blocks.1.attention.temporal_mat_bias_1.bias	torch.Size([64])	cuda:0	True
2024-01-02 18:33:18,118 - INFO - model.bert.transformer_blocks.1.attention.temporal_mat_bias_2.weight	torch.Size([1, 64])	cuda:0	True
2024-01-02 18:33:18,119 - INFO - model.bert.transformer_blocks.1.attention.temporal_mat_bias_2.bias	torch.Size([1])	cuda:0	True
2024-01-02 18:33:18,119 - INFO - model.bert.transformer_blocks.1.mlp.fc1.weight	torch.Size([1024, 256])	cuda:0	True
2024-01-02 18:33:18,119 - INFO - model.bert.transformer_blocks.1.mlp.fc1.bias	torch.Size([1024])	cuda:0	True
2024-01-02 18:33:18,119 - INFO - model.bert.transformer_blocks.1.mlp.fc2.weight	torch.Size([256, 1024])	cuda:0	True
2024-01-02 18:33:18,119 - INFO - model.bert.transformer_blocks.1.mlp.fc2.bias	torch.Size([256])	cuda:0	True
2024-01-02 18:33:18,119 - INFO - model.bert.transformer_blocks.1.norm1.weight	torch.Size([256])	cuda:0	True
2024-01-02 18:33:18,119 - INFO - model.bert.transformer_blocks.1.norm1.bias	torch.Size([256])	cuda:0	True
2024-01-02 18:33:18,119 - INFO - model.bert.transformer_blocks.1.norm2.weight	torch.Size([256])	cuda:0	True
2024-01-02 18:33:18,120 - INFO - model.bert.transformer_blocks.1.norm2.bias	torch.Size([256])	cuda:0	True
2024-01-02 18:33:18,120 - INFO - model.bert.transformer_blocks.2.attention.linear_layers.0.weight	torch.Size([256, 256])	cuda:0	True
2024-01-02 18:33:18,120 - INFO - model.bert.transformer_blocks.2.attention.linear_layers.0.bias	torch.Size([256])	cuda:0	True
2024-01-02 18:33:18,120 - INFO - model.bert.transformer_blocks.2.attention.linear_layers.1.weight	torch.Size([256, 256])	cuda:0	True
2024-01-02 18:33:18,120 - INFO - model.bert.transformer_blocks.2.attention.linear_layers.1.bias	torch.Size([256])	cuda:0	True
2024-01-02 18:33:18,120 - INFO - model.bert.transformer_blocks.2.attention.linear_layers.2.weight	torch.Size([256, 256])	cuda:0	True
2024-01-02 18:33:18,120 - INFO - model.bert.transformer_blocks.2.attention.linear_layers.2.bias	torch.Size([256])	cuda:0	True
2024-01-02 18:33:18,120 - INFO - model.bert.transformer_blocks.2.attention.proj.weight	torch.Size([256, 256])	cuda:0	True
2024-01-02 18:33:18,121 - INFO - model.bert.transformer_blocks.2.attention.proj.bias	torch.Size([256])	cuda:0	True
2024-01-02 18:33:18,121 - INFO - model.bert.transformer_blocks.2.attention.temporal_mat_bias_1.weight	torch.Size([64, 1])	cuda:0	True
2024-01-02 18:33:18,121 - INFO - model.bert.transformer_blocks.2.attention.temporal_mat_bias_1.bias	torch.Size([64])	cuda:0	True
2024-01-02 18:33:18,121 - INFO - model.bert.transformer_blocks.2.attention.temporal_mat_bias_2.weight	torch.Size([1, 64])	cuda:0	True
2024-01-02 18:33:18,121 - INFO - model.bert.transformer_blocks.2.attention.temporal_mat_bias_2.bias	torch.Size([1])	cuda:0	True
2024-01-02 18:33:18,121 - INFO - model.bert.transformer_blocks.2.mlp.fc1.weight	torch.Size([1024, 256])	cuda:0	True
2024-01-02 18:33:18,121 - INFO - model.bert.transformer_blocks.2.mlp.fc1.bias	torch.Size([1024])	cuda:0	True
2024-01-02 18:33:18,121 - INFO - model.bert.transformer_blocks.2.mlp.fc2.weight	torch.Size([256, 1024])	cuda:0	True
2024-01-02 18:33:18,122 - INFO - model.bert.transformer_blocks.2.mlp.fc2.bias	torch.Size([256])	cuda:0	True
2024-01-02 18:33:18,122 - INFO - model.bert.transformer_blocks.2.norm1.weight	torch.Size([256])	cuda:0	True
2024-01-02 18:33:18,122 - INFO - model.bert.transformer_blocks.2.norm1.bias	torch.Size([256])	cuda:0	True
2024-01-02 18:33:18,122 - INFO - model.bert.transformer_blocks.2.norm2.weight	torch.Size([256])	cuda:0	True
2024-01-02 18:33:18,122 - INFO - model.bert.transformer_blocks.2.norm2.bias	torch.Size([256])	cuda:0	True
2024-01-02 18:33:18,122 - INFO - model.bert.transformer_blocks.3.attention.linear_layers.0.weight	torch.Size([256, 256])	cuda:0	True
2024-01-02 18:33:18,122 - INFO - model.bert.transformer_blocks.3.attention.linear_layers.0.bias	torch.Size([256])	cuda:0	True
2024-01-02 18:33:18,123 - INFO - model.bert.transformer_blocks.3.attention.linear_layers.1.weight	torch.Size([256, 256])	cuda:0	True
2024-01-02 18:33:18,123 - INFO - model.bert.transformer_blocks.3.attention.linear_layers.1.bias	torch.Size([256])	cuda:0	True
2024-01-02 18:33:18,123 - INFO - model.bert.transformer_blocks.3.attention.linear_layers.2.weight	torch.Size([256, 256])	cuda:0	True
2024-01-02 18:33:18,123 - INFO - model.bert.transformer_blocks.3.attention.linear_layers.2.bias	torch.Size([256])	cuda:0	True
2024-01-02 18:33:18,123 - INFO - model.bert.transformer_blocks.3.attention.proj.weight	torch.Size([256, 256])	cuda:0	True
2024-01-02 18:33:18,123 - INFO - model.bert.transformer_blocks.3.attention.proj.bias	torch.Size([256])	cuda:0	True
2024-01-02 18:33:18,123 - INFO - model.bert.transformer_blocks.3.attention.temporal_mat_bias_1.weight	torch.Size([64, 1])	cuda:0	True
2024-01-02 18:33:18,123 - INFO - model.bert.transformer_blocks.3.attention.temporal_mat_bias_1.bias	torch.Size([64])	cuda:0	True
2024-01-02 18:33:18,123 - INFO - model.bert.transformer_blocks.3.attention.temporal_mat_bias_2.weight	torch.Size([1, 64])	cuda:0	True
2024-01-02 18:33:18,124 - INFO - model.bert.transformer_blocks.3.attention.temporal_mat_bias_2.bias	torch.Size([1])	cuda:0	True
2024-01-02 18:33:18,124 - INFO - model.bert.transformer_blocks.3.mlp.fc1.weight	torch.Size([1024, 256])	cuda:0	True
2024-01-02 18:33:18,124 - INFO - model.bert.transformer_blocks.3.mlp.fc1.bias	torch.Size([1024])	cuda:0	True
2024-01-02 18:33:18,124 - INFO - model.bert.transformer_blocks.3.mlp.fc2.weight	torch.Size([256, 1024])	cuda:0	True
2024-01-02 18:33:18,124 - INFO - model.bert.transformer_blocks.3.mlp.fc2.bias	torch.Size([256])	cuda:0	True
2024-01-02 18:33:18,124 - INFO - model.bert.transformer_blocks.3.norm1.weight	torch.Size([256])	cuda:0	True
2024-01-02 18:33:18,124 - INFO - model.bert.transformer_blocks.3.norm1.bias	torch.Size([256])	cuda:0	True
2024-01-02 18:33:18,124 - INFO - model.bert.transformer_blocks.3.norm2.weight	torch.Size([256])	cuda:0	True
2024-01-02 18:33:18,125 - INFO - model.bert.transformer_blocks.3.norm2.bias	torch.Size([256])	cuda:0	True
2024-01-02 18:33:18,125 - INFO - model.bert.transformer_blocks.4.attention.linear_layers.0.weight	torch.Size([256, 256])	cuda:0	True
2024-01-02 18:33:18,125 - INFO - model.bert.transformer_blocks.4.attention.linear_layers.0.bias	torch.Size([256])	cuda:0	True
2024-01-02 18:33:18,125 - INFO - model.bert.transformer_blocks.4.attention.linear_layers.1.weight	torch.Size([256, 256])	cuda:0	True
2024-01-02 18:33:18,125 - INFO - model.bert.transformer_blocks.4.attention.linear_layers.1.bias	torch.Size([256])	cuda:0	True
2024-01-02 18:33:18,125 - INFO - model.bert.transformer_blocks.4.attention.linear_layers.2.weight	torch.Size([256, 256])	cuda:0	True
2024-01-02 18:33:18,125 - INFO - model.bert.transformer_blocks.4.attention.linear_layers.2.bias	torch.Size([256])	cuda:0	True
2024-01-02 18:33:18,126 - INFO - model.bert.transformer_blocks.4.attention.proj.weight	torch.Size([256, 256])	cuda:0	True
2024-01-02 18:33:18,126 - INFO - model.bert.transformer_blocks.4.attention.proj.bias	torch.Size([256])	cuda:0	True
2024-01-02 18:33:18,126 - INFO - model.bert.transformer_blocks.4.attention.temporal_mat_bias_1.weight	torch.Size([64, 1])	cuda:0	True
2024-01-02 18:33:18,126 - INFO - model.bert.transformer_blocks.4.attention.temporal_mat_bias_1.bias	torch.Size([64])	cuda:0	True
2024-01-02 18:33:18,126 - INFO - model.bert.transformer_blocks.4.attention.temporal_mat_bias_2.weight	torch.Size([1, 64])	cuda:0	True
2024-01-02 18:33:18,126 - INFO - model.bert.transformer_blocks.4.attention.temporal_mat_bias_2.bias	torch.Size([1])	cuda:0	True
2024-01-02 18:33:18,126 - INFO - model.bert.transformer_blocks.4.mlp.fc1.weight	torch.Size([1024, 256])	cuda:0	True
2024-01-02 18:33:18,127 - INFO - model.bert.transformer_blocks.4.mlp.fc1.bias	torch.Size([1024])	cuda:0	True
2024-01-02 18:33:18,127 - INFO - model.bert.transformer_blocks.4.mlp.fc2.weight	torch.Size([256, 1024])	cuda:0	True
2024-01-02 18:33:18,127 - INFO - model.bert.transformer_blocks.4.mlp.fc2.bias	torch.Size([256])	cuda:0	True
2024-01-02 18:33:18,127 - INFO - model.bert.transformer_blocks.4.norm1.weight	torch.Size([256])	cuda:0	True
2024-01-02 18:33:18,127 - INFO - model.bert.transformer_blocks.4.norm1.bias	torch.Size([256])	cuda:0	True
2024-01-02 18:33:18,127 - INFO - model.bert.transformer_blocks.4.norm2.weight	torch.Size([256])	cuda:0	True
2024-01-02 18:33:18,127 - INFO - model.bert.transformer_blocks.4.norm2.bias	torch.Size([256])	cuda:0	True
2024-01-02 18:33:18,127 - INFO - model.bert.transformer_blocks.5.attention.linear_layers.0.weight	torch.Size([256, 256])	cuda:0	True
2024-01-02 18:33:18,128 - INFO - model.bert.transformer_blocks.5.attention.linear_layers.0.bias	torch.Size([256])	cuda:0	True
2024-01-02 18:33:18,128 - INFO - model.bert.transformer_blocks.5.attention.linear_layers.1.weight	torch.Size([256, 256])	cuda:0	True
2024-01-02 18:33:18,128 - INFO - model.bert.transformer_blocks.5.attention.linear_layers.1.bias	torch.Size([256])	cuda:0	True
2024-01-02 18:33:18,128 - INFO - model.bert.transformer_blocks.5.attention.linear_layers.2.weight	torch.Size([256, 256])	cuda:0	True
2024-01-02 18:33:18,128 - INFO - model.bert.transformer_blocks.5.attention.linear_layers.2.bias	torch.Size([256])	cuda:0	True
2024-01-02 18:33:18,128 - INFO - model.bert.transformer_blocks.5.attention.proj.weight	torch.Size([256, 256])	cuda:0	True
2024-01-02 18:33:18,128 - INFO - model.bert.transformer_blocks.5.attention.proj.bias	torch.Size([256])	cuda:0	True
2024-01-02 18:33:18,128 - INFO - model.bert.transformer_blocks.5.attention.temporal_mat_bias_1.weight	torch.Size([64, 1])	cuda:0	True
2024-01-02 18:33:18,128 - INFO - model.bert.transformer_blocks.5.attention.temporal_mat_bias_1.bias	torch.Size([64])	cuda:0	True
2024-01-02 18:33:18,129 - INFO - model.bert.transformer_blocks.5.attention.temporal_mat_bias_2.weight	torch.Size([1, 64])	cuda:0	True
2024-01-02 18:33:18,129 - INFO - model.bert.transformer_blocks.5.attention.temporal_mat_bias_2.bias	torch.Size([1])	cuda:0	True
2024-01-02 18:33:18,129 - INFO - model.bert.transformer_blocks.5.mlp.fc1.weight	torch.Size([1024, 256])	cuda:0	True
2024-01-02 18:33:18,129 - INFO - model.bert.transformer_blocks.5.mlp.fc1.bias	torch.Size([1024])	cuda:0	True
2024-01-02 18:33:18,129 - INFO - model.bert.transformer_blocks.5.mlp.fc2.weight	torch.Size([256, 1024])	cuda:0	True
2024-01-02 18:33:18,129 - INFO - model.bert.transformer_blocks.5.mlp.fc2.bias	torch.Size([256])	cuda:0	True
2024-01-02 18:33:18,129 - INFO - model.bert.transformer_blocks.5.norm1.weight	torch.Size([256])	cuda:0	True
2024-01-02 18:33:18,129 - INFO - model.bert.transformer_blocks.5.norm1.bias	torch.Size([256])	cuda:0	True
2024-01-02 18:33:18,130 - INFO - model.bert.transformer_blocks.5.norm2.weight	torch.Size([256])	cuda:0	True
2024-01-02 18:33:18,130 - INFO - model.bert.transformer_blocks.5.norm2.bias	torch.Size([256])	cuda:0	True
2024-01-02 18:33:18,130 - INFO - linear.weight	torch.Size([27485, 256])	cuda:0	True
2024-01-02 18:33:18,130 - INFO - linear.bias	torch.Size([27485])	cuda:0	True
2024-01-02 18:33:18,131 - INFO - Total parameter numbers: 12384867
2024-01-02 18:33:18,131 - INFO - You select `adamw` optimizer.
2024-01-02 18:33:18,132 - INFO - You select `cosinelr` lr_scheduler.
2024-01-02 18:33:18,187 - INFO - Load Pretrained-Model from libcity/cache/454824/model_cache/454824_BERTContrastiveLM_bj.pt
2024-01-02 18:33:18,234 - INFO - Start training ...
2024-01-02 18:33:18,234 - INFO - Num_batches: train=1643, eval=548
2024-01-02 18:33:19,437 - INFO - {'mode': 'Train', 'epoch': 0, 'iter': 0, 'lr': 1e-06, 'loss': 10.393976211547852, 'acc(%)': 0.0}
2024-01-02 18:34:09,688 - INFO - {'mode': 'Train', 'epoch': 0, 'iter': 500, 'lr': 1e-06, 'loss': 11.068115234375, 'acc(%)': 0.0}
2024-01-02 18:34:59,918 - INFO - {'mode': 'Train', 'epoch': 0, 'iter': 1000, 'lr': 1e-06, 'loss': 10.293181419372559, 'acc(%)': 0.012487512487512488}
2024-01-02 18:35:51,280 - INFO - {'mode': 'Train', 'epoch': 0, 'iter': 1500, 'lr': 1e-06, 'loss': 11.130422592163086, 'acc(%)': 0.024983344437041973}
2024-01-02 18:36:05,844 - INFO - Train: expid = 230271, Epoch = 0, avg_loss = 10.880444083443914, total_acc = 0.030446034404018874%.
2024-01-02 18:36:05,844 - INFO - epoch complete!
2024-01-02 18:36:05,844 - INFO - evaluating now!
2024-01-02 18:36:05,875 - INFO - {'mode': 'Eval', 'epoch': 0, 'iter': 0, 'lr': 1e-06, 'loss': 10.527007102966309, 'acc(%)': 0.0}
2024-01-02 18:36:20,859 - INFO - {'mode': 'Eval', 'epoch': 0, 'iter': 500, 'lr': 1e-06, 'loss': 10.942350387573242, 'acc(%)': 0.049900199600798396}
2024-01-02 18:36:22,310 - INFO - Eval: expid = 230271, Epoch = 0, avg_loss = 10.794946973617762, total_acc = 0.045662100456621%.
2024-01-02 18:36:22,311 - INFO - Epoch [0/30] (1643)  train_loss: 10.8804, train_acc: 0.03%, val_loss: 10.7949, val_acc: 0.05%, lr: 0.000051, 184.08s
2024-01-02 18:36:22,667 - INFO - Saved model at 0
2024-01-02 18:36:22,667 - INFO - Val loss decrease from inf to 10.7949, saving to ./libcity/cache/230271/model_cache/LinearNextLoc_bj_epoch0.tar
2024-01-02 18:36:22,772 - INFO - {'mode': 'Train', 'epoch': 1, 'iter': 0, 'lr': 5.075e-05, 'loss': 10.623466491699219, 'acc(%)': 0.0}
2024-01-02 18:37:15,571 - INFO - {'mode': 'Train', 'epoch': 1, 'iter': 500, 'lr': 5.075e-05, 'loss': 8.443053245544434, 'acc(%)': 2.944111776447106}
2024-01-02 18:38:07,763 - INFO - {'mode': 'Train', 'epoch': 1, 'iter': 1000, 'lr': 5.075e-05, 'loss': 8.814657211303711, 'acc(%)': 4.670329670329671}
2024-01-02 18:39:00,163 - INFO - {'mode': 'Train', 'epoch': 1, 'iter': 1500, 'lr': 5.075e-05, 'loss': 8.804052352905273, 'acc(%)': 5.229846768820786}
2024-01-02 18:39:15,473 - INFO - Train: expid = 230271, Epoch = 1, avg_loss = 8.903277611801443, total_acc = 5.41939412391536%.
2024-01-02 18:39:15,473 - INFO - epoch complete!
2024-01-02 18:39:15,474 - INFO - evaluating now!
2024-01-02 18:39:15,504 - INFO - {'mode': 'Eval', 'epoch': 1, 'iter': 0, 'lr': 5.075e-05, 'loss': 8.464587211608887, 'acc(%)': 12.5}
2024-01-02 18:39:31,173 - INFO - {'mode': 'Eval', 'epoch': 1, 'iter': 500, 'lr': 5.075e-05, 'loss': 11.028830528259277, 'acc(%)': 6.412175648702595}
2024-01-02 18:39:32,575 - INFO - Eval: expid = 230271, Epoch = 1, avg_loss = 9.565837949813773, total_acc = 6.4840182648401825%.
2024-01-02 18:39:32,576 - INFO - Epoch [1/30] (3286)  train_loss: 8.9033, train_acc: 5.42%, val_loss: 9.5658, val_acc: 6.48%, lr: 0.000101, 189.91s
2024-01-02 18:39:32,942 - INFO - Saved model at 1
2024-01-02 18:39:32,943 - INFO - Val loss decrease from 10.7949 to 9.5658, saving to ./libcity/cache/230271/model_cache/LinearNextLoc_bj_epoch1.tar
2024-01-02 18:39:33,045 - INFO - {'mode': 'Train', 'epoch': 2, 'iter': 0, 'lr': 0.0001005, 'loss': 8.86177921295166, 'acc(%)': 0.0}
2024-01-02 18:40:26,151 - INFO - {'mode': 'Train', 'epoch': 2, 'iter': 500, 'lr': 0.0001005, 'loss': 7.635390281677246, 'acc(%)': 7.18562874251497}
2024-01-02 18:41:20,554 - INFO - {'mode': 'Train', 'epoch': 2, 'iter': 1000, 'lr': 0.0001005, 'loss': 9.349145889282227, 'acc(%)': 6.98051948051948}
2024-01-02 18:42:15,571 - INFO - {'mode': 'Train', 'epoch': 2, 'iter': 1500, 'lr': 0.0001005, 'loss': 7.385835647583008, 'acc(%)': 7.086942038640906}
2024-01-02 18:42:31,355 - INFO - Train: expid = 230271, Epoch = 2, avg_loss = 7.835431070628277, total_acc = 7.170041102146446%.
2024-01-02 18:42:31,355 - INFO - epoch complete!
2024-01-02 18:42:31,356 - INFO - evaluating now!
2024-01-02 18:42:31,389 - INFO - {'mode': 'Eval', 'epoch': 2, 'iter': 0, 'lr': 0.0001005, 'loss': 7.052530288696289, 'acc(%)': 12.5}
2024-01-02 18:42:47,070 - INFO - {'mode': 'Eval', 'epoch': 2, 'iter': 500, 'lr': 0.0001005, 'loss': 9.629876136779785, 'acc(%)': 9.006986027944112}
2024-01-02 18:42:48,548 - INFO - Eval: expid = 230271, Epoch = 2, avg_loss = 9.01884211840695, total_acc = 9.06392694063927%.
2024-01-02 18:42:48,548 - INFO - Epoch [2/30] (4929)  train_loss: 7.8354, train_acc: 7.17%, val_loss: 9.0188, val_acc: 9.06%, lr: 0.000150, 195.60s
2024-01-02 18:42:48,902 - INFO - Saved model at 2
2024-01-02 18:42:48,902 - INFO - Val loss decrease from 9.5658 to 9.0188, saving to ./libcity/cache/230271/model_cache/LinearNextLoc_bj_epoch2.tar
2024-01-02 18:42:49,011 - INFO - {'mode': 'Train', 'epoch': 3, 'iter': 0, 'lr': 0.00015025000000000002, 'loss': 7.068341255187988, 'acc(%)': 0.0}
2024-01-02 18:43:42,605 - INFO - {'mode': 'Train', 'epoch': 3, 'iter': 500, 'lr': 0.00015025000000000002, 'loss': 6.990815162658691, 'acc(%)': 8.532934131736528}
2024-01-02 18:44:36,371 - INFO - {'mode': 'Train', 'epoch': 3, 'iter': 1000, 'lr': 0.00015025000000000002, 'loss': 7.423998832702637, 'acc(%)': 8.404095904095904}
2024-01-02 18:45:30,744 - INFO - {'mode': 'Train', 'epoch': 3, 'iter': 1500, 'lr': 0.00015025000000000002, 'loss': 7.4634552001953125, 'acc(%)': 8.894070619586943}
2024-01-02 18:45:47,237 - INFO - Train: expid = 230271, Epoch = 3, avg_loss = 6.812104893703405, total_acc = 9.072918252397626%.
2024-01-02 18:45:47,238 - INFO - epoch complete!
2024-01-02 18:45:47,238 - INFO - evaluating now!
2024-01-02 18:45:47,269 - INFO - {'mode': 'Eval', 'epoch': 3, 'iter': 0, 'lr': 0.00015025000000000002, 'loss': 7.991600036621094, 'acc(%)': 12.5}
2024-01-02 18:46:03,944 - INFO - {'mode': 'Eval', 'epoch': 3, 'iter': 500, 'lr': 0.00015025000000000002, 'loss': 6.730562686920166, 'acc(%)': 10.62874251497006}
2024-01-02 18:46:05,426 - INFO - Eval: expid = 230271, Epoch = 3, avg_loss = 8.62694526567851, total_acc = 10.296803652968038%.
2024-01-02 18:46:05,427 - INFO - Epoch [3/30] (6572)  train_loss: 6.8121, train_acc: 9.07%, val_loss: 8.6269, val_acc: 10.30%, lr: 0.000191, 196.52s
2024-01-02 18:46:05,808 - INFO - Saved model at 3
2024-01-02 18:46:05,808 - INFO - Val loss decrease from 9.0188 to 8.6269, saving to ./libcity/cache/230271/model_cache/LinearNextLoc_bj_epoch3.tar
2024-01-02 18:46:05,921 - INFO - {'mode': 'Train', 'epoch': 4, 'iter': 0, 'lr': 0.0001913545457642601, 'loss': 4.731830596923828, 'acc(%)': 25.0}
2024-01-02 18:46:57,976 - INFO - {'mode': 'Train', 'epoch': 4, 'iter': 500, 'lr': 0.0001913545457642601, 'loss': 7.19622802734375, 'acc(%)': 11.077844311377245}
2024-01-02 18:47:51,521 - INFO - {'mode': 'Train', 'epoch': 4, 'iter': 1000, 'lr': 0.0001913545457642601, 'loss': 7.12713623046875, 'acc(%)': 11.163836163836164}
2024-01-02 18:48:44,259 - INFO - {'mode': 'Train', 'epoch': 4, 'iter': 1500, 'lr': 0.0001913545457642601, 'loss': 5.05750036239624, 'acc(%)': 11.167554963357762}
2024-01-02 18:48:58,719 - INFO - Train: expid = 230271, Epoch = 4, avg_loss = 5.9419632384831145, total_acc = 11.219363677880956%.
2024-01-02 18:48:58,720 - INFO - epoch complete!
2024-01-02 18:48:58,720 - INFO - evaluating now!
2024-01-02 18:48:58,752 - INFO - {'mode': 'Eval', 'epoch': 4, 'iter': 0, 'lr': 0.0001913545457642601, 'loss': 8.991510391235352, 'acc(%)': 12.5}
2024-01-02 18:49:13,490 - INFO - {'mode': 'Eval', 'epoch': 4, 'iter': 500, 'lr': 0.0001913545457642601, 'loss': 12.879477500915527, 'acc(%)': 10.90319361277445}
2024-01-02 18:49:14,876 - INFO - Eval: expid = 230271, Epoch = 4, avg_loss = 8.447662425367799, total_acc = 10.95890410958904%.
2024-01-02 18:49:14,876 - INFO - Epoch [4/30] (8215)  train_loss: 5.9420, train_acc: 11.22%, val_loss: 8.4477, val_acc: 10.96%, lr: 0.000187, 189.07s
2024-01-02 18:49:15,248 - INFO - Saved model at 4
2024-01-02 18:49:15,249 - INFO - Val loss decrease from 8.6269 to 8.4477, saving to ./libcity/cache/230271/model_cache/LinearNextLoc_bj_epoch4.tar
2024-01-02 18:49:15,351 - INFO - {'mode': 'Train', 'epoch': 5, 'iter': 0, 'lr': 0.00018660254037844388, 'loss': 5.469861030578613, 'acc(%)': 0.0}
2024-01-02 18:50:07,122 - INFO - {'mode': 'Train', 'epoch': 5, 'iter': 500, 'lr': 0.00018660254037844388, 'loss': 5.90728235244751, 'acc(%)': 15.693612774451099}
2024-01-02 18:50:58,455 - INFO - {'mode': 'Train', 'epoch': 5, 'iter': 1000, 'lr': 0.00018660254037844388, 'loss': 5.444705009460449, 'acc(%)': 15.0974025974026}
2024-01-02 18:51:49,700 - INFO - {'mode': 'Train', 'epoch': 5, 'iter': 1500, 'lr': 0.00018660254037844388, 'loss': 4.800378799438477, 'acc(%)': 14.965023317788143}
2024-01-02 18:52:04,499 - INFO - Train: expid = 230271, Epoch = 5, avg_loss = 5.064109972031771, total_acc = 14.956614400974273%.
2024-01-02 18:52:04,500 - INFO - epoch complete!
2024-01-02 18:52:04,500 - INFO - evaluating now!
2024-01-02 18:52:04,531 - INFO - {'mode': 'Eval', 'epoch': 5, 'iter': 0, 'lr': 0.00018660254037844388, 'loss': 6.402217388153076, 'acc(%)': 25.0}
2024-01-02 18:52:19,477 - INFO - {'mode': 'Eval', 'epoch': 5, 'iter': 500, 'lr': 0.00018660254037844388, 'loss': 11.941587448120117, 'acc(%)': 12.749500998003994}
2024-01-02 18:52:20,875 - INFO - Eval: expid = 230271, Epoch = 5, avg_loss = 8.427618541020781, total_acc = 12.374429223744292%.
2024-01-02 18:52:20,876 - INFO - Epoch [5/30] (9858)  train_loss: 5.0641, train_acc: 14.96%, val_loss: 8.4276, val_acc: 12.37%, lr: 0.000181, 185.63s
2024-01-02 18:52:21,238 - INFO - Saved model at 5
2024-01-02 18:52:21,238 - INFO - Val loss decrease from 8.4477 to 8.4276, saving to ./libcity/cache/230271/model_cache/LinearNextLoc_bj_epoch5.tar
2024-01-02 18:52:21,342 - INFO - {'mode': 'Train', 'epoch': 6, 'iter': 0, 'lr': 0.00018090169943749476, 'loss': 3.2607264518737793, 'acc(%)': 50.0}
2024-01-02 18:53:13,329 - INFO - {'mode': 'Train', 'epoch': 6, 'iter': 500, 'lr': 0.00018090169943749476, 'loss': 3.3149514198303223, 'acc(%)': 20.484031936127746}
2024-01-02 18:54:05,369 - INFO - {'mode': 'Train', 'epoch': 6, 'iter': 1000, 'lr': 0.00018090169943749476, 'loss': 4.707165718078613, 'acc(%)': 19.592907092907094}
2024-01-02 18:54:57,359 - INFO - {'mode': 'Train', 'epoch': 6, 'iter': 1500, 'lr': 0.00018090169943749476, 'loss': 3.5011653900146484, 'acc(%)': 19.278814123917385}
2024-01-02 18:55:12,191 - INFO - Train: expid = 230271, Epoch = 6, avg_loss = 4.338790964764499, total_acc = 19.04399451971381%.
2024-01-02 18:55:12,191 - INFO - epoch complete!
2024-01-02 18:55:12,191 - INFO - evaluating now!
2024-01-02 18:55:12,223 - INFO - {'mode': 'Eval', 'epoch': 6, 'iter': 0, 'lr': 0.00018090169943749476, 'loss': 10.10389518737793, 'acc(%)': 12.5}
2024-01-02 18:55:27,387 - INFO - {'mode': 'Eval', 'epoch': 6, 'iter': 500, 'lr': 0.00018090169943749476, 'loss': 8.936112403869629, 'acc(%)': 13.722554890219559}
2024-01-02 18:55:28,805 - INFO - Eval: expid = 230271, Epoch = 6, avg_loss = 8.526917449742147, total_acc = 13.584474885844749%.
2024-01-02 18:55:28,805 - INFO - Epoch [6/30] (11501)  train_loss: 4.3388, train_acc: 19.04%, val_loss: 8.5269, val_acc: 13.58%, lr: 0.000174, 187.57s
2024-01-02 18:55:28,913 - INFO - {'mode': 'Train', 'epoch': 7, 'iter': 0, 'lr': 0.00017431448254773944, 'loss': 3.9008285999298096, 'acc(%)': 25.0}
2024-01-02 18:56:21,038 - INFO - {'mode': 'Train', 'epoch': 7, 'iter': 500, 'lr': 0.00017431448254773944, 'loss': 3.6696014404296875, 'acc(%)': 30.039920159680637}
2024-01-02 18:57:13,008 - INFO - {'mode': 'Train', 'epoch': 7, 'iter': 1000, 'lr': 0.00017431448254773944, 'loss': 2.755201578140259, 'acc(%)': 27.297702297702298}
2024-01-02 18:58:09,630 - INFO - {'mode': 'Train', 'epoch': 7, 'iter': 1500, 'lr': 0.00017431448254773944, 'loss': 4.316289901733398, 'acc(%)': 26.615589606928715}
2024-01-02 18:58:26,643 - INFO - Train: expid = 230271, Epoch = 7, avg_loss = 3.6674779303125784, total_acc = 26.396711828284364%.
2024-01-02 18:58:26,643 - INFO - epoch complete!
2024-01-02 18:58:26,644 - INFO - evaluating now!
2024-01-02 18:58:26,677 - INFO - {'mode': 'Eval', 'epoch': 7, 'iter': 0, 'lr': 0.00017431448254773944, 'loss': 2.6499853134155273, 'acc(%)': 75.0}
2024-01-02 18:58:44,837 - INFO - {'mode': 'Eval', 'epoch': 7, 'iter': 500, 'lr': 0.00017431448254773944, 'loss': 8.06688404083252, 'acc(%)': 14.296407185628743}
2024-01-02 18:58:46,505 - INFO - Eval: expid = 230271, Epoch = 7, avg_loss = 8.476735972922686, total_acc = 14.360730593607308%.
2024-01-02 18:58:46,505 - INFO - Epoch [7/30] (13144)  train_loss: 3.6675, train_acc: 26.40%, val_loss: 8.4767, val_acc: 14.36%, lr: 0.000167, 197.70s
2024-01-02 18:58:46,617 - INFO - {'mode': 'Train', 'epoch': 8, 'iter': 0, 'lr': 0.00016691306063588583, 'loss': 2.36928391456604, 'acc(%)': 50.0}
2024-01-02 18:59:48,913 - INFO - {'mode': 'Train', 'epoch': 8, 'iter': 500, 'lr': 0.00016691306063588583, 'loss': 2.166398048400879, 'acc(%)': 40.768463073852296}
2024-01-02 19:00:51,365 - INFO - {'mode': 'Train', 'epoch': 8, 'iter': 1000, 'lr': 0.00016691306063588583, 'loss': 3.0092616081237793, 'acc(%)': 38.16183816183816}
2024-01-02 19:01:47,654 - INFO - {'mode': 'Train', 'epoch': 8, 'iter': 1500, 'lr': 0.00016691306063588583, 'loss': 3.7156624794006348, 'acc(%)': 36.61725516322451}
2024-01-02 19:02:02,447 - INFO - Train: expid = 230271, Epoch = 8, avg_loss = 3.0168840724377715, total_acc = 36.108996803166384%.
2024-01-02 19:02:02,447 - INFO - epoch complete!
2024-01-02 19:02:02,448 - INFO - evaluating now!
2024-01-02 19:02:02,479 - INFO - {'mode': 'Eval', 'epoch': 8, 'iter': 0, 'lr': 0.00016691306063588583, 'loss': 10.820327758789062, 'acc(%)': 25.0}
2024-01-02 19:02:17,606 - INFO - {'mode': 'Eval', 'epoch': 8, 'iter': 500, 'lr': 0.00016691306063588583, 'loss': 9.279481887817383, 'acc(%)': 14.920159680638722}
2024-01-02 19:02:19,002 - INFO - Eval: expid = 230271, Epoch = 8, avg_loss = 8.604038494580413, total_acc = 15.228310502283104%.
2024-01-02 19:02:19,002 - INFO - Epoch [8/30] (14787)  train_loss: 3.0169, train_acc: 36.11%, val_loss: 8.6040, val_acc: 15.23%, lr: 0.000159, 212.50s
2024-01-02 19:02:19,114 - INFO - {'mode': 'Train', 'epoch': 9, 'iter': 0, 'lr': 0.00015877852522924732, 'loss': 2.189117908477783, 'acc(%)': 62.5}
2024-01-02 19:03:11,344 - INFO - {'mode': 'Train', 'epoch': 9, 'iter': 500, 'lr': 0.00015877852522924732, 'loss': 2.4117016792297363, 'acc(%)': 54.66566866267465}
2024-01-02 19:04:03,561 - INFO - {'mode': 'Train', 'epoch': 9, 'iter': 1000, 'lr': 0.00015877852522924732, 'loss': 3.015936851501465, 'acc(%)': 50.68681318681318}
2024-01-02 19:04:55,506 - INFO - {'mode': 'Train', 'epoch': 9, 'iter': 1500, 'lr': 0.00015877852522924732, 'loss': 2.655162811279297, 'acc(%)': 48.46768820786143}
2024-01-02 19:05:10,298 - INFO - Train: expid = 230271, Epoch = 9, avg_loss = 2.4408105347395703, total_acc = 47.800274014309636%.
2024-01-02 19:05:10,299 - INFO - epoch complete!
2024-01-02 19:05:10,299 - INFO - evaluating now!
2024-01-02 19:05:10,331 - INFO - {'mode': 'Eval', 'epoch': 9, 'iter': 0, 'lr': 0.00015877852522924732, 'loss': 9.7423734664917, 'acc(%)': 12.5}
2024-01-02 19:05:25,320 - INFO - {'mode': 'Eval', 'epoch': 9, 'iter': 500, 'lr': 0.00015877852522924732, 'loss': 9.98836898803711, 'acc(%)': 16.29241516966068}
2024-01-02 19:05:26,722 - INFO - Eval: expid = 230271, Epoch = 9, avg_loss = 8.780793296692034, total_acc = 16.392694063926943%.
2024-01-02 19:05:26,723 - INFO - Epoch [9/30] (16430)  train_loss: 2.4408, train_acc: 47.80%, val_loss: 8.7808, val_acc: 16.39%, lr: 0.000150, 187.72s
2024-01-02 19:05:26,832 - INFO - {'mode': 'Train', 'epoch': 10, 'iter': 0, 'lr': 0.00015000000000000001, 'loss': 1.5596262216567993, 'acc(%)': 75.0}
2024-01-02 19:06:19,311 - INFO - {'mode': 'Train', 'epoch': 10, 'iter': 500, 'lr': 0.00015000000000000001, 'loss': 2.2873289585113525, 'acc(%)': 64.44610778443113}
2024-01-02 19:07:13,134 - INFO - {'mode': 'Train', 'epoch': 10, 'iter': 1000, 'lr': 0.00015000000000000001, 'loss': 1.9417309761047363, 'acc(%)': 61.8006993006993}
2024-01-02 19:08:08,933 - INFO - {'mode': 'Train', 'epoch': 10, 'iter': 1500, 'lr': 0.00015000000000000001, 'loss': 1.6870304346084595, 'acc(%)': 59.7601598934044}
2024-01-02 19:08:24,904 - INFO - Train: expid = 230271, Epoch = 10, avg_loss = 1.920686712982194, total_acc = 59.32409803623078%.
2024-01-02 19:08:24,905 - INFO - epoch complete!
2024-01-02 19:08:24,905 - INFO - evaluating now!
2024-01-02 19:08:24,937 - INFO - {'mode': 'Eval', 'epoch': 10, 'iter': 0, 'lr': 0.00015000000000000001, 'loss': 12.133557319641113, 'acc(%)': 25.0}
2024-01-02 19:08:42,042 - INFO - {'mode': 'Eval', 'epoch': 10, 'iter': 500, 'lr': 0.00015000000000000001, 'loss': 8.59825611114502, 'acc(%)': 16.192614770459084}
2024-01-02 19:08:43,530 - INFO - Eval: expid = 230271, Epoch = 10, avg_loss = 8.908240405391885, total_acc = 16.552511415525114%.
2024-01-02 19:08:43,530 - INFO - Epoch [10/30] (18073)  train_loss: 1.9207, train_acc: 59.32%, val_loss: 8.9082, val_acc: 16.55%, lr: 0.000141, 196.81s
2024-01-02 19:08:43,638 - INFO - {'mode': 'Train', 'epoch': 11, 'iter': 0, 'lr': 0.00014067366430758004, 'loss': 1.312880039215088, 'acc(%)': 75.0}
2024-01-02 19:09:38,553 - INFO - {'mode': 'Train', 'epoch': 11, 'iter': 500, 'lr': 0.00014067366430758004, 'loss': 1.2737081050872803, 'acc(%)': 75.72355289421158}
2024-01-02 19:10:31,142 - INFO - {'mode': 'Train', 'epoch': 11, 'iter': 1000, 'lr': 0.00014067366430758004, 'loss': 1.344977855682373, 'acc(%)': 72.61488511488511}
2024-01-02 19:11:27,582 - INFO - {'mode': 'Train', 'epoch': 11, 'iter': 1500, 'lr': 0.00014067366430758004, 'loss': 1.5891135931015015, 'acc(%)': 69.89506995336441}
2024-01-02 19:11:43,596 - INFO - Train: expid = 230271, Epoch = 11, avg_loss = 1.4793922889357851, total_acc = 69.26472826914294%.
2024-01-02 19:11:43,597 - INFO - epoch complete!
2024-01-02 19:11:43,597 - INFO - evaluating now!
2024-01-02 19:11:43,632 - INFO - {'mode': 'Eval', 'epoch': 11, 'iter': 0, 'lr': 0.00014067366430758004, 'loss': 10.081820487976074, 'acc(%)': 12.5}
2024-01-02 19:12:00,659 - INFO - {'mode': 'Eval', 'epoch': 11, 'iter': 500, 'lr': 0.00014067366430758004, 'loss': 10.184680938720703, 'acc(%)': 16.791417165668662}
2024-01-02 19:12:02,076 - INFO - Eval: expid = 230271, Epoch = 11, avg_loss = 9.160122085597417, total_acc = 16.91780821917808%.
2024-01-02 19:12:02,076 - INFO - Epoch [11/30] (19716)  train_loss: 1.4794, train_acc: 69.26%, val_loss: 9.1601, val_acc: 16.92%, lr: 0.000131, 198.55s
2024-01-02 19:12:02,184 - INFO - {'mode': 'Train', 'epoch': 12, 'iter': 0, 'lr': 0.00013090169943749476, 'loss': 1.282435655593872, 'acc(%)': 87.5}
2024-01-02 19:12:55,313 - INFO - {'mode': 'Train', 'epoch': 12, 'iter': 500, 'lr': 0.00013090169943749476, 'loss': 0.7492591738700867, 'acc(%)': 81.73652694610777}
2024-01-02 19:13:48,903 - INFO - {'mode': 'Train', 'epoch': 12, 'iter': 1000, 'lr': 0.00013090169943749476, 'loss': 1.1512715816497803, 'acc(%)': 79.69530469530469}
2024-01-02 19:14:42,275 - INFO - {'mode': 'Train', 'epoch': 12, 'iter': 1500, 'lr': 0.00013090169943749476, 'loss': 0.7138420939445496, 'acc(%)': 78.53097934710193}
2024-01-02 19:14:57,774 - INFO - Train: expid = 230271, Epoch = 12, avg_loss = 1.1157213025013957, total_acc = 78.07885522910641%.
2024-01-02 19:14:57,774 - INFO - epoch complete!
2024-01-02 19:14:57,775 - INFO - evaluating now!
2024-01-02 19:14:57,805 - INFO - {'mode': 'Eval', 'epoch': 12, 'iter': 0, 'lr': 0.00013090169943749476, 'loss': 8.524035453796387, 'acc(%)': 25.0}
2024-01-02 19:15:13,473 - INFO - {'mode': 'Eval', 'epoch': 12, 'iter': 500, 'lr': 0.00013090169943749476, 'loss': 10.698493003845215, 'acc(%)': 17.015968063872254}
2024-01-02 19:15:14,928 - INFO - Eval: expid = 230271, Epoch = 12, avg_loss = 9.249514048937794, total_acc = 16.986301369863014%.
2024-01-02 19:15:14,928 - INFO - Epoch [12/30] (21359)  train_loss: 1.1157, train_acc: 78.08%, val_loss: 9.2495, val_acc: 16.99%, lr: 0.000121, 192.85s
2024-01-02 19:15:15,033 - INFO - {'mode': 'Train', 'epoch': 13, 'iter': 0, 'lr': 0.00012079116908177593, 'loss': 0.3491596579551697, 'acc(%)': 100.0}
2024-01-02 19:16:08,577 - INFO - {'mode': 'Train', 'epoch': 13, 'iter': 500, 'lr': 0.00012079116908177593, 'loss': 1.673240065574646, 'acc(%)': 87.250499001996}
2024-01-02 19:17:01,786 - INFO - {'mode': 'Train', 'epoch': 13, 'iter': 1000, 'lr': 0.00012079116908177593, 'loss': 0.6571665406227112, 'acc(%)': 85.36463536463536}
2024-01-02 19:17:55,090 - INFO - {'mode': 'Train', 'epoch': 13, 'iter': 1500, 'lr': 0.00012079116908177593, 'loss': 0.8295614719390869, 'acc(%)': 83.8107928047968}
2024-01-02 19:18:10,089 - INFO - Train: expid = 230271, Epoch = 13, avg_loss = 0.8552803296098463, total_acc = 83.29273862079464%.
2024-01-02 19:18:10,090 - INFO - epoch complete!
2024-01-02 19:18:10,090 - INFO - evaluating now!
2024-01-02 19:18:10,120 - INFO - {'mode': 'Eval', 'epoch': 13, 'iter': 0, 'lr': 0.00012079116908177593, 'loss': 11.066530227661133, 'acc(%)': 12.5}
2024-01-02 19:18:25,817 - INFO - {'mode': 'Eval', 'epoch': 13, 'iter': 500, 'lr': 0.00012079116908177593, 'loss': 9.731402397155762, 'acc(%)': 16.46706586826347}
2024-01-02 19:18:27,264 - INFO - Eval: expid = 230271, Epoch = 13, avg_loss = 9.565989602319727, total_acc = 16.735159817351597%.
2024-01-02 19:18:27,264 - INFO - Epoch [13/30] (23002)  train_loss: 0.8553, train_acc: 83.29%, val_loss: 9.5660, val_acc: 16.74%, lr: 0.000110, 192.34s
2024-01-02 19:18:27,372 - INFO - {'mode': 'Train', 'epoch': 14, 'iter': 0, 'lr': 0.00011045284632676536, 'loss': 0.563321590423584, 'acc(%)': 87.5}
2024-01-02 19:19:21,138 - INFO - {'mode': 'Train', 'epoch': 14, 'iter': 500, 'lr': 0.00011045284632676536, 'loss': 0.34239280223846436, 'acc(%)': 89.57085828343313}
2024-01-02 19:20:14,871 - INFO - {'mode': 'Train', 'epoch': 14, 'iter': 1000, 'lr': 0.00011045284632676536, 'loss': 0.9249281883239746, 'acc(%)': 88.82367632367632}
2024-01-02 19:21:08,701 - INFO - {'mode': 'Train', 'epoch': 14, 'iter': 1500, 'lr': 0.00011045284632676536, 'loss': 1.4986000061035156, 'acc(%)': 87.76648900732845}
2024-01-02 19:21:23,870 - INFO - Train: expid = 230271, Epoch = 14, avg_loss = 0.6596994538124992, total_acc = 87.45623382554423%.
2024-01-02 19:21:23,870 - INFO - epoch complete!
2024-01-02 19:21:23,871 - INFO - evaluating now!
2024-01-02 19:21:23,901 - INFO - {'mode': 'Eval', 'epoch': 14, 'iter': 0, 'lr': 0.00011045284632676536, 'loss': 11.217869758605957, 'acc(%)': 25.0}
2024-01-02 19:21:39,347 - INFO - {'mode': 'Eval', 'epoch': 14, 'iter': 500, 'lr': 0.00011045284632676536, 'loss': 14.350381851196289, 'acc(%)': 17.365269461077844}
2024-01-02 19:21:40,811 - INFO - Eval: expid = 230271, Epoch = 14, avg_loss = 9.69518824233312, total_acc = 17.077625570776256%.
2024-01-02 19:21:40,811 - INFO - Epoch [14/30] (24645)  train_loss: 0.6597, train_acc: 87.46%, val_loss: 9.6952, val_acc: 17.08%, lr: 0.000100, 193.55s
2024-01-02 19:21:40,924 - INFO - {'mode': 'Train', 'epoch': 15, 'iter': 0, 'lr': 0.00010000000000000003, 'loss': 0.2842037081718445, 'acc(%)': 100.0}
2024-01-02 19:22:34,041 - INFO - {'mode': 'Train', 'epoch': 15, 'iter': 500, 'lr': 0.00010000000000000003, 'loss': 0.8517430424690247, 'acc(%)': 93.23852295409182}
2024-01-02 19:23:27,296 - INFO - {'mode': 'Train', 'epoch': 15, 'iter': 1000, 'lr': 0.00010000000000000003, 'loss': 0.2475643903017044, 'acc(%)': 91.73326673326673}
2024-01-02 19:24:20,716 - INFO - {'mode': 'Train', 'epoch': 15, 'iter': 1500, 'lr': 0.00010000000000000003, 'loss': 0.9250032901763916, 'acc(%)': 90.72285143237842}
2024-01-02 19:24:35,787 - INFO - Train: expid = 230271, Epoch = 15, avg_loss = 0.5152182480818533, total_acc = 90.4703912315421%.
2024-01-02 19:24:35,788 - INFO - epoch complete!
2024-01-02 19:24:35,788 - INFO - evaluating now!
2024-01-02 19:24:35,821 - INFO - {'mode': 'Eval', 'epoch': 15, 'iter': 0, 'lr': 0.00010000000000000003, 'loss': 11.980241775512695, 'acc(%)': 0.0}
2024-01-02 19:24:51,104 - INFO - {'mode': 'Eval', 'epoch': 15, 'iter': 500, 'lr': 0.00010000000000000003, 'loss': 11.788177490234375, 'acc(%)': 17.56487025948104}
2024-01-02 19:24:52,508 - INFO - Eval: expid = 230271, Epoch = 15, avg_loss = 9.652041568494823, total_acc = 17.442922374429223%.
2024-01-02 19:24:52,509 - INFO - Epoch [15/30] (26288)  train_loss: 0.5152, train_acc: 90.47%, val_loss: 9.6520, val_acc: 17.44%, lr: 0.000090, 191.70s
2024-01-02 19:24:52,509 - WARNING - Early stopping at epoch: 15
2024-01-02 19:24:52,509 - INFO - Trained totally 16 epochs, average train time is 176.003s, average eval time is 17.252s, average train acc is 40.32%, average eval acc is 13.16%
2024-01-02 19:24:52,671 - INFO - Loaded model at 5
2024-01-02 19:24:52,756 - INFO - Save png at ./libcity/cache/230271/230271_loss.png
2024-01-02 19:24:52,821 - INFO - Save png at ./libcity/cache/230271/230271_lr.png
2024-01-02 19:24:52,885 - INFO - Save png at ./libcity/cache/230271/230271_acc.png
2024-01-02 19:24:53,256 - INFO - Saved model at ./libcity/cache/230271/model_cache/230271_LinearNextLoc_bj.pt
2024-01-02 19:24:53,257 - INFO - Start evaluating ...
2024-01-02 19:24:53,286 - INFO - {'mode': 'Test', 'epoch': 0, 'iter': 0, 'lr': 0.00018090169943749476, 'loss': 8.798211097717285, 'acc(%)': 12.5}
2024-01-02 19:25:00,997 - INFO - Test: expid = 230271, Epoch = 0, avg_loss = 8.158191090397791, total_acc = 7.430340557275541%.
2024-01-02 19:25:00,999 - INFO - Test time 7.741456508636475s.
