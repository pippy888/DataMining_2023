2024-01-02 15:16:37,253 - INFO - Log directory: ./libcity/log
2024-01-02 15:16:37,254 - INFO - Begin pretrain-pipeline, task=trajectory_embedding, model_name=LinearNextLoc, dataset_name=bj, exp_id=315043
2024-01-02 15:16:37,254 - INFO - {'task': 'trajectory_embedding', 'model': 'LinearNextLoc', 'dataset': 'bj', 'saved_model': True, 'train': True, 'gpu': True, 'gpu_id': 0, 'n_layers': 6, 'd_model': 256, 'attn_heads': 8, 'max_epoch': 30, 'batch_size': 8, 'grad_accmu_steps': 1, 'learning_rate': 0.0002, 'roadnetwork': 'bj_roadmap_edge_bj_True_1_merge', 'geo_file': 'bj_roadmap_edge_bj_True_1_merge_withdegree', 'rel_file': 'bj_roadmap_edge_bj_True_1_merge_withdegree', 'merge': True, 'min_freq': 1, 'seq_len': 128, 'test_every': 50, 'temperature': 0.05, 'contra_loss_type': 'simclr', 'classify_label': 'vflag', 'type_ln': 'post', 'add_cls': True, 'add_time_in_day': True, 'add_day_in_week': True, 'add_pe': True, 'add_temporal_bias': True, 'temporal_bias_dim': 64, 'use_mins_interval': False, 'add_gat': True, 'gat_heads_per_layer': [8, 16, 1], 'gat_features_per_layer': [16, 16, 256], 'gat_dropout': 0.1, 'gat_K': 1, 'gat_avg_last': True, 'load_trans_prob': True, 'append_degree2gcn': True, 'normal_feature': False, 'pooling': 'cls', 'dataset_class': 'NextLocDataset', 'executor': 'NextLocExecutor', 'evaluator': 'RegressionEvaluator', 'num_workers': 0, 'vocab_path': None, 'mlp_ratio': 4, 'pretrain_road_emb': None, 'future_mask': False, 'load_node2vec': False, 'dropout': 0.1, 'drop_path': 0.3, 'attn_drop': 0.1, 'seed': 0, 'learner': 'adamw', 'lr_eta_min': 0, 'lr_warmup_epoch': 4, 'lr_warmup_init': 1e-06, 'lr_decay': True, 'lr_scheduler': 'cosinelr', 'lr_decay_ratio': 0.1, 't_in_epochs': True, 'clip_grad_norm': True, 'max_grad_norm': 5, 'use_early_stop': True, 'patience': 10, 'log_batch': 500, 'log_every': 1, 'l2_reg': None, 'bidir_adj_mx': False, 'pretrain_path': None, 'freeze': False, 'metrics': ['MAE', 'RMSE', 'MAPE', 'R2', 'EVAR'], 'save_modes': ['csv', 'json'], 'device': device(type='cuda', index=0), 'exp_id': 315043}
2024-01-02 15:16:37,780 - INFO - Loading Vocab from raw_data/vocab_bj_True_1_merge.pkl
2024-01-02 15:16:37,780 - INFO - vocab_path=raw_data/vocab_bj_True_1_merge.pkl, usr_num=3, vocab_size=386
2024-01-02 15:16:37,784 - INFO - Loaded file bj_roadmap_edge_bj_True_1_merge_withdegree.geo, num_nodes=382
2024-01-02 15:16:37,786 - INFO - Loaded file bj_roadmap_edge_bj_True_1_merge_withdegree.rel, shape=(382, 382), edges=276.0
2024-01-02 15:16:37,789 - INFO - node_features: (382, 27)
2024-01-02 15:16:38,714 - INFO - node_features_encoded: torch.Size([386, 27])
2024-01-02 15:16:38,716 - INFO - edge_index: torch.Size([2, 662])
2024-01-02 15:16:38,716 - INFO - Trajectory loc-transfer prob shape=torch.Size([662, 1])
2024-01-02 15:16:38,716 - INFO - Loading Dataset!
2024-01-02 15:16:38,717 - INFO - Load dataset from raw_data/bj/cache_bj_train_True_True_1.pkl
2024-01-02 15:16:38,718 - INFO - Load dataset from raw_data/bj/cache_bj_eval_True_True_1.pkl
2024-01-02 15:16:38,718 - INFO - Load dataset from raw_data/bj/cache_bj_test_True_True_1.pkl
2024-01-02 15:16:38,718 - INFO - Size of dataset: 3/1/1
2024-01-02 15:16:38,719 - INFO - Creating Dataloader!
2024-01-02 15:16:38,721 - INFO - Building Downstream LinearNextLoc model
2024-01-02 15:16:38,721 - INFO - Building BERTDownstream model
2024-01-02 15:16:39,335 - INFO - LinearNextLoc(
  (model): BERTDownstream(
    (bert): BERT(
      (embedding): BERTEmbedding(
        (token_embedding): GAT(
          (gat_net): Sequential(
            (0): GATLayerImp3(
              (linear_proj): Linear(in_features=27, out_features=128, bias=False)
              (linear_proj_tran_prob): Linear(in_features=1, out_features=128, bias=False)
              (skip_proj): Linear(in_features=27, out_features=128, bias=False)
              (leakyReLU): LeakyReLU(negative_slope=0.2)
              (softmax): Softmax(dim=-1)
              (activation): ELU(alpha=1.0)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (1): GATLayerImp3(
              (linear_proj): Linear(in_features=128, out_features=256, bias=False)
              (linear_proj_tran_prob): Linear(in_features=1, out_features=256, bias=False)
              (skip_proj): Linear(in_features=128, out_features=256, bias=False)
              (leakyReLU): LeakyReLU(negative_slope=0.2)
              (softmax): Softmax(dim=-1)
              (activation): ELU(alpha=1.0)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (2): GATLayerImp3(
              (linear_proj): Linear(in_features=256, out_features=256, bias=False)
              (linear_proj_tran_prob): Linear(in_features=1, out_features=256, bias=False)
              (skip_proj): Linear(in_features=256, out_features=256, bias=False)
              (leakyReLU): LeakyReLU(negative_slope=0.2)
              (softmax): Softmax(dim=-1)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
        )
        (position_embedding): PositionalEmbedding()
        (daytime_embedding): Embedding(1441, 256, padding_idx=0)
        (weekday_embedding): Embedding(8, 256, padding_idx=0)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (transformer_blocks): ModuleList(
        (0): TransformerBlock(
          (attention): MultiHeadedAttention(
            (linear_layers): ModuleList(
              (0): Linear(in_features=256, out_features=256, bias=True)
              (1): Linear(in_features=256, out_features=256, bias=True)
              (2): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout): Dropout(p=0.1, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0.1, inplace=False)
            (temporal_mat_bias_1): Linear(in_features=1, out_features=64, bias=True)
            (temporal_mat_bias_2): Linear(in_features=64, out_features=1, bias=True)
          )
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0.1, inplace=False)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (drop_path): Identity()
        )
        (1): TransformerBlock(
          (attention): MultiHeadedAttention(
            (linear_layers): ModuleList(
              (0): Linear(in_features=256, out_features=256, bias=True)
              (1): Linear(in_features=256, out_features=256, bias=True)
              (2): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout): Dropout(p=0.1, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0.1, inplace=False)
            (temporal_mat_bias_1): Linear(in_features=1, out_features=64, bias=True)
            (temporal_mat_bias_2): Linear(in_features=64, out_features=1, bias=True)
          )
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0.1, inplace=False)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (drop_path): DropPath()
        )
        (2): TransformerBlock(
          (attention): MultiHeadedAttention(
            (linear_layers): ModuleList(
              (0): Linear(in_features=256, out_features=256, bias=True)
              (1): Linear(in_features=256, out_features=256, bias=True)
              (2): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout): Dropout(p=0.1, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0.1, inplace=False)
            (temporal_mat_bias_1): Linear(in_features=1, out_features=64, bias=True)
            (temporal_mat_bias_2): Linear(in_features=64, out_features=1, bias=True)
          )
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0.1, inplace=False)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (drop_path): DropPath()
        )
        (3): TransformerBlock(
          (attention): MultiHeadedAttention(
            (linear_layers): ModuleList(
              (0): Linear(in_features=256, out_features=256, bias=True)
              (1): Linear(in_features=256, out_features=256, bias=True)
              (2): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout): Dropout(p=0.1, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0.1, inplace=False)
            (temporal_mat_bias_1): Linear(in_features=1, out_features=64, bias=True)
            (temporal_mat_bias_2): Linear(in_features=64, out_features=1, bias=True)
          )
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0.1, inplace=False)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (drop_path): DropPath()
        )
        (4): TransformerBlock(
          (attention): MultiHeadedAttention(
            (linear_layers): ModuleList(
              (0): Linear(in_features=256, out_features=256, bias=True)
              (1): Linear(in_features=256, out_features=256, bias=True)
              (2): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout): Dropout(p=0.1, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0.1, inplace=False)
            (temporal_mat_bias_1): Linear(in_features=1, out_features=64, bias=True)
            (temporal_mat_bias_2): Linear(in_features=64, out_features=1, bias=True)
          )
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0.1, inplace=False)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (drop_path): DropPath()
        )
        (5): TransformerBlock(
          (attention): MultiHeadedAttention(
            (linear_layers): ModuleList(
              (0): Linear(in_features=256, out_features=256, bias=True)
              (1): Linear(in_features=256, out_features=256, bias=True)
              (2): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout): Dropout(p=0.1, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0.1, inplace=False)
            (temporal_mat_bias_1): Linear(in_features=1, out_features=64, bias=True)
            (temporal_mat_bias_2): Linear(in_features=64, out_features=1, bias=True)
          )
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0.1, inplace=False)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (drop_path): DropPath()
        )
      )
    )
  )
  (linear): Linear(in_features=256, out_features=386, bias=True)
  (softmax): LogSoftmax(dim=-1)
)
2024-01-02 15:16:39,338 - INFO - model.bert.embedding.token_embedding.gat_net.0.scoring_fn_target	torch.Size([1, 8, 16])	cuda:0	True
2024-01-02 15:16:39,338 - INFO - model.bert.embedding.token_embedding.gat_net.0.scoring_fn_source	torch.Size([1, 8, 16])	cuda:0	True
2024-01-02 15:16:39,338 - INFO - model.bert.embedding.token_embedding.gat_net.0.scoring_trans_prob	torch.Size([1, 8, 16])	cuda:0	True
2024-01-02 15:16:39,338 - INFO - model.bert.embedding.token_embedding.gat_net.0.bias	torch.Size([128])	cuda:0	True
2024-01-02 15:16:39,338 - INFO - model.bert.embedding.token_embedding.gat_net.0.linear_proj.weight	torch.Size([128, 27])	cuda:0	True
2024-01-02 15:16:39,338 - INFO - model.bert.embedding.token_embedding.gat_net.0.linear_proj_tran_prob.weight	torch.Size([128, 1])	cuda:0	True
2024-01-02 15:16:39,339 - INFO - model.bert.embedding.token_embedding.gat_net.0.skip_proj.weight	torch.Size([128, 27])	cuda:0	True
2024-01-02 15:16:39,339 - INFO - model.bert.embedding.token_embedding.gat_net.1.scoring_fn_target	torch.Size([1, 16, 16])	cuda:0	True
2024-01-02 15:16:39,339 - INFO - model.bert.embedding.token_embedding.gat_net.1.scoring_fn_source	torch.Size([1, 16, 16])	cuda:0	True
2024-01-02 15:16:39,339 - INFO - model.bert.embedding.token_embedding.gat_net.1.scoring_trans_prob	torch.Size([1, 16, 16])	cuda:0	True
2024-01-02 15:16:39,339 - INFO - model.bert.embedding.token_embedding.gat_net.1.bias	torch.Size([256])	cuda:0	True
2024-01-02 15:16:39,339 - INFO - model.bert.embedding.token_embedding.gat_net.1.linear_proj.weight	torch.Size([256, 128])	cuda:0	True
2024-01-02 15:16:39,339 - INFO - model.bert.embedding.token_embedding.gat_net.1.linear_proj_tran_prob.weight	torch.Size([256, 1])	cuda:0	True
2024-01-02 15:16:39,340 - INFO - model.bert.embedding.token_embedding.gat_net.1.skip_proj.weight	torch.Size([256, 128])	cuda:0	True
2024-01-02 15:16:39,340 - INFO - model.bert.embedding.token_embedding.gat_net.2.scoring_fn_target	torch.Size([1, 1, 256])	cuda:0	True
2024-01-02 15:16:39,340 - INFO - model.bert.embedding.token_embedding.gat_net.2.scoring_fn_source	torch.Size([1, 1, 256])	cuda:0	True
2024-01-02 15:16:39,340 - INFO - model.bert.embedding.token_embedding.gat_net.2.scoring_trans_prob	torch.Size([1, 1, 256])	cuda:0	True
2024-01-02 15:16:39,340 - INFO - model.bert.embedding.token_embedding.gat_net.2.bias	torch.Size([256])	cuda:0	True
2024-01-02 15:16:39,340 - INFO - model.bert.embedding.token_embedding.gat_net.2.linear_proj.weight	torch.Size([256, 256])	cuda:0	True
2024-01-02 15:16:39,340 - INFO - model.bert.embedding.token_embedding.gat_net.2.linear_proj_tran_prob.weight	torch.Size([256, 1])	cuda:0	True
2024-01-02 15:16:39,340 - INFO - model.bert.embedding.token_embedding.gat_net.2.skip_proj.weight	torch.Size([256, 256])	cuda:0	True
2024-01-02 15:16:39,340 - INFO - model.bert.embedding.daytime_embedding.weight	torch.Size([1441, 256])	cuda:0	True
2024-01-02 15:16:39,341 - INFO - model.bert.embedding.weekday_embedding.weight	torch.Size([8, 256])	cuda:0	True
2024-01-02 15:16:39,341 - INFO - model.bert.transformer_blocks.0.attention.linear_layers.0.weight	torch.Size([256, 256])	cuda:0	True
2024-01-02 15:16:39,341 - INFO - model.bert.transformer_blocks.0.attention.linear_layers.0.bias	torch.Size([256])	cuda:0	True
2024-01-02 15:16:39,341 - INFO - model.bert.transformer_blocks.0.attention.linear_layers.1.weight	torch.Size([256, 256])	cuda:0	True
2024-01-02 15:16:39,341 - INFO - model.bert.transformer_blocks.0.attention.linear_layers.1.bias	torch.Size([256])	cuda:0	True
2024-01-02 15:16:39,341 - INFO - model.bert.transformer_blocks.0.attention.linear_layers.2.weight	torch.Size([256, 256])	cuda:0	True
2024-01-02 15:16:39,341 - INFO - model.bert.transformer_blocks.0.attention.linear_layers.2.bias	torch.Size([256])	cuda:0	True
2024-01-02 15:16:39,341 - INFO - model.bert.transformer_blocks.0.attention.proj.weight	torch.Size([256, 256])	cuda:0	True
2024-01-02 15:16:39,342 - INFO - model.bert.transformer_blocks.0.attention.proj.bias	torch.Size([256])	cuda:0	True
2024-01-02 15:16:39,342 - INFO - model.bert.transformer_blocks.0.attention.temporal_mat_bias_1.weight	torch.Size([64, 1])	cuda:0	True
2024-01-02 15:16:39,342 - INFO - model.bert.transformer_blocks.0.attention.temporal_mat_bias_1.bias	torch.Size([64])	cuda:0	True
2024-01-02 15:16:39,342 - INFO - model.bert.transformer_blocks.0.attention.temporal_mat_bias_2.weight	torch.Size([1, 64])	cuda:0	True
2024-01-02 15:16:39,342 - INFO - model.bert.transformer_blocks.0.attention.temporal_mat_bias_2.bias	torch.Size([1])	cuda:0	True
2024-01-02 15:16:39,342 - INFO - model.bert.transformer_blocks.0.mlp.fc1.weight	torch.Size([1024, 256])	cuda:0	True
2024-01-02 15:16:39,342 - INFO - model.bert.transformer_blocks.0.mlp.fc1.bias	torch.Size([1024])	cuda:0	True
2024-01-02 15:16:39,342 - INFO - model.bert.transformer_blocks.0.mlp.fc2.weight	torch.Size([256, 1024])	cuda:0	True
2024-01-02 15:16:39,343 - INFO - model.bert.transformer_blocks.0.mlp.fc2.bias	torch.Size([256])	cuda:0	True
2024-01-02 15:16:39,343 - INFO - model.bert.transformer_blocks.0.norm1.weight	torch.Size([256])	cuda:0	True
2024-01-02 15:16:39,343 - INFO - model.bert.transformer_blocks.0.norm1.bias	torch.Size([256])	cuda:0	True
2024-01-02 15:16:39,343 - INFO - model.bert.transformer_blocks.0.norm2.weight	torch.Size([256])	cuda:0	True
2024-01-02 15:16:39,343 - INFO - model.bert.transformer_blocks.0.norm2.bias	torch.Size([256])	cuda:0	True
2024-01-02 15:16:39,343 - INFO - model.bert.transformer_blocks.1.attention.linear_layers.0.weight	torch.Size([256, 256])	cuda:0	True
2024-01-02 15:16:39,343 - INFO - model.bert.transformer_blocks.1.attention.linear_layers.0.bias	torch.Size([256])	cuda:0	True
2024-01-02 15:16:39,343 - INFO - model.bert.transformer_blocks.1.attention.linear_layers.1.weight	torch.Size([256, 256])	cuda:0	True
2024-01-02 15:16:39,344 - INFO - model.bert.transformer_blocks.1.attention.linear_layers.1.bias	torch.Size([256])	cuda:0	True
2024-01-02 15:16:39,344 - INFO - model.bert.transformer_blocks.1.attention.linear_layers.2.weight	torch.Size([256, 256])	cuda:0	True
2024-01-02 15:16:39,344 - INFO - model.bert.transformer_blocks.1.attention.linear_layers.2.bias	torch.Size([256])	cuda:0	True
2024-01-02 15:16:39,344 - INFO - model.bert.transformer_blocks.1.attention.proj.weight	torch.Size([256, 256])	cuda:0	True
2024-01-02 15:16:39,344 - INFO - model.bert.transformer_blocks.1.attention.proj.bias	torch.Size([256])	cuda:0	True
2024-01-02 15:16:39,344 - INFO - model.bert.transformer_blocks.1.attention.temporal_mat_bias_1.weight	torch.Size([64, 1])	cuda:0	True
2024-01-02 15:16:39,344 - INFO - model.bert.transformer_blocks.1.attention.temporal_mat_bias_1.bias	torch.Size([64])	cuda:0	True
2024-01-02 15:16:39,344 - INFO - model.bert.transformer_blocks.1.attention.temporal_mat_bias_2.weight	torch.Size([1, 64])	cuda:0	True
2024-01-02 15:16:39,345 - INFO - model.bert.transformer_blocks.1.attention.temporal_mat_bias_2.bias	torch.Size([1])	cuda:0	True
2024-01-02 15:16:39,345 - INFO - model.bert.transformer_blocks.1.mlp.fc1.weight	torch.Size([1024, 256])	cuda:0	True
2024-01-02 15:16:39,345 - INFO - model.bert.transformer_blocks.1.mlp.fc1.bias	torch.Size([1024])	cuda:0	True
2024-01-02 15:16:39,345 - INFO - model.bert.transformer_blocks.1.mlp.fc2.weight	torch.Size([256, 1024])	cuda:0	True
2024-01-02 15:16:39,345 - INFO - model.bert.transformer_blocks.1.mlp.fc2.bias	torch.Size([256])	cuda:0	True
2024-01-02 15:16:39,345 - INFO - model.bert.transformer_blocks.1.norm1.weight	torch.Size([256])	cuda:0	True
2024-01-02 15:16:39,345 - INFO - model.bert.transformer_blocks.1.norm1.bias	torch.Size([256])	cuda:0	True
2024-01-02 15:16:39,345 - INFO - model.bert.transformer_blocks.1.norm2.weight	torch.Size([256])	cuda:0	True
2024-01-02 15:16:39,345 - INFO - model.bert.transformer_blocks.1.norm2.bias	torch.Size([256])	cuda:0	True
2024-01-02 15:16:39,346 - INFO - model.bert.transformer_blocks.2.attention.linear_layers.0.weight	torch.Size([256, 256])	cuda:0	True
2024-01-02 15:16:39,346 - INFO - model.bert.transformer_blocks.2.attention.linear_layers.0.bias	torch.Size([256])	cuda:0	True
2024-01-02 15:16:39,346 - INFO - model.bert.transformer_blocks.2.attention.linear_layers.1.weight	torch.Size([256, 256])	cuda:0	True
2024-01-02 15:16:39,346 - INFO - model.bert.transformer_blocks.2.attention.linear_layers.1.bias	torch.Size([256])	cuda:0	True
2024-01-02 15:16:39,346 - INFO - model.bert.transformer_blocks.2.attention.linear_layers.2.weight	torch.Size([256, 256])	cuda:0	True
2024-01-02 15:16:39,346 - INFO - model.bert.transformer_blocks.2.attention.linear_layers.2.bias	torch.Size([256])	cuda:0	True
2024-01-02 15:16:39,346 - INFO - model.bert.transformer_blocks.2.attention.proj.weight	torch.Size([256, 256])	cuda:0	True
2024-01-02 15:16:39,346 - INFO - model.bert.transformer_blocks.2.attention.proj.bias	torch.Size([256])	cuda:0	True
2024-01-02 15:16:39,346 - INFO - model.bert.transformer_blocks.2.attention.temporal_mat_bias_1.weight	torch.Size([64, 1])	cuda:0	True
2024-01-02 15:16:39,347 - INFO - model.bert.transformer_blocks.2.attention.temporal_mat_bias_1.bias	torch.Size([64])	cuda:0	True
2024-01-02 15:16:39,347 - INFO - model.bert.transformer_blocks.2.attention.temporal_mat_bias_2.weight	torch.Size([1, 64])	cuda:0	True
2024-01-02 15:16:39,347 - INFO - model.bert.transformer_blocks.2.attention.temporal_mat_bias_2.bias	torch.Size([1])	cuda:0	True
2024-01-02 15:16:39,347 - INFO - model.bert.transformer_blocks.2.mlp.fc1.weight	torch.Size([1024, 256])	cuda:0	True
2024-01-02 15:16:39,347 - INFO - model.bert.transformer_blocks.2.mlp.fc1.bias	torch.Size([1024])	cuda:0	True
2024-01-02 15:16:39,347 - INFO - model.bert.transformer_blocks.2.mlp.fc2.weight	torch.Size([256, 1024])	cuda:0	True
2024-01-02 15:16:39,348 - INFO - model.bert.transformer_blocks.2.mlp.fc2.bias	torch.Size([256])	cuda:0	True
2024-01-02 15:16:39,348 - INFO - model.bert.transformer_blocks.2.norm1.weight	torch.Size([256])	cuda:0	True
2024-01-02 15:16:39,348 - INFO - model.bert.transformer_blocks.2.norm1.bias	torch.Size([256])	cuda:0	True
2024-01-02 15:16:39,348 - INFO - model.bert.transformer_blocks.2.norm2.weight	torch.Size([256])	cuda:0	True
2024-01-02 15:16:39,348 - INFO - model.bert.transformer_blocks.2.norm2.bias	torch.Size([256])	cuda:0	True
2024-01-02 15:16:39,348 - INFO - model.bert.transformer_blocks.3.attention.linear_layers.0.weight	torch.Size([256, 256])	cuda:0	True
2024-01-02 15:16:39,348 - INFO - model.bert.transformer_blocks.3.attention.linear_layers.0.bias	torch.Size([256])	cuda:0	True
2024-01-02 15:16:39,348 - INFO - model.bert.transformer_blocks.3.attention.linear_layers.1.weight	torch.Size([256, 256])	cuda:0	True
2024-01-02 15:16:39,349 - INFO - model.bert.transformer_blocks.3.attention.linear_layers.1.bias	torch.Size([256])	cuda:0	True
2024-01-02 15:16:39,349 - INFO - model.bert.transformer_blocks.3.attention.linear_layers.2.weight	torch.Size([256, 256])	cuda:0	True
2024-01-02 15:16:39,349 - INFO - model.bert.transformer_blocks.3.attention.linear_layers.2.bias	torch.Size([256])	cuda:0	True
2024-01-02 15:16:39,349 - INFO - model.bert.transformer_blocks.3.attention.proj.weight	torch.Size([256, 256])	cuda:0	True
2024-01-02 15:16:39,349 - INFO - model.bert.transformer_blocks.3.attention.proj.bias	torch.Size([256])	cuda:0	True
2024-01-02 15:16:39,349 - INFO - model.bert.transformer_blocks.3.attention.temporal_mat_bias_1.weight	torch.Size([64, 1])	cuda:0	True
2024-01-02 15:16:39,349 - INFO - model.bert.transformer_blocks.3.attention.temporal_mat_bias_1.bias	torch.Size([64])	cuda:0	True
2024-01-02 15:16:39,349 - INFO - model.bert.transformer_blocks.3.attention.temporal_mat_bias_2.weight	torch.Size([1, 64])	cuda:0	True
2024-01-02 15:16:39,350 - INFO - model.bert.transformer_blocks.3.attention.temporal_mat_bias_2.bias	torch.Size([1])	cuda:0	True
2024-01-02 15:16:39,350 - INFO - model.bert.transformer_blocks.3.mlp.fc1.weight	torch.Size([1024, 256])	cuda:0	True
2024-01-02 15:16:39,350 - INFO - model.bert.transformer_blocks.3.mlp.fc1.bias	torch.Size([1024])	cuda:0	True
2024-01-02 15:16:39,350 - INFO - model.bert.transformer_blocks.3.mlp.fc2.weight	torch.Size([256, 1024])	cuda:0	True
2024-01-02 15:16:39,350 - INFO - model.bert.transformer_blocks.3.mlp.fc2.bias	torch.Size([256])	cuda:0	True
2024-01-02 15:16:39,350 - INFO - model.bert.transformer_blocks.3.norm1.weight	torch.Size([256])	cuda:0	True
2024-01-02 15:16:39,350 - INFO - model.bert.transformer_blocks.3.norm1.bias	torch.Size([256])	cuda:0	True
2024-01-02 15:16:39,351 - INFO - model.bert.transformer_blocks.3.norm2.weight	torch.Size([256])	cuda:0	True
2024-01-02 15:16:39,351 - INFO - model.bert.transformer_blocks.3.norm2.bias	torch.Size([256])	cuda:0	True
2024-01-02 15:16:39,351 - INFO - model.bert.transformer_blocks.4.attention.linear_layers.0.weight	torch.Size([256, 256])	cuda:0	True
2024-01-02 15:16:39,351 - INFO - model.bert.transformer_blocks.4.attention.linear_layers.0.bias	torch.Size([256])	cuda:0	True
2024-01-02 15:16:39,351 - INFO - model.bert.transformer_blocks.4.attention.linear_layers.1.weight	torch.Size([256, 256])	cuda:0	True
2024-01-02 15:16:39,352 - INFO - model.bert.transformer_blocks.4.attention.linear_layers.1.bias	torch.Size([256])	cuda:0	True
2024-01-02 15:16:39,352 - INFO - model.bert.transformer_blocks.4.attention.linear_layers.2.weight	torch.Size([256, 256])	cuda:0	True
2024-01-02 15:16:39,352 - INFO - model.bert.transformer_blocks.4.attention.linear_layers.2.bias	torch.Size([256])	cuda:0	True
2024-01-02 15:16:39,352 - INFO - model.bert.transformer_blocks.4.attention.proj.weight	torch.Size([256, 256])	cuda:0	True
2024-01-02 15:16:39,352 - INFO - model.bert.transformer_blocks.4.attention.proj.bias	torch.Size([256])	cuda:0	True
2024-01-02 15:16:39,352 - INFO - model.bert.transformer_blocks.4.attention.temporal_mat_bias_1.weight	torch.Size([64, 1])	cuda:0	True
2024-01-02 15:16:39,352 - INFO - model.bert.transformer_blocks.4.attention.temporal_mat_bias_1.bias	torch.Size([64])	cuda:0	True
2024-01-02 15:16:39,352 - INFO - model.bert.transformer_blocks.4.attention.temporal_mat_bias_2.weight	torch.Size([1, 64])	cuda:0	True
2024-01-02 15:16:39,353 - INFO - model.bert.transformer_blocks.4.attention.temporal_mat_bias_2.bias	torch.Size([1])	cuda:0	True
2024-01-02 15:16:39,353 - INFO - model.bert.transformer_blocks.4.mlp.fc1.weight	torch.Size([1024, 256])	cuda:0	True
2024-01-02 15:16:39,353 - INFO - model.bert.transformer_blocks.4.mlp.fc1.bias	torch.Size([1024])	cuda:0	True
2024-01-02 15:16:39,353 - INFO - model.bert.transformer_blocks.4.mlp.fc2.weight	torch.Size([256, 1024])	cuda:0	True
2024-01-02 15:16:39,353 - INFO - model.bert.transformer_blocks.4.mlp.fc2.bias	torch.Size([256])	cuda:0	True
2024-01-02 15:16:39,353 - INFO - model.bert.transformer_blocks.4.norm1.weight	torch.Size([256])	cuda:0	True
2024-01-02 15:16:39,353 - INFO - model.bert.transformer_blocks.4.norm1.bias	torch.Size([256])	cuda:0	True
2024-01-02 15:16:39,353 - INFO - model.bert.transformer_blocks.4.norm2.weight	torch.Size([256])	cuda:0	True
2024-01-02 15:16:39,353 - INFO - model.bert.transformer_blocks.4.norm2.bias	torch.Size([256])	cuda:0	True
2024-01-02 15:16:39,354 - INFO - model.bert.transformer_blocks.5.attention.linear_layers.0.weight	torch.Size([256, 256])	cuda:0	True
2024-01-02 15:16:39,354 - INFO - model.bert.transformer_blocks.5.attention.linear_layers.0.bias	torch.Size([256])	cuda:0	True
2024-01-02 15:16:39,354 - INFO - model.bert.transformer_blocks.5.attention.linear_layers.1.weight	torch.Size([256, 256])	cuda:0	True
2024-01-02 15:16:39,354 - INFO - model.bert.transformer_blocks.5.attention.linear_layers.1.bias	torch.Size([256])	cuda:0	True
2024-01-02 15:16:39,354 - INFO - model.bert.transformer_blocks.5.attention.linear_layers.2.weight	torch.Size([256, 256])	cuda:0	True
2024-01-02 15:16:39,354 - INFO - model.bert.transformer_blocks.5.attention.linear_layers.2.bias	torch.Size([256])	cuda:0	True
2024-01-02 15:16:39,354 - INFO - model.bert.transformer_blocks.5.attention.proj.weight	torch.Size([256, 256])	cuda:0	True
2024-01-02 15:16:39,354 - INFO - model.bert.transformer_blocks.5.attention.proj.bias	torch.Size([256])	cuda:0	True
2024-01-02 15:16:39,355 - INFO - model.bert.transformer_blocks.5.attention.temporal_mat_bias_1.weight	torch.Size([64, 1])	cuda:0	True
2024-01-02 15:16:39,355 - INFO - model.bert.transformer_blocks.5.attention.temporal_mat_bias_1.bias	torch.Size([64])	cuda:0	True
2024-01-02 15:16:39,355 - INFO - model.bert.transformer_blocks.5.attention.temporal_mat_bias_2.weight	torch.Size([1, 64])	cuda:0	True
2024-01-02 15:16:39,355 - INFO - model.bert.transformer_blocks.5.attention.temporal_mat_bias_2.bias	torch.Size([1])	cuda:0	True
2024-01-02 15:16:39,355 - INFO - model.bert.transformer_blocks.5.mlp.fc1.weight	torch.Size([1024, 256])	cuda:0	True
2024-01-02 15:16:39,355 - INFO - model.bert.transformer_blocks.5.mlp.fc1.bias	torch.Size([1024])	cuda:0	True
2024-01-02 15:16:39,355 - INFO - model.bert.transformer_blocks.5.mlp.fc2.weight	torch.Size([256, 1024])	cuda:0	True
2024-01-02 15:16:39,355 - INFO - model.bert.transformer_blocks.5.mlp.fc2.bias	torch.Size([256])	cuda:0	True
2024-01-02 15:16:39,356 - INFO - model.bert.transformer_blocks.5.norm1.weight	torch.Size([256])	cuda:0	True
2024-01-02 15:16:39,356 - INFO - model.bert.transformer_blocks.5.norm1.bias	torch.Size([256])	cuda:0	True
2024-01-02 15:16:39,356 - INFO - model.bert.transformer_blocks.5.norm2.weight	torch.Size([256])	cuda:0	True
2024-01-02 15:16:39,356 - INFO - model.bert.transformer_blocks.5.norm2.bias	torch.Size([256])	cuda:0	True
2024-01-02 15:16:39,356 - INFO - linear.weight	torch.Size([386, 256])	cuda:0	True
2024-01-02 15:16:39,356 - INFO - linear.bias	torch.Size([386])	cuda:0	True
2024-01-02 15:16:39,357 - INFO - Total parameter numbers: 5416584
2024-01-02 15:16:39,357 - INFO - You select `adamw` optimizer.
2024-01-02 15:16:39,358 - INFO - You select `cosinelr` lr_scheduler.
2024-01-02 15:16:39,362 - INFO - Start training ...
2024-01-02 15:16:39,362 - INFO - Num_batches: train=1, eval=1
2024-01-02 15:16:39,997 - INFO - {'mode': 'Train', 'epoch': 0, 'iter': 0, 'lr': 1e-06, 'loss': 6.297550201416016, 'acc(%)': 0.0}
2024-01-02 15:16:39,998 - INFO - Train: expid = 315043, Epoch = 0, avg_loss = 6.297550201416016, total_acc = 0.0%.
2024-01-02 15:16:39,998 - INFO - epoch complete!
2024-01-02 15:16:39,998 - INFO - evaluating now!
2024-01-02 15:16:40,009 - INFO - {'mode': 'Eval', 'epoch': 0, 'iter': 0, 'lr': 1e-06, 'loss': 6.6447038650512695, 'acc(%)': 0.0}
2024-01-02 15:16:40,010 - INFO - Eval: expid = 315043, Epoch = 0, avg_loss = 6.6447038650512695, total_acc = 0.0%.
2024-01-02 15:16:40,010 - INFO - Epoch [0/30] (1)  train_loss: 6.2976, train_acc: 0.00%, val_loss: 6.6447, val_acc: 0.00%, lr: 0.000051, 0.65s
2024-01-02 15:16:40,209 - INFO - Saved model at 0
2024-01-02 15:16:40,209 - INFO - Val loss decrease from inf to 6.6447, saving to ./libcity/cache/315043/model_cache/LinearNextLoc_bj_epoch0.tar
2024-01-02 15:16:40,284 - INFO - {'mode': 'Train', 'epoch': 1, 'iter': 0, 'lr': 5.075e-05, 'loss': 6.217057704925537, 'acc(%)': 0.0}
2024-01-02 15:16:40,285 - INFO - Train: expid = 315043, Epoch = 1, avg_loss = 6.217057545979817, total_acc = 0.0%.
2024-01-02 15:16:40,285 - INFO - epoch complete!
2024-01-02 15:16:40,286 - INFO - evaluating now!
2024-01-02 15:16:40,296 - INFO - {'mode': 'Eval', 'epoch': 1, 'iter': 0, 'lr': 5.075e-05, 'loss': 6.663748741149902, 'acc(%)': 0.0}
2024-01-02 15:16:40,296 - INFO - Eval: expid = 315043, Epoch = 1, avg_loss = 6.663748741149902, total_acc = 0.0%.
2024-01-02 15:16:40,297 - INFO - Epoch [1/30] (2)  train_loss: 6.2171, train_acc: 0.00%, val_loss: 6.6637, val_acc: 0.00%, lr: 0.000101, 0.09s
2024-01-02 15:16:40,370 - INFO - {'mode': 'Train', 'epoch': 2, 'iter': 0, 'lr': 0.0001005, 'loss': 5.31730842590332, 'acc(%)': 0.0}
2024-01-02 15:16:40,371 - INFO - Train: expid = 315043, Epoch = 2, avg_loss = 5.31730842590332, total_acc = 0.0%.
2024-01-02 15:16:40,371 - INFO - epoch complete!
2024-01-02 15:16:40,371 - INFO - evaluating now!
2024-01-02 15:16:40,382 - INFO - {'mode': 'Eval', 'epoch': 2, 'iter': 0, 'lr': 0.0001005, 'loss': 6.691052436828613, 'acc(%)': 0.0}
2024-01-02 15:16:40,383 - INFO - Eval: expid = 315043, Epoch = 2, avg_loss = 6.691052436828613, total_acc = 0.0%.
2024-01-02 15:16:40,383 - INFO - Epoch [2/30] (3)  train_loss: 5.3173, train_acc: 0.00%, val_loss: 6.6911, val_acc: 0.00%, lr: 0.000150, 0.09s
2024-01-02 15:16:40,460 - INFO - {'mode': 'Train', 'epoch': 3, 'iter': 0, 'lr': 0.00015025000000000002, 'loss': 4.562585830688477, 'acc(%)': 100.0}
2024-01-02 15:16:40,461 - INFO - Train: expid = 315043, Epoch = 3, avg_loss = 4.562585512797038, total_acc = 100.0%.
2024-01-02 15:16:40,461 - INFO - epoch complete!
2024-01-02 15:16:40,462 - INFO - evaluating now!
2024-01-02 15:16:40,473 - INFO - {'mode': 'Eval', 'epoch': 3, 'iter': 0, 'lr': 0.00015025000000000002, 'loss': 6.688294887542725, 'acc(%)': 0.0}
2024-01-02 15:16:40,474 - INFO - Eval: expid = 315043, Epoch = 3, avg_loss = 6.688294887542725, total_acc = 0.0%.
2024-01-02 15:16:40,474 - INFO - Epoch [3/30] (4)  train_loss: 4.5626, train_acc: 100.00%, val_loss: 6.6883, val_acc: 0.00%, lr: 0.000191, 0.09s
2024-01-02 15:16:40,551 - INFO - {'mode': 'Train', 'epoch': 4, 'iter': 0, 'lr': 0.0001913545457642601, 'loss': 3.6155200004577637, 'acc(%)': 100.0}
2024-01-02 15:16:40,551 - INFO - Train: expid = 315043, Epoch = 4, avg_loss = 3.6155198415120444, total_acc = 100.0%.
2024-01-02 15:16:40,552 - INFO - epoch complete!
2024-01-02 15:16:40,552 - INFO - evaluating now!
2024-01-02 15:16:40,563 - INFO - {'mode': 'Eval', 'epoch': 4, 'iter': 0, 'lr': 0.0001913545457642601, 'loss': 6.726455211639404, 'acc(%)': 0.0}
2024-01-02 15:16:40,563 - INFO - Eval: expid = 315043, Epoch = 4, avg_loss = 6.726455211639404, total_acc = 0.0%.
2024-01-02 15:16:40,564 - INFO - Epoch [4/30] (5)  train_loss: 3.6155, train_acc: 100.00%, val_loss: 6.7265, val_acc: 0.00%, lr: 0.000187, 0.09s
2024-01-02 15:16:40,638 - INFO - {'mode': 'Train', 'epoch': 5, 'iter': 0, 'lr': 0.00018660254037844388, 'loss': 3.117952823638916, 'acc(%)': 100.0}
2024-01-02 15:16:40,638 - INFO - Train: expid = 315043, Epoch = 5, avg_loss = 3.1179526646931968, total_acc = 100.0%.
2024-01-02 15:16:40,639 - INFO - epoch complete!
2024-01-02 15:16:40,639 - INFO - evaluating now!
2024-01-02 15:16:40,650 - INFO - {'mode': 'Eval', 'epoch': 5, 'iter': 0, 'lr': 0.00018660254037844388, 'loss': 6.763549327850342, 'acc(%)': 0.0}
2024-01-02 15:16:40,651 - INFO - Eval: expid = 315043, Epoch = 5, avg_loss = 6.763549327850342, total_acc = 0.0%.
2024-01-02 15:16:40,651 - INFO - Epoch [5/30] (6)  train_loss: 3.1180, train_acc: 100.00%, val_loss: 6.7635, val_acc: 0.00%, lr: 0.000181, 0.09s
2024-01-02 15:16:40,723 - INFO - {'mode': 'Train', 'epoch': 6, 'iter': 0, 'lr': 0.00018090169943749476, 'loss': 2.2979655265808105, 'acc(%)': 100.0}
2024-01-02 15:16:40,724 - INFO - Train: expid = 315043, Epoch = 6, avg_loss = 2.2979653676350913, total_acc = 100.0%.
2024-01-02 15:16:40,724 - INFO - epoch complete!
2024-01-02 15:16:40,724 - INFO - evaluating now!
2024-01-02 15:16:40,736 - INFO - {'mode': 'Eval', 'epoch': 6, 'iter': 0, 'lr': 0.00018090169943749476, 'loss': 6.826040267944336, 'acc(%)': 0.0}
2024-01-02 15:16:40,736 - INFO - Eval: expid = 315043, Epoch = 6, avg_loss = 6.826040267944336, total_acc = 0.0%.
2024-01-02 15:16:40,736 - INFO - Epoch [6/30] (7)  train_loss: 2.2980, train_acc: 100.00%, val_loss: 6.8260, val_acc: 0.00%, lr: 0.000174, 0.09s
2024-01-02 15:16:40,809 - INFO - {'mode': 'Train', 'epoch': 7, 'iter': 0, 'lr': 0.00017431448254773944, 'loss': 2.3255677223205566, 'acc(%)': 100.0}
2024-01-02 15:16:40,809 - INFO - Train: expid = 315043, Epoch = 7, avg_loss = 2.3255675633748374, total_acc = 100.0%.
2024-01-02 15:16:40,810 - INFO - epoch complete!
2024-01-02 15:16:40,810 - INFO - evaluating now!
2024-01-02 15:16:40,821 - INFO - {'mode': 'Eval', 'epoch': 7, 'iter': 0, 'lr': 0.00017431448254773944, 'loss': 6.878979206085205, 'acc(%)': 0.0}
2024-01-02 15:16:40,821 - INFO - Eval: expid = 315043, Epoch = 7, avg_loss = 6.878979206085205, total_acc = 0.0%.
2024-01-02 15:16:40,822 - INFO - Epoch [7/30] (8)  train_loss: 2.3256, train_acc: 100.00%, val_loss: 6.8790, val_acc: 0.00%, lr: 0.000167, 0.08s
2024-01-02 15:16:40,904 - INFO - {'mode': 'Train', 'epoch': 8, 'iter': 0, 'lr': 0.00016691306063588583, 'loss': 1.972981333732605, 'acc(%)': 100.0}
2024-01-02 15:16:40,905 - INFO - Train: expid = 315043, Epoch = 8, avg_loss = 1.972981293996175, total_acc = 100.0%.
2024-01-02 15:16:40,905 - INFO - epoch complete!
2024-01-02 15:16:40,905 - INFO - evaluating now!
2024-01-02 15:16:40,917 - INFO - {'mode': 'Eval', 'epoch': 8, 'iter': 0, 'lr': 0.00016691306063588583, 'loss': 6.9266037940979, 'acc(%)': 0.0}
2024-01-02 15:16:40,917 - INFO - Eval: expid = 315043, Epoch = 8, avg_loss = 6.9266037940979, total_acc = 0.0%.
2024-01-02 15:16:40,917 - INFO - Epoch [8/30] (9)  train_loss: 1.9730, train_acc: 100.00%, val_loss: 6.9266, val_acc: 0.00%, lr: 0.000159, 0.10s
2024-01-02 15:16:40,986 - INFO - {'mode': 'Train', 'epoch': 9, 'iter': 0, 'lr': 0.00015877852522924732, 'loss': 1.9393309354782104, 'acc(%)': 100.0}
2024-01-02 15:16:40,987 - INFO - Train: expid = 315043, Epoch = 9, avg_loss = 1.9393308957417805, total_acc = 100.0%.
2024-01-02 15:16:40,987 - INFO - epoch complete!
2024-01-02 15:16:40,988 - INFO - evaluating now!
2024-01-02 15:16:41,001 - INFO - {'mode': 'Eval', 'epoch': 9, 'iter': 0, 'lr': 0.00015877852522924732, 'loss': 6.984902381896973, 'acc(%)': 0.0}
2024-01-02 15:16:41,001 - INFO - Eval: expid = 315043, Epoch = 9, avg_loss = 6.984902381896973, total_acc = 0.0%.
2024-01-02 15:16:41,001 - INFO - Epoch [9/30] (10)  train_loss: 1.9393, train_acc: 100.00%, val_loss: 6.9849, val_acc: 0.00%, lr: 0.000150, 0.08s
2024-01-02 15:16:41,076 - INFO - {'mode': 'Train', 'epoch': 10, 'iter': 0, 'lr': 0.00015000000000000001, 'loss': 1.93985116481781, 'acc(%)': 100.0}
2024-01-02 15:16:41,076 - INFO - Train: expid = 315043, Epoch = 10, avg_loss = 1.9398511250813801, total_acc = 100.0%.
2024-01-02 15:16:41,077 - INFO - epoch complete!
2024-01-02 15:16:41,077 - INFO - evaluating now!
2024-01-02 15:16:41,088 - INFO - {'mode': 'Eval', 'epoch': 10, 'iter': 0, 'lr': 0.00015000000000000001, 'loss': 7.068767547607422, 'acc(%)': 0.0}
2024-01-02 15:16:41,088 - INFO - Eval: expid = 315043, Epoch = 10, avg_loss = 7.068767547607422, total_acc = 0.0%.
2024-01-02 15:16:41,088 - INFO - Epoch [10/30] (11)  train_loss: 1.9399, train_acc: 100.00%, val_loss: 7.0688, val_acc: 0.00%, lr: 0.000141, 0.09s
2024-01-02 15:16:41,088 - WARNING - Early stopping at epoch: 10
2024-01-02 15:16:41,089 - INFO - Trained totally 11 epochs, average train time is 0.126s, average eval time is 0.012s, average train acc is 72.73%, average eval acc is 0.00%
2024-01-02 15:16:41,188 - INFO - Loaded model at 0
2024-01-02 15:16:41,258 - INFO - Save png at ./libcity/cache/315043/315043_loss.png
2024-01-02 15:16:41,318 - INFO - Save png at ./libcity/cache/315043/315043_lr.png
2024-01-02 15:16:41,375 - INFO - Save png at ./libcity/cache/315043/315043_acc.png
2024-01-02 15:16:41,517 - INFO - Saved model at ./libcity/cache/315043/model_cache/315043_LinearNextLoc_bj.pt
2024-01-02 15:16:41,518 - INFO - Start evaluating ...
2024-01-02 15:16:41,528 - INFO - {'mode': 'Test', 'epoch': 0, 'iter': 0, 'lr': 5.075e-05, 'loss': 6.172487258911133, 'acc(%)': 0.0}
2024-01-02 15:16:41,529 - INFO - Test: expid = 315043, Epoch = 0, avg_loss = 6.172487258911133, total_acc = 0.0%.
2024-01-02 15:16:41,530 - INFO - Test time 0.011999368667602539s.
