2024-01-02 13:41:24,979 - INFO - Log directory: ./libcity/log
2024-01-02 13:41:24,979 - INFO - Begin pretrain-pipeline, task=trajectory_embedding, model_name=LinearETA, dataset_name=bj, exp_id=834447
2024-01-02 13:41:24,979 - INFO - {'task': 'trajectory_embedding', 'model': 'LinearETA', 'dataset': 'bj', 'saved_model': True, 'train': True, 'gpu': True, 'gpu_id': 0, 'pretrain_path': 'libcity/cache/454824/model_cache/454824_BERTContrastiveLM_bj.pt', 'n_layers': 6, 'd_model': 256, 'attn_heads': 8, 'max_epoch': 30, 'batch_size': 8, 'grad_accmu_steps': 1, 'learning_rate': 0.0002, 'roadnetwork': 'bj_roadmap_edge_bj_True_1_merge', 'geo_file': 'bj_roadmap_edge_bj_True_1_merge_withdegree', 'rel_file': 'bj_roadmap_edge_bj_True_1_merge_withdegree', 'merge': True, 'min_freq': 1, 'seq_len': 128, 'test_every': 50, 'temperature': 0.05, 'contra_loss_type': 'simclr', 'classify_label': 'vflag', 'type_ln': 'post', 'add_cls': True, 'add_time_in_day': True, 'add_day_in_week': True, 'add_pe': True, 'add_temporal_bias': True, 'temporal_bias_dim': 64, 'use_mins_interval': False, 'add_gat': True, 'gat_heads_per_layer': [8, 16, 1], 'gat_features_per_layer': [16, 16, 256], 'gat_dropout': 0.1, 'gat_K': 1, 'gat_avg_last': True, 'load_trans_prob': True, 'append_degree2gcn': True, 'normal_feature': False, 'pooling': 'cls', 'dataset_class': 'ETADataset', 'executor': 'ETAExecutor', 'evaluator': 'RegressionEvaluator', 'num_workers': 0, 'vocab_path': None, 'mlp_ratio': 4, 'pretrain_road_emb': None, 'future_mask': False, 'load_node2vec': False, 'dropout': 0.1, 'drop_path': 0.3, 'attn_drop': 0.1, 'seed': 0, 'learner': 'adamw', 'lr_eta_min': 0, 'lr_warmup_epoch': 4, 'lr_warmup_init': 1e-06, 'lr_decay': True, 'lr_scheduler': 'cosinelr', 'lr_decay_ratio': 0.1, 't_in_epochs': True, 'clip_grad_norm': True, 'max_grad_norm': 5, 'use_early_stop': True, 'patience': 10, 'log_batch': 500, 'log_every': 1, 'l2_reg': None, 'bidir_adj_mx': False, 'freeze': False, 'metrics': ['MAE', 'RMSE', 'MAPE', 'R2', 'EVAR'], 'save_modes': ['csv', 'json'], 'device': device(type='cuda', index=0), 'exp_id': 834447}
2024-01-02 13:41:25,488 - INFO - Loading Vocab from raw_data/vocab_bj_True_1_merge.pkl
2024-01-02 13:41:25,495 - INFO - vocab_path=raw_data/vocab_bj_True_1_merge.pkl, usr_num=75, vocab_size=28547
2024-01-02 13:41:25,544 - INFO - Loaded file bj_roadmap_edge_bj_True_1_merge_withdegree.geo, num_nodes=28543
2024-01-02 13:41:27,058 - INFO - Loaded file bj_roadmap_edge_bj_True_1_merge_withdegree.rel, shape=(28543, 28543), edges=54307.0
2024-01-02 13:41:27,063 - INFO - node_features: (28543, 42)
2024-01-02 13:41:28,005 - INFO - node_features_encoded: torch.Size([28547, 42])
2024-01-02 13:41:28,153 - INFO - edge_index: torch.Size([2, 82815])
2024-01-02 13:41:28,156 - INFO - Trajectory loc-transfer prob shape=torch.Size([82815, 1])
2024-01-02 13:41:28,166 - INFO - Loading Dataset!
2024-01-02 13:41:28,477 - INFO - Processing dataset in TrajectoryProcessingDataset!
2024-01-02 13:42:28,287 - INFO - Processing dataset in TrajectoryProcessingDataset!
2024-01-02 13:42:48,610 - INFO - Processing dataset in TrajectoryProcessingDataset!
2024-01-02 13:42:52,182 - INFO - Size of dataset: 13138/4380/4389
2024-01-02 13:42:52,182 - INFO - Creating Dataloader!
2024-01-02 13:42:52,185 - INFO - Building Downstream LinearETA model
2024-01-02 13:42:52,186 - INFO - Building BERTDownstream model
2024-01-02 13:42:53,071 - INFO - LinearETA(
  (model): BERTDownstream(
    (bert): BERT(
      (embedding): BERTEmbedding(
        (token_embedding): GAT(
          (gat_net): Sequential(
            (0): GATLayerImp3(
              (linear_proj): Linear(in_features=42, out_features=128, bias=False)
              (linear_proj_tran_prob): Linear(in_features=1, out_features=128, bias=False)
              (skip_proj): Linear(in_features=42, out_features=128, bias=False)
              (leakyReLU): LeakyReLU(negative_slope=0.2)
              (softmax): Softmax(dim=-1)
              (activation): ELU(alpha=1.0)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (1): GATLayerImp3(
              (linear_proj): Linear(in_features=128, out_features=256, bias=False)
              (linear_proj_tran_prob): Linear(in_features=1, out_features=256, bias=False)
              (skip_proj): Linear(in_features=128, out_features=256, bias=False)
              (leakyReLU): LeakyReLU(negative_slope=0.2)
              (softmax): Softmax(dim=-1)
              (activation): ELU(alpha=1.0)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (2): GATLayerImp3(
              (linear_proj): Linear(in_features=256, out_features=256, bias=False)
              (linear_proj_tran_prob): Linear(in_features=1, out_features=256, bias=False)
              (skip_proj): Linear(in_features=256, out_features=256, bias=False)
              (leakyReLU): LeakyReLU(negative_slope=0.2)
              (softmax): Softmax(dim=-1)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
        )
        (position_embedding): PositionalEmbedding()
        (daytime_embedding): Embedding(1441, 256, padding_idx=0)
        (weekday_embedding): Embedding(8, 256, padding_idx=0)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (transformer_blocks): ModuleList(
        (0): TransformerBlock(
          (attention): MultiHeadedAttention(
            (linear_layers): ModuleList(
              (0): Linear(in_features=256, out_features=256, bias=True)
              (1): Linear(in_features=256, out_features=256, bias=True)
              (2): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout): Dropout(p=0.1, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0.1, inplace=False)
            (temporal_mat_bias_1): Linear(in_features=1, out_features=64, bias=True)
            (temporal_mat_bias_2): Linear(in_features=64, out_features=1, bias=True)
          )
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0.1, inplace=False)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (drop_path): Identity()
        )
        (1): TransformerBlock(
          (attention): MultiHeadedAttention(
            (linear_layers): ModuleList(
              (0): Linear(in_features=256, out_features=256, bias=True)
              (1): Linear(in_features=256, out_features=256, bias=True)
              (2): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout): Dropout(p=0.1, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0.1, inplace=False)
            (temporal_mat_bias_1): Linear(in_features=1, out_features=64, bias=True)
            (temporal_mat_bias_2): Linear(in_features=64, out_features=1, bias=True)
          )
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0.1, inplace=False)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (drop_path): DropPath()
        )
        (2): TransformerBlock(
          (attention): MultiHeadedAttention(
            (linear_layers): ModuleList(
              (0): Linear(in_features=256, out_features=256, bias=True)
              (1): Linear(in_features=256, out_features=256, bias=True)
              (2): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout): Dropout(p=0.1, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0.1, inplace=False)
            (temporal_mat_bias_1): Linear(in_features=1, out_features=64, bias=True)
            (temporal_mat_bias_2): Linear(in_features=64, out_features=1, bias=True)
          )
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0.1, inplace=False)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (drop_path): DropPath()
        )
        (3): TransformerBlock(
          (attention): MultiHeadedAttention(
            (linear_layers): ModuleList(
              (0): Linear(in_features=256, out_features=256, bias=True)
              (1): Linear(in_features=256, out_features=256, bias=True)
              (2): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout): Dropout(p=0.1, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0.1, inplace=False)
            (temporal_mat_bias_1): Linear(in_features=1, out_features=64, bias=True)
            (temporal_mat_bias_2): Linear(in_features=64, out_features=1, bias=True)
          )
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0.1, inplace=False)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (drop_path): DropPath()
        )
        (4): TransformerBlock(
          (attention): MultiHeadedAttention(
            (linear_layers): ModuleList(
              (0): Linear(in_features=256, out_features=256, bias=True)
              (1): Linear(in_features=256, out_features=256, bias=True)
              (2): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout): Dropout(p=0.1, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0.1, inplace=False)
            (temporal_mat_bias_1): Linear(in_features=1, out_features=64, bias=True)
            (temporal_mat_bias_2): Linear(in_features=64, out_features=1, bias=True)
          )
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0.1, inplace=False)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (drop_path): DropPath()
        )
        (5): TransformerBlock(
          (attention): MultiHeadedAttention(
            (linear_layers): ModuleList(
              (0): Linear(in_features=256, out_features=256, bias=True)
              (1): Linear(in_features=256, out_features=256, bias=True)
              (2): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout): Dropout(p=0.1, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0.1, inplace=False)
            (temporal_mat_bias_1): Linear(in_features=1, out_features=64, bias=True)
            (temporal_mat_bias_2): Linear(in_features=64, out_features=1, bias=True)
          )
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0.1, inplace=False)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (drop_path): DropPath()
        )
      )
    )
  )
  (linear): Linear(in_features=256, out_features=1, bias=True)
)
2024-01-02 13:42:53,074 - INFO - model.bert.embedding.token_embedding.gat_net.0.scoring_fn_target	torch.Size([1, 8, 16])	cuda:0	True
2024-01-02 13:42:53,075 - INFO - model.bert.embedding.token_embedding.gat_net.0.scoring_fn_source	torch.Size([1, 8, 16])	cuda:0	True
2024-01-02 13:42:53,075 - INFO - model.bert.embedding.token_embedding.gat_net.0.scoring_trans_prob	torch.Size([1, 8, 16])	cuda:0	True
2024-01-02 13:42:53,075 - INFO - model.bert.embedding.token_embedding.gat_net.0.bias	torch.Size([128])	cuda:0	True
2024-01-02 13:42:53,075 - INFO - model.bert.embedding.token_embedding.gat_net.0.linear_proj.weight	torch.Size([128, 42])	cuda:0	True
2024-01-02 13:42:53,075 - INFO - model.bert.embedding.token_embedding.gat_net.0.linear_proj_tran_prob.weight	torch.Size([128, 1])	cuda:0	True
2024-01-02 13:42:53,075 - INFO - model.bert.embedding.token_embedding.gat_net.0.skip_proj.weight	torch.Size([128, 42])	cuda:0	True
2024-01-02 13:42:53,076 - INFO - model.bert.embedding.token_embedding.gat_net.1.scoring_fn_target	torch.Size([1, 16, 16])	cuda:0	True
2024-01-02 13:42:53,076 - INFO - model.bert.embedding.token_embedding.gat_net.1.scoring_fn_source	torch.Size([1, 16, 16])	cuda:0	True
2024-01-02 13:42:53,076 - INFO - model.bert.embedding.token_embedding.gat_net.1.scoring_trans_prob	torch.Size([1, 16, 16])	cuda:0	True
2024-01-02 13:42:53,076 - INFO - model.bert.embedding.token_embedding.gat_net.1.bias	torch.Size([256])	cuda:0	True
2024-01-02 13:42:53,076 - INFO - model.bert.embedding.token_embedding.gat_net.1.linear_proj.weight	torch.Size([256, 128])	cuda:0	True
2024-01-02 13:42:53,076 - INFO - model.bert.embedding.token_embedding.gat_net.1.linear_proj_tran_prob.weight	torch.Size([256, 1])	cuda:0	True
2024-01-02 13:42:53,076 - INFO - model.bert.embedding.token_embedding.gat_net.1.skip_proj.weight	torch.Size([256, 128])	cuda:0	True
2024-01-02 13:42:53,076 - INFO - model.bert.embedding.token_embedding.gat_net.2.scoring_fn_target	torch.Size([1, 1, 256])	cuda:0	True
2024-01-02 13:42:53,076 - INFO - model.bert.embedding.token_embedding.gat_net.2.scoring_fn_source	torch.Size([1, 1, 256])	cuda:0	True
2024-01-02 13:42:53,077 - INFO - model.bert.embedding.token_embedding.gat_net.2.scoring_trans_prob	torch.Size([1, 1, 256])	cuda:0	True
2024-01-02 13:42:53,077 - INFO - model.bert.embedding.token_embedding.gat_net.2.bias	torch.Size([256])	cuda:0	True
2024-01-02 13:42:53,077 - INFO - model.bert.embedding.token_embedding.gat_net.2.linear_proj.weight	torch.Size([256, 256])	cuda:0	True
2024-01-02 13:42:53,077 - INFO - model.bert.embedding.token_embedding.gat_net.2.linear_proj_tran_prob.weight	torch.Size([256, 1])	cuda:0	True
2024-01-02 13:42:53,077 - INFO - model.bert.embedding.token_embedding.gat_net.2.skip_proj.weight	torch.Size([256, 256])	cuda:0	True
2024-01-02 13:42:53,077 - INFO - model.bert.embedding.daytime_embedding.weight	torch.Size([1441, 256])	cuda:0	True
2024-01-02 13:42:53,077 - INFO - model.bert.embedding.weekday_embedding.weight	torch.Size([8, 256])	cuda:0	True
2024-01-02 13:42:53,077 - INFO - model.bert.transformer_blocks.0.attention.linear_layers.0.weight	torch.Size([256, 256])	cuda:0	True
2024-01-02 13:42:53,078 - INFO - model.bert.transformer_blocks.0.attention.linear_layers.0.bias	torch.Size([256])	cuda:0	True
2024-01-02 13:42:53,078 - INFO - model.bert.transformer_blocks.0.attention.linear_layers.1.weight	torch.Size([256, 256])	cuda:0	True
2024-01-02 13:42:53,078 - INFO - model.bert.transformer_blocks.0.attention.linear_layers.1.bias	torch.Size([256])	cuda:0	True
2024-01-02 13:42:53,078 - INFO - model.bert.transformer_blocks.0.attention.linear_layers.2.weight	torch.Size([256, 256])	cuda:0	True
2024-01-02 13:42:53,078 - INFO - model.bert.transformer_blocks.0.attention.linear_layers.2.bias	torch.Size([256])	cuda:0	True
2024-01-02 13:42:53,078 - INFO - model.bert.transformer_blocks.0.attention.proj.weight	torch.Size([256, 256])	cuda:0	True
2024-01-02 13:42:53,078 - INFO - model.bert.transformer_blocks.0.attention.proj.bias	torch.Size([256])	cuda:0	True
2024-01-02 13:42:53,078 - INFO - model.bert.transformer_blocks.0.attention.temporal_mat_bias_1.weight	torch.Size([64, 1])	cuda:0	True
2024-01-02 13:42:53,079 - INFO - model.bert.transformer_blocks.0.attention.temporal_mat_bias_1.bias	torch.Size([64])	cuda:0	True
2024-01-02 13:42:53,079 - INFO - model.bert.transformer_blocks.0.attention.temporal_mat_bias_2.weight	torch.Size([1, 64])	cuda:0	True
2024-01-02 13:42:53,079 - INFO - model.bert.transformer_blocks.0.attention.temporal_mat_bias_2.bias	torch.Size([1])	cuda:0	True
2024-01-02 13:42:53,079 - INFO - model.bert.transformer_blocks.0.mlp.fc1.weight	torch.Size([1024, 256])	cuda:0	True
2024-01-02 13:42:53,079 - INFO - model.bert.transformer_blocks.0.mlp.fc1.bias	torch.Size([1024])	cuda:0	True
2024-01-02 13:42:53,079 - INFO - model.bert.transformer_blocks.0.mlp.fc2.weight	torch.Size([256, 1024])	cuda:0	True
2024-01-02 13:42:53,080 - INFO - model.bert.transformer_blocks.0.mlp.fc2.bias	torch.Size([256])	cuda:0	True
2024-01-02 13:42:53,080 - INFO - model.bert.transformer_blocks.0.norm1.weight	torch.Size([256])	cuda:0	True
2024-01-02 13:42:53,080 - INFO - model.bert.transformer_blocks.0.norm1.bias	torch.Size([256])	cuda:0	True
2024-01-02 13:42:53,080 - INFO - model.bert.transformer_blocks.0.norm2.weight	torch.Size([256])	cuda:0	True
2024-01-02 13:42:53,080 - INFO - model.bert.transformer_blocks.0.norm2.bias	torch.Size([256])	cuda:0	True
2024-01-02 13:42:53,080 - INFO - model.bert.transformer_blocks.1.attention.linear_layers.0.weight	torch.Size([256, 256])	cuda:0	True
2024-01-02 13:42:53,080 - INFO - model.bert.transformer_blocks.1.attention.linear_layers.0.bias	torch.Size([256])	cuda:0	True
2024-01-02 13:42:53,080 - INFO - model.bert.transformer_blocks.1.attention.linear_layers.1.weight	torch.Size([256, 256])	cuda:0	True
2024-01-02 13:42:53,080 - INFO - model.bert.transformer_blocks.1.attention.linear_layers.1.bias	torch.Size([256])	cuda:0	True
2024-01-02 13:42:53,081 - INFO - model.bert.transformer_blocks.1.attention.linear_layers.2.weight	torch.Size([256, 256])	cuda:0	True
2024-01-02 13:42:53,081 - INFO - model.bert.transformer_blocks.1.attention.linear_layers.2.bias	torch.Size([256])	cuda:0	True
2024-01-02 13:42:53,081 - INFO - model.bert.transformer_blocks.1.attention.proj.weight	torch.Size([256, 256])	cuda:0	True
2024-01-02 13:42:53,081 - INFO - model.bert.transformer_blocks.1.attention.proj.bias	torch.Size([256])	cuda:0	True
2024-01-02 13:42:53,081 - INFO - model.bert.transformer_blocks.1.attention.temporal_mat_bias_1.weight	torch.Size([64, 1])	cuda:0	True
2024-01-02 13:42:53,081 - INFO - model.bert.transformer_blocks.1.attention.temporal_mat_bias_1.bias	torch.Size([64])	cuda:0	True
2024-01-02 13:42:53,081 - INFO - model.bert.transformer_blocks.1.attention.temporal_mat_bias_2.weight	torch.Size([1, 64])	cuda:0	True
2024-01-02 13:42:53,081 - INFO - model.bert.transformer_blocks.1.attention.temporal_mat_bias_2.bias	torch.Size([1])	cuda:0	True
2024-01-02 13:42:53,081 - INFO - model.bert.transformer_blocks.1.mlp.fc1.weight	torch.Size([1024, 256])	cuda:0	True
2024-01-02 13:42:53,082 - INFO - model.bert.transformer_blocks.1.mlp.fc1.bias	torch.Size([1024])	cuda:0	True
2024-01-02 13:42:53,082 - INFO - model.bert.transformer_blocks.1.mlp.fc2.weight	torch.Size([256, 1024])	cuda:0	True
2024-01-02 13:42:53,082 - INFO - model.bert.transformer_blocks.1.mlp.fc2.bias	torch.Size([256])	cuda:0	True
2024-01-02 13:42:53,082 - INFO - model.bert.transformer_blocks.1.norm1.weight	torch.Size([256])	cuda:0	True
2024-01-02 13:42:53,082 - INFO - model.bert.transformer_blocks.1.norm1.bias	torch.Size([256])	cuda:0	True
2024-01-02 13:42:53,082 - INFO - model.bert.transformer_blocks.1.norm2.weight	torch.Size([256])	cuda:0	True
2024-01-02 13:42:53,082 - INFO - model.bert.transformer_blocks.1.norm2.bias	torch.Size([256])	cuda:0	True
2024-01-02 13:42:53,082 - INFO - model.bert.transformer_blocks.2.attention.linear_layers.0.weight	torch.Size([256, 256])	cuda:0	True
2024-01-02 13:42:53,083 - INFO - model.bert.transformer_blocks.2.attention.linear_layers.0.bias	torch.Size([256])	cuda:0	True
2024-01-02 13:42:53,083 - INFO - model.bert.transformer_blocks.2.attention.linear_layers.1.weight	torch.Size([256, 256])	cuda:0	True
2024-01-02 13:42:53,083 - INFO - model.bert.transformer_blocks.2.attention.linear_layers.1.bias	torch.Size([256])	cuda:0	True
2024-01-02 13:42:53,083 - INFO - model.bert.transformer_blocks.2.attention.linear_layers.2.weight	torch.Size([256, 256])	cuda:0	True
2024-01-02 13:42:53,083 - INFO - model.bert.transformer_blocks.2.attention.linear_layers.2.bias	torch.Size([256])	cuda:0	True
2024-01-02 13:42:53,083 - INFO - model.bert.transformer_blocks.2.attention.proj.weight	torch.Size([256, 256])	cuda:0	True
2024-01-02 13:42:53,083 - INFO - model.bert.transformer_blocks.2.attention.proj.bias	torch.Size([256])	cuda:0	True
2024-01-02 13:42:53,083 - INFO - model.bert.transformer_blocks.2.attention.temporal_mat_bias_1.weight	torch.Size([64, 1])	cuda:0	True
2024-01-02 13:42:53,084 - INFO - model.bert.transformer_blocks.2.attention.temporal_mat_bias_1.bias	torch.Size([64])	cuda:0	True
2024-01-02 13:42:53,084 - INFO - model.bert.transformer_blocks.2.attention.temporal_mat_bias_2.weight	torch.Size([1, 64])	cuda:0	True
2024-01-02 13:42:53,084 - INFO - model.bert.transformer_blocks.2.attention.temporal_mat_bias_2.bias	torch.Size([1])	cuda:0	True
2024-01-02 13:42:53,084 - INFO - model.bert.transformer_blocks.2.mlp.fc1.weight	torch.Size([1024, 256])	cuda:0	True
2024-01-02 13:42:53,084 - INFO - model.bert.transformer_blocks.2.mlp.fc1.bias	torch.Size([1024])	cuda:0	True
2024-01-02 13:42:53,084 - INFO - model.bert.transformer_blocks.2.mlp.fc2.weight	torch.Size([256, 1024])	cuda:0	True
2024-01-02 13:42:53,084 - INFO - model.bert.transformer_blocks.2.mlp.fc2.bias	torch.Size([256])	cuda:0	True
2024-01-02 13:42:53,084 - INFO - model.bert.transformer_blocks.2.norm1.weight	torch.Size([256])	cuda:0	True
2024-01-02 13:42:53,085 - INFO - model.bert.transformer_blocks.2.norm1.bias	torch.Size([256])	cuda:0	True
2024-01-02 13:42:53,085 - INFO - model.bert.transformer_blocks.2.norm2.weight	torch.Size([256])	cuda:0	True
2024-01-02 13:42:53,085 - INFO - model.bert.transformer_blocks.2.norm2.bias	torch.Size([256])	cuda:0	True
2024-01-02 13:42:53,085 - INFO - model.bert.transformer_blocks.3.attention.linear_layers.0.weight	torch.Size([256, 256])	cuda:0	True
2024-01-02 13:42:53,085 - INFO - model.bert.transformer_blocks.3.attention.linear_layers.0.bias	torch.Size([256])	cuda:0	True
2024-01-02 13:42:53,085 - INFO - model.bert.transformer_blocks.3.attention.linear_layers.1.weight	torch.Size([256, 256])	cuda:0	True
2024-01-02 13:42:53,085 - INFO - model.bert.transformer_blocks.3.attention.linear_layers.1.bias	torch.Size([256])	cuda:0	True
2024-01-02 13:42:53,085 - INFO - model.bert.transformer_blocks.3.attention.linear_layers.2.weight	torch.Size([256, 256])	cuda:0	True
2024-01-02 13:42:53,085 - INFO - model.bert.transformer_blocks.3.attention.linear_layers.2.bias	torch.Size([256])	cuda:0	True
2024-01-02 13:42:53,086 - INFO - model.bert.transformer_blocks.3.attention.proj.weight	torch.Size([256, 256])	cuda:0	True
2024-01-02 13:42:53,086 - INFO - model.bert.transformer_blocks.3.attention.proj.bias	torch.Size([256])	cuda:0	True
2024-01-02 13:42:53,086 - INFO - model.bert.transformer_blocks.3.attention.temporal_mat_bias_1.weight	torch.Size([64, 1])	cuda:0	True
2024-01-02 13:42:53,086 - INFO - model.bert.transformer_blocks.3.attention.temporal_mat_bias_1.bias	torch.Size([64])	cuda:0	True
2024-01-02 13:42:53,086 - INFO - model.bert.transformer_blocks.3.attention.temporal_mat_bias_2.weight	torch.Size([1, 64])	cuda:0	True
2024-01-02 13:42:53,086 - INFO - model.bert.transformer_blocks.3.attention.temporal_mat_bias_2.bias	torch.Size([1])	cuda:0	True
2024-01-02 13:42:53,086 - INFO - model.bert.transformer_blocks.3.mlp.fc1.weight	torch.Size([1024, 256])	cuda:0	True
2024-01-02 13:42:53,086 - INFO - model.bert.transformer_blocks.3.mlp.fc1.bias	torch.Size([1024])	cuda:0	True
2024-01-02 13:42:53,087 - INFO - model.bert.transformer_blocks.3.mlp.fc2.weight	torch.Size([256, 1024])	cuda:0	True
2024-01-02 13:42:53,087 - INFO - model.bert.transformer_blocks.3.mlp.fc2.bias	torch.Size([256])	cuda:0	True
2024-01-02 13:42:53,087 - INFO - model.bert.transformer_blocks.3.norm1.weight	torch.Size([256])	cuda:0	True
2024-01-02 13:42:53,087 - INFO - model.bert.transformer_blocks.3.norm1.bias	torch.Size([256])	cuda:0	True
2024-01-02 13:42:53,087 - INFO - model.bert.transformer_blocks.3.norm2.weight	torch.Size([256])	cuda:0	True
2024-01-02 13:42:53,087 - INFO - model.bert.transformer_blocks.3.norm2.bias	torch.Size([256])	cuda:0	True
2024-01-02 13:42:53,088 - INFO - model.bert.transformer_blocks.4.attention.linear_layers.0.weight	torch.Size([256, 256])	cuda:0	True
2024-01-02 13:42:53,088 - INFO - model.bert.transformer_blocks.4.attention.linear_layers.0.bias	torch.Size([256])	cuda:0	True
2024-01-02 13:42:53,088 - INFO - model.bert.transformer_blocks.4.attention.linear_layers.1.weight	torch.Size([256, 256])	cuda:0	True
2024-01-02 13:42:53,088 - INFO - model.bert.transformer_blocks.4.attention.linear_layers.1.bias	torch.Size([256])	cuda:0	True
2024-01-02 13:42:53,088 - INFO - model.bert.transformer_blocks.4.attention.linear_layers.2.weight	torch.Size([256, 256])	cuda:0	True
2024-01-02 13:42:53,088 - INFO - model.bert.transformer_blocks.4.attention.linear_layers.2.bias	torch.Size([256])	cuda:0	True
2024-01-02 13:42:53,088 - INFO - model.bert.transformer_blocks.4.attention.proj.weight	torch.Size([256, 256])	cuda:0	True
2024-01-02 13:42:53,089 - INFO - model.bert.transformer_blocks.4.attention.proj.bias	torch.Size([256])	cuda:0	True
2024-01-02 13:42:53,089 - INFO - model.bert.transformer_blocks.4.attention.temporal_mat_bias_1.weight	torch.Size([64, 1])	cuda:0	True
2024-01-02 13:42:53,089 - INFO - model.bert.transformer_blocks.4.attention.temporal_mat_bias_1.bias	torch.Size([64])	cuda:0	True
2024-01-02 13:42:53,089 - INFO - model.bert.transformer_blocks.4.attention.temporal_mat_bias_2.weight	torch.Size([1, 64])	cuda:0	True
2024-01-02 13:42:53,089 - INFO - model.bert.transformer_blocks.4.attention.temporal_mat_bias_2.bias	torch.Size([1])	cuda:0	True
2024-01-02 13:42:53,089 - INFO - model.bert.transformer_blocks.4.mlp.fc1.weight	torch.Size([1024, 256])	cuda:0	True
2024-01-02 13:42:53,089 - INFO - model.bert.transformer_blocks.4.mlp.fc1.bias	torch.Size([1024])	cuda:0	True
2024-01-02 13:42:53,089 - INFO - model.bert.transformer_blocks.4.mlp.fc2.weight	torch.Size([256, 1024])	cuda:0	True
2024-01-02 13:42:53,089 - INFO - model.bert.transformer_blocks.4.mlp.fc2.bias	torch.Size([256])	cuda:0	True
2024-01-02 13:42:53,090 - INFO - model.bert.transformer_blocks.4.norm1.weight	torch.Size([256])	cuda:0	True
2024-01-02 13:42:53,090 - INFO - model.bert.transformer_blocks.4.norm1.bias	torch.Size([256])	cuda:0	True
2024-01-02 13:42:53,090 - INFO - model.bert.transformer_blocks.4.norm2.weight	torch.Size([256])	cuda:0	True
2024-01-02 13:42:53,090 - INFO - model.bert.transformer_blocks.4.norm2.bias	torch.Size([256])	cuda:0	True
2024-01-02 13:42:53,090 - INFO - model.bert.transformer_blocks.5.attention.linear_layers.0.weight	torch.Size([256, 256])	cuda:0	True
2024-01-02 13:42:53,090 - INFO - model.bert.transformer_blocks.5.attention.linear_layers.0.bias	torch.Size([256])	cuda:0	True
2024-01-02 13:42:53,090 - INFO - model.bert.transformer_blocks.5.attention.linear_layers.1.weight	torch.Size([256, 256])	cuda:0	True
2024-01-02 13:42:53,090 - INFO - model.bert.transformer_blocks.5.attention.linear_layers.1.bias	torch.Size([256])	cuda:0	True
2024-01-02 13:42:53,091 - INFO - model.bert.transformer_blocks.5.attention.linear_layers.2.weight	torch.Size([256, 256])	cuda:0	True
2024-01-02 13:42:53,091 - INFO - model.bert.transformer_blocks.5.attention.linear_layers.2.bias	torch.Size([256])	cuda:0	True
2024-01-02 13:42:53,091 - INFO - model.bert.transformer_blocks.5.attention.proj.weight	torch.Size([256, 256])	cuda:0	True
2024-01-02 13:42:53,091 - INFO - model.bert.transformer_blocks.5.attention.proj.bias	torch.Size([256])	cuda:0	True
2024-01-02 13:42:53,091 - INFO - model.bert.transformer_blocks.5.attention.temporal_mat_bias_1.weight	torch.Size([64, 1])	cuda:0	True
2024-01-02 13:42:53,091 - INFO - model.bert.transformer_blocks.5.attention.temporal_mat_bias_1.bias	torch.Size([64])	cuda:0	True
2024-01-02 13:42:53,091 - INFO - model.bert.transformer_blocks.5.attention.temporal_mat_bias_2.weight	torch.Size([1, 64])	cuda:0	True
2024-01-02 13:42:53,092 - INFO - model.bert.transformer_blocks.5.attention.temporal_mat_bias_2.bias	torch.Size([1])	cuda:0	True
2024-01-02 13:42:53,092 - INFO - model.bert.transformer_blocks.5.mlp.fc1.weight	torch.Size([1024, 256])	cuda:0	True
2024-01-02 13:42:53,092 - INFO - model.bert.transformer_blocks.5.mlp.fc1.bias	torch.Size([1024])	cuda:0	True
2024-01-02 13:42:53,092 - INFO - model.bert.transformer_blocks.5.mlp.fc2.weight	torch.Size([256, 1024])	cuda:0	True
2024-01-02 13:42:53,092 - INFO - model.bert.transformer_blocks.5.mlp.fc2.bias	torch.Size([256])	cuda:0	True
2024-01-02 13:42:53,092 - INFO - model.bert.transformer_blocks.5.norm1.weight	torch.Size([256])	cuda:0	True
2024-01-02 13:42:53,092 - INFO - model.bert.transformer_blocks.5.norm1.bias	torch.Size([256])	cuda:0	True
2024-01-02 13:42:53,093 - INFO - model.bert.transformer_blocks.5.norm2.weight	torch.Size([256])	cuda:0	True
2024-01-02 13:42:53,093 - INFO - model.bert.transformer_blocks.5.norm2.bias	torch.Size([256])	cuda:0	True
2024-01-02 13:42:53,093 - INFO - linear.weight	torch.Size([1, 256])	cuda:0	True
2024-01-02 13:42:53,093 - INFO - linear.bias	torch.Size([1])	cuda:0	True
2024-01-02 13:42:53,094 - INFO - Total parameter numbers: 5321479
2024-01-02 13:42:53,094 - INFO - You select `adamw` optimizer.
2024-01-02 13:42:53,095 - INFO - You select `cosinelr` lr_scheduler.
2024-01-02 13:42:53,250 - INFO - Load Pretrained-Model from libcity/cache/454824/model_cache/454824_BERTContrastiveLM_bj.pt
2024-01-02 13:42:53,303 - INFO - Start training ...
2024-01-02 13:42:53,304 - INFO - Num_batches: train=1643, eval=548
2024-01-02 13:42:54,941 - INFO - {'mode': 'Train', 'epoch': 0, 'iter': 0, 'lr': 1e-06, 'loss': 258.52947998046875}
2024-01-02 13:43:43,915 - INFO - {'mode': 'Train', 'epoch': 0, 'iter': 500, 'lr': 1e-06, 'loss': 182.7052764892578}
2024-01-02 13:44:33,226 - INFO - {'mode': 'Train', 'epoch': 0, 'iter': 1000, 'lr': 1e-06, 'loss': 82.16829681396484}
2024-01-02 13:45:23,168 - INFO - {'mode': 'Train', 'epoch': 0, 'iter': 1500, 'lr': 1e-06, 'loss': 41.519775390625}
2024-01-02 13:45:37,687 - INFO - Train: expid = 834447, Epoch = 0, avg_loss = 162.9323808792284.
2024-01-02 13:45:37,688 - INFO - epoch complete!
2024-01-02 13:45:37,688 - INFO - evaluating now!
2024-01-02 13:45:37,718 - INFO - {'mode': 'Eval', 'epoch': 0, 'iter': 0, 'lr': 1e-06, 'loss': 94.7412109375}
2024-01-02 13:45:52,541 - INFO - {'mode': 'Eval', 'epoch': 0, 'iter': 500, 'lr': 1e-06, 'loss': 484.2088928222656}
2024-01-02 13:45:53,935 - INFO - Eval: expid = 834447, Epoch = 0, avg_loss = 76.47737613486373.
2024-01-02 13:45:53,936 - INFO - Epoch [0/30] (1643)  train_loss: 162.9324, val_loss: 76.4774, lr: 0.000051, 180.63s
2024-01-02 13:45:54,117 - INFO - Saved model at 0
2024-01-02 13:45:54,117 - INFO - Val loss decrease from inf to 76.4774, saving to ./libcity/cache/834447/model_cache/LinearETA_bj_epoch0.tar
2024-01-02 13:45:54,220 - INFO - {'mode': 'Train', 'epoch': 1, 'iter': 0, 'lr': 5.075e-05, 'loss': 48.80758285522461}
2024-01-02 13:46:50,823 - INFO - {'mode': 'Train', 'epoch': 1, 'iter': 500, 'lr': 5.075e-05, 'loss': 137.51731872558594}
2024-01-02 13:47:49,928 - INFO - {'mode': 'Train', 'epoch': 1, 'iter': 1000, 'lr': 5.075e-05, 'loss': 14.430933952331543}
2024-01-02 13:48:43,121 - INFO - {'mode': 'Train', 'epoch': 1, 'iter': 1500, 'lr': 5.075e-05, 'loss': 163.3092498779297}
2024-01-02 13:48:58,240 - INFO - Train: expid = 834447, Epoch = 1, avg_loss = 51.74949993857112.
2024-01-02 13:48:58,241 - INFO - epoch complete!
2024-01-02 13:48:58,241 - INFO - evaluating now!
2024-01-02 13:48:58,273 - INFO - {'mode': 'Eval', 'epoch': 1, 'iter': 0, 'lr': 5.075e-05, 'loss': 12.953139305114746}
2024-01-02 13:49:13,844 - INFO - {'mode': 'Eval', 'epoch': 1, 'iter': 500, 'lr': 5.075e-05, 'loss': 46.77726364135742}
2024-01-02 13:49:15,309 - INFO - Eval: expid = 834447, Epoch = 1, avg_loss = 45.2882143484403.
2024-01-02 13:49:15,309 - INFO - Epoch [1/30] (3286)  train_loss: 51.7495, val_loss: 45.2882, lr: 0.000101, 201.19s
2024-01-02 13:49:15,504 - INFO - Saved model at 1
2024-01-02 13:49:15,504 - INFO - Val loss decrease from 76.4774 to 45.2882, saving to ./libcity/cache/834447/model_cache/LinearETA_bj_epoch1.tar
2024-01-02 13:49:15,609 - INFO - {'mode': 'Train', 'epoch': 2, 'iter': 0, 'lr': 0.0001005, 'loss': 21.391525268554688}
2024-01-02 13:50:09,018 - INFO - {'mode': 'Train', 'epoch': 2, 'iter': 500, 'lr': 0.0001005, 'loss': 33.979244232177734}
2024-01-02 13:51:02,615 - INFO - {'mode': 'Train', 'epoch': 2, 'iter': 1000, 'lr': 0.0001005, 'loss': 28.965320587158203}
2024-01-02 13:51:56,328 - INFO - {'mode': 'Train', 'epoch': 2, 'iter': 1500, 'lr': 0.0001005, 'loss': 48.29623031616211}
2024-01-02 13:52:11,703 - INFO - Train: expid = 834447, Epoch = 2, avg_loss = 49.1176079709266.
2024-01-02 13:52:11,704 - INFO - epoch complete!
2024-01-02 13:52:11,704 - INFO - evaluating now!
2024-01-02 13:52:11,736 - INFO - {'mode': 'Eval', 'epoch': 2, 'iter': 0, 'lr': 0.0001005, 'loss': 48.44786071777344}
2024-01-02 13:52:27,530 - INFO - {'mode': 'Eval', 'epoch': 2, 'iter': 500, 'lr': 0.0001005, 'loss': 125.82379913330078}
2024-01-02 13:52:29,017 - INFO - Eval: expid = 834447, Epoch = 2, avg_loss = 46.55435766716526.
2024-01-02 13:52:29,018 - INFO - Epoch [2/30] (4929)  train_loss: 49.1176, val_loss: 46.5544, lr: 0.000150, 193.51s
2024-01-02 13:52:29,127 - INFO - {'mode': 'Train', 'epoch': 3, 'iter': 0, 'lr': 0.00015025000000000002, 'loss': 25.115083694458008}
2024-01-02 13:53:23,304 - INFO - {'mode': 'Train', 'epoch': 3, 'iter': 500, 'lr': 0.00015025000000000002, 'loss': 54.491241455078125}
2024-01-02 13:54:18,165 - INFO - {'mode': 'Train', 'epoch': 3, 'iter': 1000, 'lr': 0.00015025000000000002, 'loss': 73.94570922851562}
2024-01-02 13:55:12,696 - INFO - {'mode': 'Train', 'epoch': 3, 'iter': 1500, 'lr': 0.00015025000000000002, 'loss': 58.199790954589844}
2024-01-02 13:55:28,258 - INFO - Train: expid = 834447, Epoch = 3, avg_loss = 48.94244225026266.
2024-01-02 13:55:28,258 - INFO - epoch complete!
2024-01-02 13:55:28,259 - INFO - evaluating now!
2024-01-02 13:55:28,292 - INFO - {'mode': 'Eval', 'epoch': 3, 'iter': 0, 'lr': 0.00015025000000000002, 'loss': 41.60230255126953}
2024-01-02 13:55:44,684 - INFO - {'mode': 'Eval', 'epoch': 3, 'iter': 500, 'lr': 0.00015025000000000002, 'loss': 7.692437171936035}
2024-01-02 13:55:46,220 - INFO - Eval: expid = 834447, Epoch = 3, avg_loss = 46.09559703286924.
2024-01-02 13:55:46,220 - INFO - Epoch [3/30] (6572)  train_loss: 48.9424, val_loss: 46.0956, lr: 0.000191, 197.20s
2024-01-02 13:55:46,339 - INFO - {'mode': 'Train', 'epoch': 4, 'iter': 0, 'lr': 0.0001913545457642601, 'loss': 19.878189086914062}
2024-01-02 13:56:40,941 - INFO - {'mode': 'Train', 'epoch': 4, 'iter': 500, 'lr': 0.0001913545457642601, 'loss': 56.62311553955078}
2024-01-02 13:57:35,700 - INFO - {'mode': 'Train', 'epoch': 4, 'iter': 1000, 'lr': 0.0001913545457642601, 'loss': 31.073373794555664}
2024-01-02 13:58:30,711 - INFO - {'mode': 'Train', 'epoch': 4, 'iter': 1500, 'lr': 0.0001913545457642601, 'loss': 46.45072937011719}
2024-01-02 13:58:46,500 - INFO - Train: expid = 834447, Epoch = 4, avg_loss = 49.10152546000963.
2024-01-02 13:58:46,501 - INFO - epoch complete!
2024-01-02 13:58:46,501 - INFO - evaluating now!
2024-01-02 13:58:46,533 - INFO - {'mode': 'Eval', 'epoch': 4, 'iter': 0, 'lr': 0.0001913545457642601, 'loss': 106.2408218383789}
2024-01-02 13:59:04,120 - INFO - {'mode': 'Eval', 'epoch': 4, 'iter': 500, 'lr': 0.0001913545457642601, 'loss': 35.03960418701172}
2024-01-02 13:59:05,601 - INFO - Eval: expid = 834447, Epoch = 4, avg_loss = 45.18531560593.
2024-01-02 13:59:05,601 - INFO - Epoch [4/30] (8215)  train_loss: 49.1015, val_loss: 45.1853, lr: 0.000187, 199.38s
2024-01-02 13:59:05,787 - INFO - Saved model at 4
2024-01-02 13:59:05,788 - INFO - Val loss decrease from 45.2882 to 45.1853, saving to ./libcity/cache/834447/model_cache/LinearETA_bj_epoch4.tar
2024-01-02 13:59:05,898 - INFO - {'mode': 'Train', 'epoch': 5, 'iter': 0, 'lr': 0.00018660254037844388, 'loss': 144.4447784423828}
2024-01-02 14:00:00,777 - INFO - {'mode': 'Train', 'epoch': 5, 'iter': 500, 'lr': 0.00018660254037844388, 'loss': 25.134666442871094}
2024-01-02 14:00:55,843 - INFO - {'mode': 'Train', 'epoch': 5, 'iter': 1000, 'lr': 0.00018660254037844388, 'loss': 9.469863891601562}
2024-01-02 14:01:51,226 - INFO - {'mode': 'Train', 'epoch': 5, 'iter': 1500, 'lr': 0.00018660254037844388, 'loss': 11.209959030151367}
2024-01-02 14:02:06,726 - INFO - Train: expid = 834447, Epoch = 5, avg_loss = 48.57496020731851.
2024-01-02 14:02:06,726 - INFO - epoch complete!
2024-01-02 14:02:06,727 - INFO - evaluating now!
2024-01-02 14:02:06,759 - INFO - {'mode': 'Eval', 'epoch': 5, 'iter': 0, 'lr': 0.00018660254037844388, 'loss': 7.584710121154785}
2024-01-02 14:02:23,537 - INFO - {'mode': 'Eval', 'epoch': 5, 'iter': 500, 'lr': 0.00018660254037844388, 'loss': 20.986217498779297}
2024-01-02 14:02:25,257 - INFO - Eval: expid = 834447, Epoch = 5, avg_loss = 47.42921520477016.
2024-01-02 14:02:25,258 - INFO - Epoch [5/30] (9858)  train_loss: 48.5750, val_loss: 47.4292, lr: 0.000181, 199.47s
2024-01-02 14:02:25,381 - INFO - {'mode': 'Train', 'epoch': 6, 'iter': 0, 'lr': 0.00018090169943749476, 'loss': 246.32423400878906}
2024-01-02 14:03:20,509 - INFO - {'mode': 'Train', 'epoch': 6, 'iter': 500, 'lr': 0.00018090169943749476, 'loss': 34.80248260498047}
2024-01-02 14:04:15,744 - INFO - {'mode': 'Train', 'epoch': 6, 'iter': 1000, 'lr': 0.00018090169943749476, 'loss': 55.69577407836914}
2024-01-02 14:05:10,968 - INFO - {'mode': 'Train', 'epoch': 6, 'iter': 1500, 'lr': 0.00018090169943749476, 'loss': 67.95247650146484}
2024-01-02 14:05:26,693 - INFO - Train: expid = 834447, Epoch = 6, avg_loss = 47.815716083490024.
2024-01-02 14:05:26,693 - INFO - epoch complete!
2024-01-02 14:05:26,694 - INFO - evaluating now!
2024-01-02 14:05:26,727 - INFO - {'mode': 'Eval', 'epoch': 6, 'iter': 0, 'lr': 0.00018090169943749476, 'loss': 34.180458068847656}
2024-01-02 14:05:44,086 - INFO - {'mode': 'Eval', 'epoch': 6, 'iter': 500, 'lr': 0.00018090169943749476, 'loss': 18.122432708740234}
2024-01-02 14:05:45,717 - INFO - Eval: expid = 834447, Epoch = 6, avg_loss = 46.1676196795076.
2024-01-02 14:05:45,717 - INFO - Epoch [6/30] (11501)  train_loss: 47.8157, val_loss: 46.1676, lr: 0.000174, 200.46s
2024-01-02 14:05:45,838 - INFO - {'mode': 'Train', 'epoch': 7, 'iter': 0, 'lr': 0.00017431448254773944, 'loss': 20.207183837890625}
2024-01-02 14:06:40,751 - INFO - {'mode': 'Train', 'epoch': 7, 'iter': 500, 'lr': 0.00017431448254773944, 'loss': 175.97811889648438}
2024-01-02 14:07:36,257 - INFO - {'mode': 'Train', 'epoch': 7, 'iter': 1000, 'lr': 0.00017431448254773944, 'loss': 73.48535919189453}
2024-01-02 14:08:31,571 - INFO - {'mode': 'Train', 'epoch': 7, 'iter': 1500, 'lr': 0.00017431448254773944, 'loss': 26.1374454498291}
2024-01-02 14:08:47,128 - INFO - Train: expid = 834447, Epoch = 7, avg_loss = 47.25739731631117.
2024-01-02 14:08:47,129 - INFO - epoch complete!
2024-01-02 14:08:47,129 - INFO - evaluating now!
2024-01-02 14:08:47,160 - INFO - {'mode': 'Eval', 'epoch': 7, 'iter': 0, 'lr': 0.00017431448254773944, 'loss': 18.40860939025879}
2024-01-02 14:09:04,523 - INFO - {'mode': 'Eval', 'epoch': 7, 'iter': 500, 'lr': 0.00017431448254773944, 'loss': 26.005876541137695}
2024-01-02 14:09:06,241 - INFO - Eval: expid = 834447, Epoch = 7, avg_loss = 44.85705653499795.
2024-01-02 14:09:06,241 - INFO - Epoch [7/30] (13144)  train_loss: 47.2574, val_loss: 44.8571, lr: 0.000167, 200.52s
2024-01-02 14:09:06,429 - INFO - Saved model at 7
2024-01-02 14:09:06,429 - INFO - Val loss decrease from 45.1853 to 44.8571, saving to ./libcity/cache/834447/model_cache/LinearETA_bj_epoch7.tar
2024-01-02 14:09:06,540 - INFO - {'mode': 'Train', 'epoch': 8, 'iter': 0, 'lr': 0.00016691306063588583, 'loss': 24.249942779541016}
2024-01-02 14:10:01,848 - INFO - {'mode': 'Train', 'epoch': 8, 'iter': 500, 'lr': 0.00016691306063588583, 'loss': 52.042076110839844}
2024-01-02 14:10:56,402 - INFO - {'mode': 'Train', 'epoch': 8, 'iter': 1000, 'lr': 0.00016691306063588583, 'loss': 32.612728118896484}
2024-01-02 14:11:48,696 - INFO - {'mode': 'Train', 'epoch': 8, 'iter': 1500, 'lr': 0.00016691306063588583, 'loss': 37.229793548583984}
2024-01-02 14:12:03,417 - INFO - Train: expid = 834447, Epoch = 8, avg_loss = 46.19884943225319.
2024-01-02 14:12:03,417 - INFO - epoch complete!
2024-01-02 14:12:03,418 - INFO - evaluating now!
2024-01-02 14:12:03,448 - INFO - {'mode': 'Eval', 'epoch': 8, 'iter': 0, 'lr': 0.00016691306063588583, 'loss': 140.70535278320312}
2024-01-02 14:12:18,404 - INFO - {'mode': 'Eval', 'epoch': 8, 'iter': 500, 'lr': 0.00016691306063588583, 'loss': 9.75321102142334}
2024-01-02 14:12:19,803 - INFO - Eval: expid = 834447, Epoch = 8, avg_loss = 45.438549641387105.
2024-01-02 14:12:19,803 - INFO - Epoch [8/30] (14787)  train_loss: 46.1988, val_loss: 45.4385, lr: 0.000159, 193.37s
2024-01-02 14:12:19,913 - INFO - {'mode': 'Train', 'epoch': 9, 'iter': 0, 'lr': 0.00015877852522924732, 'loss': 8.445087432861328}
2024-01-02 14:13:11,400 - INFO - {'mode': 'Train', 'epoch': 9, 'iter': 500, 'lr': 0.00015877852522924732, 'loss': 10.103957176208496}
2024-01-02 14:14:02,860 - INFO - {'mode': 'Train', 'epoch': 9, 'iter': 1000, 'lr': 0.00015877852522924732, 'loss': 8.866816520690918}
2024-01-02 14:14:54,139 - INFO - {'mode': 'Train', 'epoch': 9, 'iter': 1500, 'lr': 0.00015877852522924732, 'loss': 67.47601318359375}
2024-01-02 14:15:08,826 - INFO - Train: expid = 834447, Epoch = 9, avg_loss = 45.83589376686806.
2024-01-02 14:15:08,827 - INFO - epoch complete!
2024-01-02 14:15:08,827 - INFO - evaluating now!
2024-01-02 14:15:08,861 - INFO - {'mode': 'Eval', 'epoch': 9, 'iter': 0, 'lr': 0.00015877852522924732, 'loss': 8.580201148986816}
2024-01-02 14:15:23,745 - INFO - {'mode': 'Eval', 'epoch': 9, 'iter': 500, 'lr': 0.00015877852522924732, 'loss': 122.31414794921875}
2024-01-02 14:15:25,139 - INFO - Eval: expid = 834447, Epoch = 9, avg_loss = 44.302922045912375.
2024-01-02 14:15:25,139 - INFO - Epoch [9/30] (16430)  train_loss: 45.8359, val_loss: 44.3029, lr: 0.000150, 185.34s
2024-01-02 14:15:25,323 - INFO - Saved model at 9
2024-01-02 14:15:25,323 - INFO - Val loss decrease from 44.8571 to 44.3029, saving to ./libcity/cache/834447/model_cache/LinearETA_bj_epoch9.tar
2024-01-02 14:15:25,423 - INFO - {'mode': 'Train', 'epoch': 10, 'iter': 0, 'lr': 0.00015000000000000001, 'loss': 95.98563385009766}
2024-01-02 14:16:16,931 - INFO - {'mode': 'Train', 'epoch': 10, 'iter': 500, 'lr': 0.00015000000000000001, 'loss': 22.874813079833984}
2024-01-02 14:17:08,244 - INFO - {'mode': 'Train', 'epoch': 10, 'iter': 1000, 'lr': 0.00015000000000000001, 'loss': 26.168140411376953}
2024-01-02 14:17:59,609 - INFO - {'mode': 'Train', 'epoch': 10, 'iter': 1500, 'lr': 0.00015000000000000001, 'loss': 53.32290267944336}
2024-01-02 14:18:14,205 - INFO - Train: expid = 834447, Epoch = 10, avg_loss = 44.526590095043254.
2024-01-02 14:18:14,206 - INFO - epoch complete!
2024-01-02 14:18:14,206 - INFO - evaluating now!
2024-01-02 14:18:14,237 - INFO - {'mode': 'Eval', 'epoch': 10, 'iter': 0, 'lr': 0.00015000000000000001, 'loss': 26.468090057373047}
2024-01-02 14:18:29,181 - INFO - {'mode': 'Eval', 'epoch': 10, 'iter': 500, 'lr': 0.00015000000000000001, 'loss': 50.10601043701172}
2024-01-02 14:18:30,589 - INFO - Eval: expid = 834447, Epoch = 10, avg_loss = 46.535185646248735.
2024-01-02 14:18:30,589 - INFO - Epoch [10/30] (18073)  train_loss: 44.5266, val_loss: 46.5352, lr: 0.000141, 185.27s
2024-01-02 14:18:30,694 - INFO - {'mode': 'Train', 'epoch': 11, 'iter': 0, 'lr': 0.00014067366430758004, 'loss': 19.071067810058594}
2024-01-02 14:19:22,188 - INFO - {'mode': 'Train', 'epoch': 11, 'iter': 500, 'lr': 0.00014067366430758004, 'loss': 53.405250549316406}
2024-01-02 14:20:13,509 - INFO - {'mode': 'Train', 'epoch': 11, 'iter': 1000, 'lr': 0.00014067366430758004, 'loss': 220.51759338378906}
2024-01-02 14:21:04,969 - INFO - {'mode': 'Train', 'epoch': 11, 'iter': 1500, 'lr': 0.00014067366430758004, 'loss': 14.845190048217773}
2024-01-02 14:21:19,530 - INFO - Train: expid = 834447, Epoch = 11, avg_loss = 44.5267747332288.
2024-01-02 14:21:19,531 - INFO - epoch complete!
2024-01-02 14:21:19,531 - INFO - evaluating now!
2024-01-02 14:21:19,561 - INFO - {'mode': 'Eval', 'epoch': 11, 'iter': 0, 'lr': 0.00014067366430758004, 'loss': 31.949092864990234}
2024-01-02 14:21:34,460 - INFO - {'mode': 'Eval', 'epoch': 11, 'iter': 500, 'lr': 0.00014067366430758004, 'loss': 63.20191955566406}
2024-01-02 14:21:35,853 - INFO - Eval: expid = 834447, Epoch = 11, avg_loss = 47.47489544332844.
2024-01-02 14:21:35,854 - INFO - Epoch [11/30] (19716)  train_loss: 44.5268, val_loss: 47.4749, lr: 0.000131, 185.27s
2024-01-02 14:21:35,957 - INFO - {'mode': 'Train', 'epoch': 12, 'iter': 0, 'lr': 0.00013090169943749476, 'loss': 20.82378387451172}
2024-01-02 14:22:27,350 - INFO - {'mode': 'Train', 'epoch': 12, 'iter': 500, 'lr': 0.00013090169943749476, 'loss': 25.283000946044922}
2024-01-02 14:23:18,832 - INFO - {'mode': 'Train', 'epoch': 12, 'iter': 1000, 'lr': 0.00013090169943749476, 'loss': 112.307861328125}
2024-01-02 14:24:10,254 - INFO - {'mode': 'Train', 'epoch': 12, 'iter': 1500, 'lr': 0.00013090169943749476, 'loss': 19.8457088470459}
2024-01-02 14:24:24,870 - INFO - Train: expid = 834447, Epoch = 12, avg_loss = 43.413037915592014.
2024-01-02 14:24:24,870 - INFO - epoch complete!
2024-01-02 14:24:24,871 - INFO - evaluating now!
2024-01-02 14:24:24,900 - INFO - {'mode': 'Eval', 'epoch': 12, 'iter': 0, 'lr': 0.00013090169943749476, 'loss': 41.073246002197266}
2024-01-02 14:24:39,805 - INFO - {'mode': 'Eval', 'epoch': 12, 'iter': 500, 'lr': 0.00013090169943749476, 'loss': 29.771678924560547}
2024-01-02 14:24:41,210 - INFO - Eval: expid = 834447, Epoch = 12, avg_loss = 46.7016803297278.
2024-01-02 14:24:41,211 - INFO - Epoch [12/30] (21359)  train_loss: 43.4130, val_loss: 46.7017, lr: 0.000121, 185.36s
2024-01-02 14:24:41,315 - INFO - {'mode': 'Train', 'epoch': 13, 'iter': 0, 'lr': 0.00012079116908177593, 'loss': 33.062191009521484}
2024-01-02 14:25:32,765 - INFO - {'mode': 'Train', 'epoch': 13, 'iter': 500, 'lr': 0.00012079116908177593, 'loss': 21.762815475463867}
2024-01-02 14:26:24,212 - INFO - {'mode': 'Train', 'epoch': 13, 'iter': 1000, 'lr': 0.00012079116908177593, 'loss': 8.882882118225098}
2024-01-02 14:27:15,770 - INFO - {'mode': 'Train', 'epoch': 13, 'iter': 1500, 'lr': 0.00012079116908177593, 'loss': 27.413677215576172}
2024-01-02 14:27:30,373 - INFO - Train: expid = 834447, Epoch = 13, avg_loss = 42.29876080363612.
2024-01-02 14:27:30,374 - INFO - epoch complete!
2024-01-02 14:27:30,374 - INFO - evaluating now!
2024-01-02 14:27:30,406 - INFO - {'mode': 'Eval', 'epoch': 13, 'iter': 0, 'lr': 0.00012079116908177593, 'loss': 25.28173828125}
2024-01-02 14:27:45,401 - INFO - {'mode': 'Eval', 'epoch': 13, 'iter': 500, 'lr': 0.00012079116908177593, 'loss': 53.92731857299805}
2024-01-02 14:27:46,804 - INFO - Eval: expid = 834447, Epoch = 13, avg_loss = 44.038118298413.
2024-01-02 14:27:46,805 - INFO - Epoch [13/30] (23002)  train_loss: 42.2988, val_loss: 44.0381, lr: 0.000110, 185.59s
2024-01-02 14:27:46,988 - INFO - Saved model at 13
2024-01-02 14:27:46,989 - INFO - Val loss decrease from 44.3029 to 44.0381, saving to ./libcity/cache/834447/model_cache/LinearETA_bj_epoch13.tar
2024-01-02 14:27:47,095 - INFO - {'mode': 'Train', 'epoch': 14, 'iter': 0, 'lr': 0.00011045284632676536, 'loss': 17.345548629760742}
2024-01-02 14:28:38,629 - INFO - {'mode': 'Train', 'epoch': 14, 'iter': 500, 'lr': 0.00011045284632676536, 'loss': 19.729351043701172}
2024-01-02 14:29:30,213 - INFO - {'mode': 'Train', 'epoch': 14, 'iter': 1000, 'lr': 0.00011045284632676536, 'loss': 45.485137939453125}
2024-01-02 14:30:21,724 - INFO - {'mode': 'Train', 'epoch': 14, 'iter': 1500, 'lr': 0.00011045284632676536, 'loss': 30.582151412963867}
2024-01-02 14:30:36,476 - INFO - Train: expid = 834447, Epoch = 14, avg_loss = 42.04024640834029.
2024-01-02 14:30:36,476 - INFO - epoch complete!
2024-01-02 14:30:36,476 - INFO - evaluating now!
2024-01-02 14:30:36,506 - INFO - {'mode': 'Eval', 'epoch': 14, 'iter': 0, 'lr': 0.00011045284632676536, 'loss': 44.965843200683594}
2024-01-02 14:30:51,382 - INFO - {'mode': 'Eval', 'epoch': 14, 'iter': 500, 'lr': 0.00011045284632676536, 'loss': 23.462371826171875}
2024-01-02 14:30:52,788 - INFO - Eval: expid = 834447, Epoch = 14, avg_loss = 43.82712224324544.
2024-01-02 14:30:52,788 - INFO - Epoch [14/30] (24645)  train_loss: 42.0402, val_loss: 43.8271, lr: 0.000100, 185.80s
2024-01-02 14:30:52,976 - INFO - Saved model at 14
2024-01-02 14:30:52,976 - INFO - Val loss decrease from 44.0381 to 43.8271, saving to ./libcity/cache/834447/model_cache/LinearETA_bj_epoch14.tar
2024-01-02 14:30:53,077 - INFO - {'mode': 'Train', 'epoch': 15, 'iter': 0, 'lr': 0.00010000000000000003, 'loss': 18.56673812866211}
2024-01-02 14:31:45,040 - INFO - {'mode': 'Train', 'epoch': 15, 'iter': 500, 'lr': 0.00010000000000000003, 'loss': 7.733660697937012}
2024-01-02 14:32:36,550 - INFO - {'mode': 'Train', 'epoch': 15, 'iter': 1000, 'lr': 0.00010000000000000003, 'loss': 40.466697692871094}
2024-01-02 14:33:28,186 - INFO - {'mode': 'Train', 'epoch': 15, 'iter': 1500, 'lr': 0.00010000000000000003, 'loss': 43.4488525390625}
2024-01-02 14:33:42,886 - INFO - Train: expid = 834447, Epoch = 15, avg_loss = 41.32548694854495.
2024-01-02 14:33:42,887 - INFO - epoch complete!
2024-01-02 14:33:42,887 - INFO - evaluating now!
2024-01-02 14:33:42,918 - INFO - {'mode': 'Eval', 'epoch': 15, 'iter': 0, 'lr': 0.00010000000000000003, 'loss': 6.590490341186523}
2024-01-02 14:33:57,886 - INFO - {'mode': 'Eval', 'epoch': 15, 'iter': 500, 'lr': 0.00010000000000000003, 'loss': 20.45388412475586}
2024-01-02 14:33:59,279 - INFO - Eval: expid = 834447, Epoch = 15, avg_loss = 43.265622372823216.
2024-01-02 14:33:59,279 - INFO - Epoch [15/30] (26288)  train_loss: 41.3255, val_loss: 43.2656, lr: 0.000090, 186.30s
2024-01-02 14:33:59,463 - INFO - Saved model at 15
2024-01-02 14:33:59,463 - INFO - Val loss decrease from 43.8271 to 43.2656, saving to ./libcity/cache/834447/model_cache/LinearETA_bj_epoch15.tar
2024-01-02 14:33:59,563 - INFO - {'mode': 'Train', 'epoch': 16, 'iter': 0, 'lr': 8.954715367323468e-05, 'loss': 27.938581466674805}
2024-01-02 14:34:51,270 - INFO - {'mode': 'Train', 'epoch': 16, 'iter': 500, 'lr': 8.954715367323468e-05, 'loss': 24.36490249633789}
2024-01-02 14:35:42,957 - INFO - {'mode': 'Train', 'epoch': 16, 'iter': 1000, 'lr': 8.954715367323468e-05, 'loss': 37.25260925292969}
2024-01-02 14:36:34,643 - INFO - {'mode': 'Train', 'epoch': 16, 'iter': 1500, 'lr': 8.954715367323468e-05, 'loss': 26.350624084472656}
2024-01-02 14:36:49,357 - INFO - Train: expid = 834447, Epoch = 16, avg_loss = 40.351216133298855.
2024-01-02 14:36:49,357 - INFO - epoch complete!
2024-01-02 14:36:49,358 - INFO - evaluating now!
2024-01-02 14:36:49,387 - INFO - {'mode': 'Eval', 'epoch': 16, 'iter': 0, 'lr': 8.954715367323468e-05, 'loss': 60.31254196166992}
2024-01-02 14:37:04,363 - INFO - {'mode': 'Eval', 'epoch': 16, 'iter': 500, 'lr': 8.954715367323468e-05, 'loss': 9.64529800415039}
2024-01-02 14:37:05,767 - INFO - Eval: expid = 834447, Epoch = 16, avg_loss = 43.56668693333456.
2024-01-02 14:37:05,768 - INFO - Epoch [16/30] (27931)  train_loss: 40.3512, val_loss: 43.5667, lr: 0.000079, 186.31s
2024-01-02 14:37:05,875 - INFO - {'mode': 'Train', 'epoch': 17, 'iter': 0, 'lr': 7.920883091822407e-05, 'loss': 39.30999755859375}
2024-01-02 14:37:57,340 - INFO - {'mode': 'Train', 'epoch': 17, 'iter': 500, 'lr': 7.920883091822407e-05, 'loss': 14.292717933654785}
2024-01-02 14:38:49,051 - INFO - {'mode': 'Train', 'epoch': 17, 'iter': 1000, 'lr': 7.920883091822407e-05, 'loss': 24.024673461914062}
2024-01-02 14:39:40,969 - INFO - {'mode': 'Train', 'epoch': 17, 'iter': 1500, 'lr': 7.920883091822407e-05, 'loss': 9.250210762023926}
2024-01-02 14:39:55,611 - INFO - Train: expid = 834447, Epoch = 17, avg_loss = 39.228277773341006.
2024-01-02 14:39:55,611 - INFO - epoch complete!
2024-01-02 14:39:55,612 - INFO - evaluating now!
2024-01-02 14:39:55,642 - INFO - {'mode': 'Eval', 'epoch': 17, 'iter': 0, 'lr': 7.920883091822407e-05, 'loss': 50.585655212402344}
2024-01-02 14:40:10,657 - INFO - {'mode': 'Eval', 'epoch': 17, 'iter': 500, 'lr': 7.920883091822407e-05, 'loss': 25.658254623413086}
2024-01-02 14:40:12,056 - INFO - Eval: expid = 834447, Epoch = 17, avg_loss = 43.916342326821805.
2024-01-02 14:40:12,057 - INFO - Epoch [17/30] (29574)  train_loss: 39.2283, val_loss: 43.9163, lr: 0.000069, 186.29s
2024-01-02 14:40:12,160 - INFO - {'mode': 'Train', 'epoch': 18, 'iter': 0, 'lr': 6.909830056250527e-05, 'loss': 10.328685760498047}
2024-01-02 14:41:03,944 - INFO - {'mode': 'Train', 'epoch': 18, 'iter': 500, 'lr': 6.909830056250527e-05, 'loss': 80.08724975585938}
2024-01-02 14:41:55,831 - INFO - {'mode': 'Train', 'epoch': 18, 'iter': 1000, 'lr': 6.909830056250527e-05, 'loss': 16.081966400146484}
2024-01-02 14:42:47,772 - INFO - {'mode': 'Train', 'epoch': 18, 'iter': 1500, 'lr': 6.909830056250527e-05, 'loss': 13.241403579711914}
2024-01-02 14:43:02,436 - INFO - Train: expid = 834447, Epoch = 18, avg_loss = 38.4206311269009.
2024-01-02 14:43:02,437 - INFO - epoch complete!
2024-01-02 14:43:02,437 - INFO - evaluating now!
2024-01-02 14:43:02,468 - INFO - {'mode': 'Eval', 'epoch': 18, 'iter': 0, 'lr': 6.909830056250527e-05, 'loss': 16.446014404296875}
2024-01-02 14:43:17,370 - INFO - {'mode': 'Eval', 'epoch': 18, 'iter': 500, 'lr': 6.909830056250527e-05, 'loss': 6.242410659790039}
2024-01-02 14:43:18,769 - INFO - Eval: expid = 834447, Epoch = 18, avg_loss = 44.235391919906824.
2024-01-02 14:43:18,769 - INFO - Epoch [18/30] (31217)  train_loss: 38.4206, val_loss: 44.2354, lr: 0.000059, 186.71s
2024-01-02 14:43:18,875 - INFO - {'mode': 'Train', 'epoch': 19, 'iter': 0, 'lr': 5.932633569241999e-05, 'loss': 21.03034210205078}
2024-01-02 14:44:10,804 - INFO - {'mode': 'Train', 'epoch': 19, 'iter': 500, 'lr': 5.932633569241999e-05, 'loss': 52.30773162841797}
2024-01-02 14:45:02,560 - INFO - {'mode': 'Train', 'epoch': 19, 'iter': 1000, 'lr': 5.932633569241999e-05, 'loss': 96.42395782470703}
2024-01-02 14:45:54,427 - INFO - {'mode': 'Train', 'epoch': 19, 'iter': 1500, 'lr': 5.932633569241999e-05, 'loss': 6.259635925292969}
2024-01-02 14:46:09,248 - INFO - Train: expid = 834447, Epoch = 19, avg_loss = 37.524777679396905.
2024-01-02 14:46:09,248 - INFO - epoch complete!
2024-01-02 14:46:09,248 - INFO - evaluating now!
2024-01-02 14:46:09,279 - INFO - {'mode': 'Eval', 'epoch': 19, 'iter': 0, 'lr': 5.932633569241999e-05, 'loss': 41.858848571777344}
2024-01-02 14:46:24,258 - INFO - {'mode': 'Eval', 'epoch': 19, 'iter': 500, 'lr': 5.932633569241999e-05, 'loss': 157.39573669433594}
2024-01-02 14:46:25,660 - INFO - Eval: expid = 834447, Epoch = 19, avg_loss = 43.67719803313686.
2024-01-02 14:46:25,660 - INFO - Epoch [19/30] (32860)  train_loss: 37.5248, val_loss: 43.6772, lr: 0.000050, 186.89s
2024-01-02 14:46:25,766 - INFO - {'mode': 'Train', 'epoch': 20, 'iter': 0, 'lr': 5.000000000000002e-05, 'loss': 22.331939697265625}
2024-01-02 14:47:17,406 - INFO - {'mode': 'Train', 'epoch': 20, 'iter': 500, 'lr': 5.000000000000002e-05, 'loss': 18.115310668945312}
2024-01-02 14:48:09,388 - INFO - {'mode': 'Train', 'epoch': 20, 'iter': 1000, 'lr': 5.000000000000002e-05, 'loss': 159.8574676513672}
2024-01-02 14:49:01,205 - INFO - {'mode': 'Train', 'epoch': 20, 'iter': 1500, 'lr': 5.000000000000002e-05, 'loss': 11.798221588134766}
2024-01-02 14:49:15,962 - INFO - Train: expid = 834447, Epoch = 20, avg_loss = 36.079221552992394.
2024-01-02 14:49:15,962 - INFO - epoch complete!
2024-01-02 14:49:15,963 - INFO - evaluating now!
2024-01-02 14:49:15,995 - INFO - {'mode': 'Eval', 'epoch': 20, 'iter': 0, 'lr': 5.000000000000002e-05, 'loss': 49.04583740234375}
2024-01-02 14:49:31,022 - INFO - {'mode': 'Eval', 'epoch': 20, 'iter': 500, 'lr': 5.000000000000002e-05, 'loss': 9.873051643371582}
2024-01-02 14:49:32,455 - INFO - Eval: expid = 834447, Epoch = 20, avg_loss = 45.51450692965015.
2024-01-02 14:49:32,455 - INFO - Epoch [20/30] (34503)  train_loss: 36.0792, val_loss: 45.5145, lr: 0.000041, 186.79s
2024-01-02 14:49:32,564 - INFO - {'mode': 'Train', 'epoch': 21, 'iter': 0, 'lr': 4.12214747707527e-05, 'loss': 55.199241638183594}
2024-01-02 14:50:24,365 - INFO - {'mode': 'Train', 'epoch': 21, 'iter': 500, 'lr': 4.12214747707527e-05, 'loss': 10.06158447265625}
2024-01-02 14:51:16,261 - INFO - {'mode': 'Train', 'epoch': 21, 'iter': 1000, 'lr': 4.12214747707527e-05, 'loss': 31.149072647094727}
2024-01-02 14:52:08,128 - INFO - {'mode': 'Train', 'epoch': 21, 'iter': 1500, 'lr': 4.12214747707527e-05, 'loss': 66.8346939086914}
2024-01-02 14:52:22,789 - INFO - Train: expid = 834447, Epoch = 21, avg_loss = 36.196474354785046.
2024-01-02 14:52:22,790 - INFO - epoch complete!
2024-01-02 14:52:22,790 - INFO - evaluating now!
2024-01-02 14:52:22,821 - INFO - {'mode': 'Eval', 'epoch': 21, 'iter': 0, 'lr': 4.12214747707527e-05, 'loss': 179.59429931640625}
2024-01-02 14:52:37,742 - INFO - {'mode': 'Eval', 'epoch': 21, 'iter': 500, 'lr': 4.12214747707527e-05, 'loss': 16.142391204833984}
2024-01-02 14:52:39,137 - INFO - Eval: expid = 834447, Epoch = 21, avg_loss = 44.242902569792584.
2024-01-02 14:52:39,138 - INFO - Epoch [21/30] (36146)  train_loss: 36.1965, val_loss: 44.2429, lr: 0.000033, 186.68s
2024-01-02 14:52:39,245 - INFO - {'mode': 'Train', 'epoch': 22, 'iter': 0, 'lr': 3.308693936411421e-05, 'loss': 52.56109619140625}
2024-01-02 14:53:31,722 - INFO - {'mode': 'Train', 'epoch': 22, 'iter': 500, 'lr': 3.308693936411421e-05, 'loss': 29.81079864501953}
2024-01-02 14:54:23,457 - INFO - {'mode': 'Train', 'epoch': 22, 'iter': 1000, 'lr': 3.308693936411421e-05, 'loss': 50.08235168457031}
2024-01-02 14:55:15,408 - INFO - {'mode': 'Train', 'epoch': 22, 'iter': 1500, 'lr': 3.308693936411421e-05, 'loss': 20.595855712890625}
2024-01-02 14:55:29,860 - INFO - Train: expid = 834447, Epoch = 22, avg_loss = 34.942016502841454.
2024-01-02 14:55:29,860 - INFO - epoch complete!
2024-01-02 14:55:29,861 - INFO - evaluating now!
2024-01-02 14:55:29,891 - INFO - {'mode': 'Eval', 'epoch': 22, 'iter': 0, 'lr': 3.308693936411421e-05, 'loss': 26.116283416748047}
2024-01-02 14:55:44,596 - INFO - {'mode': 'Eval', 'epoch': 22, 'iter': 500, 'lr': 3.308693936411421e-05, 'loss': 82.83238220214844}
2024-01-02 14:55:45,981 - INFO - Eval: expid = 834447, Epoch = 22, avg_loss = 45.81624857976556.
2024-01-02 14:55:45,982 - INFO - Epoch [22/30] (37789)  train_loss: 34.9420, val_loss: 45.8162, lr: 0.000026, 186.84s
2024-01-02 14:55:46,091 - INFO - {'mode': 'Train', 'epoch': 23, 'iter': 0, 'lr': 2.5685517452260587e-05, 'loss': 37.838294982910156}
2024-01-02 14:56:37,004 - INFO - {'mode': 'Train', 'epoch': 23, 'iter': 500, 'lr': 2.5685517452260587e-05, 'loss': 7.129486083984375}
2024-01-02 14:57:28,380 - INFO - {'mode': 'Train', 'epoch': 23, 'iter': 1000, 'lr': 2.5685517452260587e-05, 'loss': 17.60321807861328}
2024-01-02 14:58:19,417 - INFO - {'mode': 'Train', 'epoch': 23, 'iter': 1500, 'lr': 2.5685517452260587e-05, 'loss': 24.971118927001953}
2024-01-02 14:58:33,886 - INFO - Train: expid = 834447, Epoch = 23, avg_loss = 34.37611934877019.
2024-01-02 14:58:33,887 - INFO - epoch complete!
2024-01-02 14:58:33,887 - INFO - evaluating now!
2024-01-02 14:58:33,918 - INFO - {'mode': 'Eval', 'epoch': 23, 'iter': 0, 'lr': 2.5685517452260587e-05, 'loss': 108.5748062133789}
2024-01-02 14:58:48,624 - INFO - {'mode': 'Eval', 'epoch': 23, 'iter': 500, 'lr': 2.5685517452260587e-05, 'loss': 13.593507766723633}
2024-01-02 14:58:50,009 - INFO - Eval: expid = 834447, Epoch = 23, avg_loss = 44.51335896052182.
2024-01-02 14:58:50,010 - INFO - Epoch [23/30] (39432)  train_loss: 34.3761, val_loss: 44.5134, lr: 0.000019, 184.03s
2024-01-02 14:58:50,115 - INFO - {'mode': 'Train', 'epoch': 24, 'iter': 0, 'lr': 1.9098300562505266e-05, 'loss': 50.856483459472656}
2024-01-02 14:59:41,220 - INFO - {'mode': 'Train', 'epoch': 24, 'iter': 500, 'lr': 1.9098300562505266e-05, 'loss': 19.83675765991211}
2024-01-02 15:00:32,322 - INFO - {'mode': 'Train', 'epoch': 24, 'iter': 1000, 'lr': 1.9098300562505266e-05, 'loss': 87.1064453125}
2024-01-02 15:01:23,407 - INFO - {'mode': 'Train', 'epoch': 24, 'iter': 1500, 'lr': 1.9098300562505266e-05, 'loss': 12.702499389648438}
2024-01-02 15:01:37,923 - INFO - Train: expid = 834447, Epoch = 24, avg_loss = 33.21891201850867.
2024-01-02 15:01:37,923 - INFO - epoch complete!
2024-01-02 15:01:37,923 - INFO - evaluating now!
2024-01-02 15:01:37,953 - INFO - {'mode': 'Eval', 'epoch': 24, 'iter': 0, 'lr': 1.9098300562505266e-05, 'loss': 24.71710968017578}
2024-01-02 15:01:52,609 - INFO - {'mode': 'Eval', 'epoch': 24, 'iter': 500, 'lr': 1.9098300562505266e-05, 'loss': 80.37952423095703}
2024-01-02 15:01:54,008 - INFO - Eval: expid = 834447, Epoch = 24, avg_loss = 45.38718933993823.
2024-01-02 15:01:54,009 - INFO - Epoch [24/30] (41075)  train_loss: 33.2189, val_loss: 45.3872, lr: 0.000013, 184.00s
2024-01-02 15:01:54,119 - INFO - {'mode': 'Train', 'epoch': 25, 'iter': 0, 'lr': 1.339745962155613e-05, 'loss': 24.60681915283203}
2024-01-02 15:02:45,237 - INFO - {'mode': 'Train', 'epoch': 25, 'iter': 500, 'lr': 1.339745962155613e-05, 'loss': 32.6680908203125}
2024-01-02 15:03:36,336 - INFO - {'mode': 'Train', 'epoch': 25, 'iter': 1000, 'lr': 1.339745962155613e-05, 'loss': 9.540313720703125}
2024-01-02 15:04:27,559 - INFO - {'mode': 'Train', 'epoch': 25, 'iter': 1500, 'lr': 1.339745962155613e-05, 'loss': 36.593170166015625}
2024-01-02 15:04:42,132 - INFO - Train: expid = 834447, Epoch = 25, avg_loss = 32.968976241120615.
2024-01-02 15:04:42,132 - INFO - epoch complete!
2024-01-02 15:04:42,133 - INFO - evaluating now!
2024-01-02 15:04:42,165 - INFO - {'mode': 'Eval', 'epoch': 25, 'iter': 0, 'lr': 1.339745962155613e-05, 'loss': 55.35820770263672}
2024-01-02 15:04:57,260 - INFO - {'mode': 'Eval', 'epoch': 25, 'iter': 500, 'lr': 1.339745962155613e-05, 'loss': 15.058029174804688}
2024-01-02 15:04:58,657 - INFO - Eval: expid = 834447, Epoch = 25, avg_loss = 45.54154232408358.
2024-01-02 15:04:58,658 - INFO - Epoch [25/30] (42718)  train_loss: 32.9690, val_loss: 45.5415, lr: 0.000009, 184.65s
2024-01-02 15:04:58,658 - WARNING - Early stopping at epoch: 25
2024-01-02 15:04:58,658 - INFO - Trained totally 26 epochs, average train time is 172.512s, average eval time is 16.866s
2024-01-02 15:04:58,753 - INFO - Loaded model at 15
2024-01-02 15:04:58,843 - INFO - Save png at ./libcity/cache/834447/834447_loss.png
2024-01-02 15:04:58,906 - INFO - Save png at ./libcity/cache/834447/834447_lr.png
2024-01-02 15:04:59,111 - INFO - Saved model at ./libcity/cache/834447/model_cache/834447_LinearETA_bj.pt
2024-01-02 15:04:59,111 - INFO - Start evaluating ...
2024-01-02 15:04:59,148 - INFO - {'mode': 'Test', 'epoch': 0, 'iter': 0, 'lr': 8.954715367323468e-05, 'loss': 65.88885498046875}
2024-01-02 15:05:15,515 - INFO - {'mode': 'Test', 'epoch': 0, 'iter': 500, 'lr': 8.954715367323468e-05, 'loss': 91.11639404296875}
2024-01-02 15:05:17,091 - INFO - Test: expid = 834447, Epoch = 0, avg_loss = 125.66449663671119.
2024-01-02 15:05:17,094 - INFO - Evaluate result is {"MAE": 10.35101410339439, "RMSE": 11.012813219824341, "MAPE": Infinity, "R2": 0.0, "EVAR": 0.0}
2024-01-02 15:05:17,095 - INFO - Evaluate result is saved at ./libcity/cache/834447/evaluate_cache\834447_2024_01_02_15_05_17_LinearETA_bj.json
2024-01-02 15:05:17,095 - INFO - 
{
 "MAE": 10.35101410339439,
 "RMSE": 11.012813219824341,
 "MAPE": Infinity,
 "R2": 0.0,
 "EVAR": 0.0
}
2024-01-02 15:05:17,099 - INFO - Evaluate result is saved at ./libcity/cache/834447/evaluate_cache\834447_2024_01_02_15_05_17_LinearETA_bj.csv
2024-01-02 15:05:17,104 - INFO - 
         MAE       RMSE  MAPE   R2  EVAR
1  10.351014  11.012813   inf  0.0   0.0
2024-01-02 15:05:17,104 - INFO - Test time 17.993346214294434s.
