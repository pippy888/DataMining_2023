2024-01-01 15:10:41,316 - INFO - Log directory: ./libcity/log
2024-01-01 15:10:41,316 - INFO - Begin pretrain-pipeline, task=trajectory_embedding, model_name=BERTContrastiveLM, dataset_name=bj, exp_id=171360
2024-01-01 15:10:41,316 - INFO - {'task': 'trajectory_embedding', 'model': 'BERTContrastiveLM', 'dataset': 'bj', 'saved_model': True, 'train': True, 'gpu': True, 'gpu_id': 0, 'distribution': 'geometric', 'avg_mask_len': 2, 'contra_ratio': 0.4, 'mlm_ratio': 0.6, 'split': True, 'n_layers': 6, 'd_model': 256, 'attn_heads': 8, 'max_epoch': 5, 'batch_size': 8, 'grad_accmu_steps': 1, 'learning_rate': 0.0002, 'roadnetwork': 'bj_roadmap_edge_bj_True_1_merge', 'geo_file': 'bj_roadmap_edge_bj_True_1_merge_withdegree', 'rel_file': 'bj_roadmap_edge_bj_True_1_merge_withdegree', 'merge': True, 'min_freq': 1, 'seq_len': 128, 'test_every': 50, 'temperature': 0.05, 'contra_loss_type': 'simclr', 'classify_label': 'vflag', 'type_ln': 'post', 'add_cls': True, 'add_time_in_day': True, 'add_day_in_week': True, 'add_pe': True, 'add_temporal_bias': True, 'temporal_bias_dim': 64, 'use_mins_interval': False, 'add_gat': True, 'gat_heads_per_layer': [8, 16, 1], 'gat_features_per_layer': [16, 16, 256], 'gat_dropout': 0.1, 'gat_K': 1, 'gat_avg_last': True, 'load_trans_prob': True, 'append_degree2gcn': True, 'normal_feature': False, 'pooling': 'cls', 'dataset_class': 'ContrastiveSplitLMDataset', 'executor': 'ContrastiveSplitMLMExecutor', 'evaluator': 'ClassificationEvaluator', 'num_workers': 0, 'vocab_path': None, 'masking_ratio': 0.15, 'masking_mode': 'together', 'mlp_ratio': 4, 'pretrain_road_emb': None, 'future_mask': False, 'load_node2vec': False, 'dropout': 0.1, 'drop_path': 0.3, 'attn_drop': 0.1, 'seed': 0, 'learner': 'adamw', 'lr_eta_min': 0, 'lr_warmup_epoch': 4, 'lr_warmup_init': 1e-06, 'lr_decay': True, 'lr_scheduler': 'cosinelr', 'lr_decay_ratio': 0.1, 't_in_epochs': True, 'clip_grad_norm': True, 'max_grad_norm': 5, 'use_early_stop': True, 'patience': 10, 'log_batch': 500, 'log_every': 1, 'l2_reg': None, 'n_views': 2, 'similarity': 'cosine', 'data_argument1': [], 'data_argument2': [], 'cutoff_row_rate': 0.2, 'cutoff_column_rate': 0.2, 'cutoff_random_rate': 0.2, 'sample_rate': 0.2, 'align_w': 1.0, 'unif_w': 1.0, 'align_alpha': 2, 'unif_t': 2, 'train_align_uniform': False, 'test_align_uniform': True, 'norm_align_uniform': False, 'bidir_adj_mx': False, 'out_data_argument1': None, 'out_data_argument2': None, 'metrics': ['Precision', 'Recall', 'F1', 'MRR', 'NDCG'], 'save_modes': ['csv', 'json'], 'topk': [1, 5, 10], 'device': device(type='cuda', index=0), 'exp_id': 171360}
2024-01-01 15:10:42,004 - INFO - Loading Vocab from raw_data/vocab_bj_True_1_merge.pkl
2024-01-01 15:10:42,004 - INFO - vocab_path=raw_data/vocab_bj_True_1_merge.pkl, usr_num=75, vocab_size=27313
2024-01-01 15:10:42,062 - INFO - Loaded file bj_roadmap_edge_bj_True_1_merge_withdegree.geo, num_nodes=27309
2024-01-01 15:10:43,455 - INFO - Loaded file bj_roadmap_edge_bj_True_1_merge_withdegree.rel, shape=(27309, 27309), edges=51196.0
2024-01-01 15:10:43,482 - INFO - node_features: (27309, 42)
2024-01-01 15:10:44,500 - INFO - node_features_encoded: torch.Size([27313, 42])
2024-01-01 15:10:44,654 - INFO - edge_index: torch.Size([2, 78471])
2024-01-01 15:10:44,658 - INFO - Trajectory loc-transfer prob shape=torch.Size([78471, 1])
2024-01-01 15:10:44,665 - INFO - Loading Dataset!
2024-01-01 15:10:44,907 - INFO - Processing dataset in TrajectoryProcessingDataset!
2024-01-01 15:11:33,262 - INFO - Init TrajectoryProcessingDatasetSplitLM!
2024-01-01 15:11:34,756 - INFO - Load dataset from raw_data/bj/cache_bj_train_True_True_1.pkl, raw_data/bj/cache_bj_train_True_True_1.pkl
2024-01-01 15:11:34,834 - INFO - Processing dataset in TrajectoryProcessingDataset!
2024-01-01 15:11:50,735 - INFO - Init TrajectoryProcessingDatasetSplitLM!
2024-01-01 15:11:51,219 - INFO - Load dataset from raw_data/bj/cache_bj_eval_True_True_1.pkl, raw_data/bj/cache_bj_eval_True_True_1.pkl
2024-01-01 15:11:51,290 - INFO - Processing dataset in TrajectoryProcessingDataset!
2024-01-01 15:12:07,694 - INFO - Init TrajectoryProcessingDatasetSplitLM!
2024-01-01 15:12:08,284 - INFO - Load dataset from raw_data/bj/cache_bj_test_True_True_1.pkl, raw_data/bj/cache_bj_test_True_True_1.pkl
2024-01-01 15:12:08,284 - INFO - Size of dataset: 10510/3504/3504
2024-01-01 15:12:08,284 - INFO - Creating Dataloader!
2024-01-01 15:12:08,331 - INFO - Building BERTContrastiveLM model
2024-01-01 15:12:08,440 - INFO - Building BERTPooler model
2024-01-01 15:12:09,343 - INFO - BERTContrastiveLM(
  (bert): BERT(
    (embedding): BERTEmbedding(
      (token_embedding): GAT(
        (gat_net): Sequential(
          (0): GATLayerImp3(
            (linear_proj): Linear(in_features=42, out_features=128, bias=False)
            (linear_proj_tran_prob): Linear(in_features=1, out_features=128, bias=False)
            (skip_proj): Linear(in_features=42, out_features=128, bias=False)
            (leakyReLU): LeakyReLU(negative_slope=0.2)
            (softmax): Softmax(dim=-1)
            (activation): ELU(alpha=1.0)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (1): GATLayerImp3(
            (linear_proj): Linear(in_features=128, out_features=256, bias=False)
            (linear_proj_tran_prob): Linear(in_features=1, out_features=256, bias=False)
            (skip_proj): Linear(in_features=128, out_features=256, bias=False)
            (leakyReLU): LeakyReLU(negative_slope=0.2)
            (softmax): Softmax(dim=-1)
            (activation): ELU(alpha=1.0)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (2): GATLayerImp3(
            (linear_proj): Linear(in_features=256, out_features=256, bias=False)
            (linear_proj_tran_prob): Linear(in_features=1, out_features=256, bias=False)
            (skip_proj): Linear(in_features=256, out_features=256, bias=False)
            (leakyReLU): LeakyReLU(negative_slope=0.2)
            (softmax): Softmax(dim=-1)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (position_embedding): PositionalEmbedding()
      (daytime_embedding): Embedding(1441, 256, padding_idx=0)
      (weekday_embedding): Embedding(8, 256, padding_idx=0)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (transformer_blocks): ModuleList(
      (0): TransformerBlock(
        (attention): MultiHeadedAttention(
          (linear_layers): ModuleList(
            (0): Linear(in_features=256, out_features=256, bias=True)
            (1): Linear(in_features=256, out_features=256, bias=True)
            (2): Linear(in_features=256, out_features=256, bias=True)
          )
          (dropout): Dropout(p=0.1, inplace=False)
          (proj): Linear(in_features=256, out_features=256, bias=True)
          (proj_drop): Dropout(p=0.1, inplace=False)
          (temporal_mat_bias_1): Linear(in_features=1, out_features=64, bias=True)
          (temporal_mat_bias_2): Linear(in_features=64, out_features=1, bias=True)
        )
        (mlp): Mlp(
          (fc1): Linear(in_features=256, out_features=1024, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=1024, out_features=256, bias=True)
          (drop): Dropout(p=0.1, inplace=False)
        )
        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (drop_path): Identity()
      )
      (1): TransformerBlock(
        (attention): MultiHeadedAttention(
          (linear_layers): ModuleList(
            (0): Linear(in_features=256, out_features=256, bias=True)
            (1): Linear(in_features=256, out_features=256, bias=True)
            (2): Linear(in_features=256, out_features=256, bias=True)
          )
          (dropout): Dropout(p=0.1, inplace=False)
          (proj): Linear(in_features=256, out_features=256, bias=True)
          (proj_drop): Dropout(p=0.1, inplace=False)
          (temporal_mat_bias_1): Linear(in_features=1, out_features=64, bias=True)
          (temporal_mat_bias_2): Linear(in_features=64, out_features=1, bias=True)
        )
        (mlp): Mlp(
          (fc1): Linear(in_features=256, out_features=1024, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=1024, out_features=256, bias=True)
          (drop): Dropout(p=0.1, inplace=False)
        )
        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath()
      )
      (2): TransformerBlock(
        (attention): MultiHeadedAttention(
          (linear_layers): ModuleList(
            (0): Linear(in_features=256, out_features=256, bias=True)
            (1): Linear(in_features=256, out_features=256, bias=True)
            (2): Linear(in_features=256, out_features=256, bias=True)
          )
          (dropout): Dropout(p=0.1, inplace=False)
          (proj): Linear(in_features=256, out_features=256, bias=True)
          (proj_drop): Dropout(p=0.1, inplace=False)
          (temporal_mat_bias_1): Linear(in_features=1, out_features=64, bias=True)
          (temporal_mat_bias_2): Linear(in_features=64, out_features=1, bias=True)
        )
        (mlp): Mlp(
          (fc1): Linear(in_features=256, out_features=1024, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=1024, out_features=256, bias=True)
          (drop): Dropout(p=0.1, inplace=False)
        )
        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath()
      )
      (3): TransformerBlock(
        (attention): MultiHeadedAttention(
          (linear_layers): ModuleList(
            (0): Linear(in_features=256, out_features=256, bias=True)
            (1): Linear(in_features=256, out_features=256, bias=True)
            (2): Linear(in_features=256, out_features=256, bias=True)
          )
          (dropout): Dropout(p=0.1, inplace=False)
          (proj): Linear(in_features=256, out_features=256, bias=True)
          (proj_drop): Dropout(p=0.1, inplace=False)
          (temporal_mat_bias_1): Linear(in_features=1, out_features=64, bias=True)
          (temporal_mat_bias_2): Linear(in_features=64, out_features=1, bias=True)
        )
        (mlp): Mlp(
          (fc1): Linear(in_features=256, out_features=1024, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=1024, out_features=256, bias=True)
          (drop): Dropout(p=0.1, inplace=False)
        )
        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath()
      )
      (4): TransformerBlock(
        (attention): MultiHeadedAttention(
          (linear_layers): ModuleList(
            (0): Linear(in_features=256, out_features=256, bias=True)
            (1): Linear(in_features=256, out_features=256, bias=True)
            (2): Linear(in_features=256, out_features=256, bias=True)
          )
          (dropout): Dropout(p=0.1, inplace=False)
          (proj): Linear(in_features=256, out_features=256, bias=True)
          (proj_drop): Dropout(p=0.1, inplace=False)
          (temporal_mat_bias_1): Linear(in_features=1, out_features=64, bias=True)
          (temporal_mat_bias_2): Linear(in_features=64, out_features=1, bias=True)
        )
        (mlp): Mlp(
          (fc1): Linear(in_features=256, out_features=1024, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=1024, out_features=256, bias=True)
          (drop): Dropout(p=0.1, inplace=False)
        )
        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath()
      )
      (5): TransformerBlock(
        (attention): MultiHeadedAttention(
          (linear_layers): ModuleList(
            (0): Linear(in_features=256, out_features=256, bias=True)
            (1): Linear(in_features=256, out_features=256, bias=True)
            (2): Linear(in_features=256, out_features=256, bias=True)
          )
          (dropout): Dropout(p=0.1, inplace=False)
          (proj): Linear(in_features=256, out_features=256, bias=True)
          (proj_drop): Dropout(p=0.1, inplace=False)
          (temporal_mat_bias_1): Linear(in_features=1, out_features=64, bias=True)
          (temporal_mat_bias_2): Linear(in_features=64, out_features=1, bias=True)
        )
        (mlp): Mlp(
          (fc1): Linear(in_features=256, out_features=1024, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=1024, out_features=256, bias=True)
          (drop): Dropout(p=0.1, inplace=False)
        )
        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath()
      )
    )
  )
  (mask_l): MaskedLanguageModel(
    (linear): Linear(in_features=256, out_features=27313, bias=True)
    (softmax): LogSoftmax(dim=-1)
  )
  (pooler): BERTPooler(
    (linear): MLPLayer(
      (dense): Linear(in_features=256, out_features=256, bias=True)
      (activation): Tanh()
    )
  )
)
2024-01-01 15:12:09,343 - INFO - bert.embedding.token_embedding.gat_net.0.scoring_fn_target	torch.Size([1, 8, 16])	cuda:0	True
2024-01-01 15:12:09,343 - INFO - bert.embedding.token_embedding.gat_net.0.scoring_fn_source	torch.Size([1, 8, 16])	cuda:0	True
2024-01-01 15:12:09,343 - INFO - bert.embedding.token_embedding.gat_net.0.scoring_trans_prob	torch.Size([1, 8, 16])	cuda:0	True
2024-01-01 15:12:09,343 - INFO - bert.embedding.token_embedding.gat_net.0.bias	torch.Size([128])	cuda:0	True
2024-01-01 15:12:09,343 - INFO - bert.embedding.token_embedding.gat_net.0.linear_proj.weight	torch.Size([128, 42])	cuda:0	True
2024-01-01 15:12:09,343 - INFO - bert.embedding.token_embedding.gat_net.0.linear_proj_tran_prob.weight	torch.Size([128, 1])	cuda:0	True
2024-01-01 15:12:09,343 - INFO - bert.embedding.token_embedding.gat_net.0.skip_proj.weight	torch.Size([128, 42])	cuda:0	True
2024-01-01 15:12:09,343 - INFO - bert.embedding.token_embedding.gat_net.1.scoring_fn_target	torch.Size([1, 16, 16])	cuda:0	True
2024-01-01 15:12:09,343 - INFO - bert.embedding.token_embedding.gat_net.1.scoring_fn_source	torch.Size([1, 16, 16])	cuda:0	True
2024-01-01 15:12:09,343 - INFO - bert.embedding.token_embedding.gat_net.1.scoring_trans_prob	torch.Size([1, 16, 16])	cuda:0	True
2024-01-01 15:12:09,343 - INFO - bert.embedding.token_embedding.gat_net.1.bias	torch.Size([256])	cuda:0	True
2024-01-01 15:12:09,343 - INFO - bert.embedding.token_embedding.gat_net.1.linear_proj.weight	torch.Size([256, 128])	cuda:0	True
2024-01-01 15:12:09,343 - INFO - bert.embedding.token_embedding.gat_net.1.linear_proj_tran_prob.weight	torch.Size([256, 1])	cuda:0	True
2024-01-01 15:12:09,343 - INFO - bert.embedding.token_embedding.gat_net.1.skip_proj.weight	torch.Size([256, 128])	cuda:0	True
2024-01-01 15:12:09,343 - INFO - bert.embedding.token_embedding.gat_net.2.scoring_fn_target	torch.Size([1, 1, 256])	cuda:0	True
2024-01-01 15:12:09,343 - INFO - bert.embedding.token_embedding.gat_net.2.scoring_fn_source	torch.Size([1, 1, 256])	cuda:0	True
2024-01-01 15:12:09,343 - INFO - bert.embedding.token_embedding.gat_net.2.scoring_trans_prob	torch.Size([1, 1, 256])	cuda:0	True
2024-01-01 15:12:09,343 - INFO - bert.embedding.token_embedding.gat_net.2.bias	torch.Size([256])	cuda:0	True
2024-01-01 15:12:09,343 - INFO - bert.embedding.token_embedding.gat_net.2.linear_proj.weight	torch.Size([256, 256])	cuda:0	True
2024-01-01 15:12:09,343 - INFO - bert.embedding.token_embedding.gat_net.2.linear_proj_tran_prob.weight	torch.Size([256, 1])	cuda:0	True
2024-01-01 15:12:09,343 - INFO - bert.embedding.token_embedding.gat_net.2.skip_proj.weight	torch.Size([256, 256])	cuda:0	True
2024-01-01 15:12:09,343 - INFO - bert.embedding.daytime_embedding.weight	torch.Size([1441, 256])	cuda:0	True
2024-01-01 15:12:09,343 - INFO - bert.embedding.weekday_embedding.weight	torch.Size([8, 256])	cuda:0	True
2024-01-01 15:12:09,343 - INFO - bert.transformer_blocks.0.attention.linear_layers.0.weight	torch.Size([256, 256])	cuda:0	True
2024-01-01 15:12:09,343 - INFO - bert.transformer_blocks.0.attention.linear_layers.0.bias	torch.Size([256])	cuda:0	True
2024-01-01 15:12:09,343 - INFO - bert.transformer_blocks.0.attention.linear_layers.1.weight	torch.Size([256, 256])	cuda:0	True
2024-01-01 15:12:09,343 - INFO - bert.transformer_blocks.0.attention.linear_layers.1.bias	torch.Size([256])	cuda:0	True
2024-01-01 15:12:09,343 - INFO - bert.transformer_blocks.0.attention.linear_layers.2.weight	torch.Size([256, 256])	cuda:0	True
2024-01-01 15:12:09,343 - INFO - bert.transformer_blocks.0.attention.linear_layers.2.bias	torch.Size([256])	cuda:0	True
2024-01-01 15:12:09,343 - INFO - bert.transformer_blocks.0.attention.proj.weight	torch.Size([256, 256])	cuda:0	True
2024-01-01 15:12:09,343 - INFO - bert.transformer_blocks.0.attention.proj.bias	torch.Size([256])	cuda:0	True
2024-01-01 15:12:09,343 - INFO - bert.transformer_blocks.0.attention.temporal_mat_bias_1.weight	torch.Size([64, 1])	cuda:0	True
2024-01-01 15:12:09,343 - INFO - bert.transformer_blocks.0.attention.temporal_mat_bias_1.bias	torch.Size([64])	cuda:0	True
2024-01-01 15:12:09,343 - INFO - bert.transformer_blocks.0.attention.temporal_mat_bias_2.weight	torch.Size([1, 64])	cuda:0	True
2024-01-01 15:12:09,343 - INFO - bert.transformer_blocks.0.attention.temporal_mat_bias_2.bias	torch.Size([1])	cuda:0	True
2024-01-01 15:12:09,343 - INFO - bert.transformer_blocks.0.mlp.fc1.weight	torch.Size([1024, 256])	cuda:0	True
2024-01-01 15:12:09,343 - INFO - bert.transformer_blocks.0.mlp.fc1.bias	torch.Size([1024])	cuda:0	True
2024-01-01 15:12:09,343 - INFO - bert.transformer_blocks.0.mlp.fc2.weight	torch.Size([256, 1024])	cuda:0	True
2024-01-01 15:12:09,343 - INFO - bert.transformer_blocks.0.mlp.fc2.bias	torch.Size([256])	cuda:0	True
2024-01-01 15:12:09,343 - INFO - bert.transformer_blocks.0.norm1.weight	torch.Size([256])	cuda:0	True
2024-01-01 15:12:09,343 - INFO - bert.transformer_blocks.0.norm1.bias	torch.Size([256])	cuda:0	True
2024-01-01 15:12:09,343 - INFO - bert.transformer_blocks.0.norm2.weight	torch.Size([256])	cuda:0	True
2024-01-01 15:12:09,343 - INFO - bert.transformer_blocks.0.norm2.bias	torch.Size([256])	cuda:0	True
2024-01-01 15:12:09,343 - INFO - bert.transformer_blocks.1.attention.linear_layers.0.weight	torch.Size([256, 256])	cuda:0	True
2024-01-01 15:12:09,343 - INFO - bert.transformer_blocks.1.attention.linear_layers.0.bias	torch.Size([256])	cuda:0	True
2024-01-01 15:12:09,343 - INFO - bert.transformer_blocks.1.attention.linear_layers.1.weight	torch.Size([256, 256])	cuda:0	True
2024-01-01 15:12:09,343 - INFO - bert.transformer_blocks.1.attention.linear_layers.1.bias	torch.Size([256])	cuda:0	True
2024-01-01 15:12:09,343 - INFO - bert.transformer_blocks.1.attention.linear_layers.2.weight	torch.Size([256, 256])	cuda:0	True
2024-01-01 15:12:09,343 - INFO - bert.transformer_blocks.1.attention.linear_layers.2.bias	torch.Size([256])	cuda:0	True
2024-01-01 15:12:09,343 - INFO - bert.transformer_blocks.1.attention.proj.weight	torch.Size([256, 256])	cuda:0	True
2024-01-01 15:12:09,343 - INFO - bert.transformer_blocks.1.attention.proj.bias	torch.Size([256])	cuda:0	True
2024-01-01 15:12:09,343 - INFO - bert.transformer_blocks.1.attention.temporal_mat_bias_1.weight	torch.Size([64, 1])	cuda:0	True
2024-01-01 15:12:09,343 - INFO - bert.transformer_blocks.1.attention.temporal_mat_bias_1.bias	torch.Size([64])	cuda:0	True
2024-01-01 15:12:09,343 - INFO - bert.transformer_blocks.1.attention.temporal_mat_bias_2.weight	torch.Size([1, 64])	cuda:0	True
2024-01-01 15:12:09,343 - INFO - bert.transformer_blocks.1.attention.temporal_mat_bias_2.bias	torch.Size([1])	cuda:0	True
2024-01-01 15:12:09,343 - INFO - bert.transformer_blocks.1.mlp.fc1.weight	torch.Size([1024, 256])	cuda:0	True
2024-01-01 15:12:09,343 - INFO - bert.transformer_blocks.1.mlp.fc1.bias	torch.Size([1024])	cuda:0	True
2024-01-01 15:12:09,359 - INFO - bert.transformer_blocks.1.mlp.fc2.weight	torch.Size([256, 1024])	cuda:0	True
2024-01-01 15:12:09,359 - INFO - bert.transformer_blocks.1.mlp.fc2.bias	torch.Size([256])	cuda:0	True
2024-01-01 15:12:09,359 - INFO - bert.transformer_blocks.1.norm1.weight	torch.Size([256])	cuda:0	True
2024-01-01 15:12:09,359 - INFO - bert.transformer_blocks.1.norm1.bias	torch.Size([256])	cuda:0	True
2024-01-01 15:12:09,359 - INFO - bert.transformer_blocks.1.norm2.weight	torch.Size([256])	cuda:0	True
2024-01-01 15:12:09,359 - INFO - bert.transformer_blocks.1.norm2.bias	torch.Size([256])	cuda:0	True
2024-01-01 15:12:09,359 - INFO - bert.transformer_blocks.2.attention.linear_layers.0.weight	torch.Size([256, 256])	cuda:0	True
2024-01-01 15:12:09,359 - INFO - bert.transformer_blocks.2.attention.linear_layers.0.bias	torch.Size([256])	cuda:0	True
2024-01-01 15:12:09,359 - INFO - bert.transformer_blocks.2.attention.linear_layers.1.weight	torch.Size([256, 256])	cuda:0	True
2024-01-01 15:12:09,359 - INFO - bert.transformer_blocks.2.attention.linear_layers.1.bias	torch.Size([256])	cuda:0	True
2024-01-01 15:12:09,359 - INFO - bert.transformer_blocks.2.attention.linear_layers.2.weight	torch.Size([256, 256])	cuda:0	True
2024-01-01 15:12:09,359 - INFO - bert.transformer_blocks.2.attention.linear_layers.2.bias	torch.Size([256])	cuda:0	True
2024-01-01 15:12:09,359 - INFO - bert.transformer_blocks.2.attention.proj.weight	torch.Size([256, 256])	cuda:0	True
2024-01-01 15:12:09,359 - INFO - bert.transformer_blocks.2.attention.proj.bias	torch.Size([256])	cuda:0	True
2024-01-01 15:12:09,359 - INFO - bert.transformer_blocks.2.attention.temporal_mat_bias_1.weight	torch.Size([64, 1])	cuda:0	True
2024-01-01 15:12:09,359 - INFO - bert.transformer_blocks.2.attention.temporal_mat_bias_1.bias	torch.Size([64])	cuda:0	True
2024-01-01 15:12:09,359 - INFO - bert.transformer_blocks.2.attention.temporal_mat_bias_2.weight	torch.Size([1, 64])	cuda:0	True
2024-01-01 15:12:09,359 - INFO - bert.transformer_blocks.2.attention.temporal_mat_bias_2.bias	torch.Size([1])	cuda:0	True
2024-01-01 15:12:09,359 - INFO - bert.transformer_blocks.2.mlp.fc1.weight	torch.Size([1024, 256])	cuda:0	True
2024-01-01 15:12:09,359 - INFO - bert.transformer_blocks.2.mlp.fc1.bias	torch.Size([1024])	cuda:0	True
2024-01-01 15:12:09,359 - INFO - bert.transformer_blocks.2.mlp.fc2.weight	torch.Size([256, 1024])	cuda:0	True
2024-01-01 15:12:09,359 - INFO - bert.transformer_blocks.2.mlp.fc2.bias	torch.Size([256])	cuda:0	True
2024-01-01 15:12:09,359 - INFO - bert.transformer_blocks.2.norm1.weight	torch.Size([256])	cuda:0	True
2024-01-01 15:12:09,359 - INFO - bert.transformer_blocks.2.norm1.bias	torch.Size([256])	cuda:0	True
2024-01-01 15:12:09,359 - INFO - bert.transformer_blocks.2.norm2.weight	torch.Size([256])	cuda:0	True
2024-01-01 15:12:09,359 - INFO - bert.transformer_blocks.2.norm2.bias	torch.Size([256])	cuda:0	True
2024-01-01 15:12:09,359 - INFO - bert.transformer_blocks.3.attention.linear_layers.0.weight	torch.Size([256, 256])	cuda:0	True
2024-01-01 15:12:09,359 - INFO - bert.transformer_blocks.3.attention.linear_layers.0.bias	torch.Size([256])	cuda:0	True
2024-01-01 15:12:09,359 - INFO - bert.transformer_blocks.3.attention.linear_layers.1.weight	torch.Size([256, 256])	cuda:0	True
2024-01-01 15:12:09,359 - INFO - bert.transformer_blocks.3.attention.linear_layers.1.bias	torch.Size([256])	cuda:0	True
2024-01-01 15:12:09,359 - INFO - bert.transformer_blocks.3.attention.linear_layers.2.weight	torch.Size([256, 256])	cuda:0	True
2024-01-01 15:12:09,359 - INFO - bert.transformer_blocks.3.attention.linear_layers.2.bias	torch.Size([256])	cuda:0	True
2024-01-01 15:12:09,359 - INFO - bert.transformer_blocks.3.attention.proj.weight	torch.Size([256, 256])	cuda:0	True
2024-01-01 15:12:09,359 - INFO - bert.transformer_blocks.3.attention.proj.bias	torch.Size([256])	cuda:0	True
2024-01-01 15:12:09,359 - INFO - bert.transformer_blocks.3.attention.temporal_mat_bias_1.weight	torch.Size([64, 1])	cuda:0	True
2024-01-01 15:12:09,359 - INFO - bert.transformer_blocks.3.attention.temporal_mat_bias_1.bias	torch.Size([64])	cuda:0	True
2024-01-01 15:12:09,359 - INFO - bert.transformer_blocks.3.attention.temporal_mat_bias_2.weight	torch.Size([1, 64])	cuda:0	True
2024-01-01 15:12:09,359 - INFO - bert.transformer_blocks.3.attention.temporal_mat_bias_2.bias	torch.Size([1])	cuda:0	True
2024-01-01 15:12:09,359 - INFO - bert.transformer_blocks.3.mlp.fc1.weight	torch.Size([1024, 256])	cuda:0	True
2024-01-01 15:12:09,359 - INFO - bert.transformer_blocks.3.mlp.fc1.bias	torch.Size([1024])	cuda:0	True
2024-01-01 15:12:09,359 - INFO - bert.transformer_blocks.3.mlp.fc2.weight	torch.Size([256, 1024])	cuda:0	True
2024-01-01 15:12:09,359 - INFO - bert.transformer_blocks.3.mlp.fc2.bias	torch.Size([256])	cuda:0	True
2024-01-01 15:12:09,359 - INFO - bert.transformer_blocks.3.norm1.weight	torch.Size([256])	cuda:0	True
2024-01-01 15:12:09,359 - INFO - bert.transformer_blocks.3.norm1.bias	torch.Size([256])	cuda:0	True
2024-01-01 15:12:09,359 - INFO - bert.transformer_blocks.3.norm2.weight	torch.Size([256])	cuda:0	True
2024-01-01 15:12:09,359 - INFO - bert.transformer_blocks.3.norm2.bias	torch.Size([256])	cuda:0	True
2024-01-01 15:12:09,359 - INFO - bert.transformer_blocks.4.attention.linear_layers.0.weight	torch.Size([256, 256])	cuda:0	True
2024-01-01 15:12:09,359 - INFO - bert.transformer_blocks.4.attention.linear_layers.0.bias	torch.Size([256])	cuda:0	True
2024-01-01 15:12:09,359 - INFO - bert.transformer_blocks.4.attention.linear_layers.1.weight	torch.Size([256, 256])	cuda:0	True
2024-01-01 15:12:09,359 - INFO - bert.transformer_blocks.4.attention.linear_layers.1.bias	torch.Size([256])	cuda:0	True
2024-01-01 15:12:09,359 - INFO - bert.transformer_blocks.4.attention.linear_layers.2.weight	torch.Size([256, 256])	cuda:0	True
2024-01-01 15:12:09,359 - INFO - bert.transformer_blocks.4.attention.linear_layers.2.bias	torch.Size([256])	cuda:0	True
2024-01-01 15:12:09,359 - INFO - bert.transformer_blocks.4.attention.proj.weight	torch.Size([256, 256])	cuda:0	True
2024-01-01 15:12:09,359 - INFO - bert.transformer_blocks.4.attention.proj.bias	torch.Size([256])	cuda:0	True
2024-01-01 15:12:09,359 - INFO - bert.transformer_blocks.4.attention.temporal_mat_bias_1.weight	torch.Size([64, 1])	cuda:0	True
2024-01-01 15:12:09,359 - INFO - bert.transformer_blocks.4.attention.temporal_mat_bias_1.bias	torch.Size([64])	cuda:0	True
2024-01-01 15:12:09,359 - INFO - bert.transformer_blocks.4.attention.temporal_mat_bias_2.weight	torch.Size([1, 64])	cuda:0	True
2024-01-01 15:12:09,359 - INFO - bert.transformer_blocks.4.attention.temporal_mat_bias_2.bias	torch.Size([1])	cuda:0	True
2024-01-01 15:12:09,359 - INFO - bert.transformer_blocks.4.mlp.fc1.weight	torch.Size([1024, 256])	cuda:0	True
2024-01-01 15:12:09,359 - INFO - bert.transformer_blocks.4.mlp.fc1.bias	torch.Size([1024])	cuda:0	True
2024-01-01 15:12:09,359 - INFO - bert.transformer_blocks.4.mlp.fc2.weight	torch.Size([256, 1024])	cuda:0	True
2024-01-01 15:12:09,359 - INFO - bert.transformer_blocks.4.mlp.fc2.bias	torch.Size([256])	cuda:0	True
2024-01-01 15:12:09,359 - INFO - bert.transformer_blocks.4.norm1.weight	torch.Size([256])	cuda:0	True
2024-01-01 15:12:09,359 - INFO - bert.transformer_blocks.4.norm1.bias	torch.Size([256])	cuda:0	True
2024-01-01 15:12:09,359 - INFO - bert.transformer_blocks.4.norm2.weight	torch.Size([256])	cuda:0	True
2024-01-01 15:12:09,359 - INFO - bert.transformer_blocks.4.norm2.bias	torch.Size([256])	cuda:0	True
2024-01-01 15:12:09,359 - INFO - bert.transformer_blocks.5.attention.linear_layers.0.weight	torch.Size([256, 256])	cuda:0	True
2024-01-01 15:12:09,359 - INFO - bert.transformer_blocks.5.attention.linear_layers.0.bias	torch.Size([256])	cuda:0	True
2024-01-01 15:12:09,359 - INFO - bert.transformer_blocks.5.attention.linear_layers.1.weight	torch.Size([256, 256])	cuda:0	True
2024-01-01 15:12:09,359 - INFO - bert.transformer_blocks.5.attention.linear_layers.1.bias	torch.Size([256])	cuda:0	True
2024-01-01 15:12:09,359 - INFO - bert.transformer_blocks.5.attention.linear_layers.2.weight	torch.Size([256, 256])	cuda:0	True
2024-01-01 15:12:09,359 - INFO - bert.transformer_blocks.5.attention.linear_layers.2.bias	torch.Size([256])	cuda:0	True
2024-01-01 15:12:09,359 - INFO - bert.transformer_blocks.5.attention.proj.weight	torch.Size([256, 256])	cuda:0	True
2024-01-01 15:12:09,359 - INFO - bert.transformer_blocks.5.attention.proj.bias	torch.Size([256])	cuda:0	True
2024-01-01 15:12:09,359 - INFO - bert.transformer_blocks.5.attention.temporal_mat_bias_1.weight	torch.Size([64, 1])	cuda:0	True
2024-01-01 15:12:09,359 - INFO - bert.transformer_blocks.5.attention.temporal_mat_bias_1.bias	torch.Size([64])	cuda:0	True
2024-01-01 15:12:09,359 - INFO - bert.transformer_blocks.5.attention.temporal_mat_bias_2.weight	torch.Size([1, 64])	cuda:0	True
2024-01-01 15:12:09,359 - INFO - bert.transformer_blocks.5.attention.temporal_mat_bias_2.bias	torch.Size([1])	cuda:0	True
2024-01-01 15:12:09,359 - INFO - bert.transformer_blocks.5.mlp.fc1.weight	torch.Size([1024, 256])	cuda:0	True
2024-01-01 15:12:09,359 - INFO - bert.transformer_blocks.5.mlp.fc1.bias	torch.Size([1024])	cuda:0	True
2024-01-01 15:12:09,359 - INFO - bert.transformer_blocks.5.mlp.fc2.weight	torch.Size([256, 1024])	cuda:0	True
2024-01-01 15:12:09,359 - INFO - bert.transformer_blocks.5.mlp.fc2.bias	torch.Size([256])	cuda:0	True
2024-01-01 15:12:09,359 - INFO - bert.transformer_blocks.5.norm1.weight	torch.Size([256])	cuda:0	True
2024-01-01 15:12:09,359 - INFO - bert.transformer_blocks.5.norm1.bias	torch.Size([256])	cuda:0	True
2024-01-01 15:12:09,359 - INFO - bert.transformer_blocks.5.norm2.weight	torch.Size([256])	cuda:0	True
2024-01-01 15:12:09,359 - INFO - bert.transformer_blocks.5.norm2.bias	torch.Size([256])	cuda:0	True
2024-01-01 15:12:09,359 - INFO - mask_l.linear.weight	torch.Size([27313, 256])	cuda:0	True
2024-01-01 15:12:09,359 - INFO - mask_l.linear.bias	torch.Size([27313])	cuda:0	True
2024-01-01 15:12:09,359 - INFO - pooler.linear.dense.weight	torch.Size([256, 256])	cuda:0	True
2024-01-01 15:12:09,359 - INFO - pooler.linear.dense.bias	torch.Size([256])	cuda:0	True
2024-01-01 15:12:09,359 - INFO - Total parameter numbers: 12406455
2024-01-01 15:12:09,359 - INFO - You select `adamw` optimizer.
2024-01-01 15:12:09,359 - INFO - You select `cosinelr` lr_scheduler.
2024-01-01 15:12:09,375 - INFO - Start training ...
2024-01-01 15:12:09,375 - INFO - Num_batches: train=1314, eval=438
2024-01-01 15:12:13,881 - INFO - {'mode': 'Train', 'epoch': 0, 'iter': 0, 'lr': 1e-06, 'Loc acc(%)': 0.0, 'MLM loss': 10.395707130432129, 'Contrastive loss': 0.13149213790893555, 'align_loss': 32.58793640136719, 'uniform_loss': -inf}
2024-01-01 15:14:37,290 - INFO - {'mode': 'Train', 'epoch': 0, 'iter': 500, 'lr': 1e-06, 'Loc acc(%)': 0.0043496226702333576, 'MLM loss': 10.336143493652344, 'Contrastive loss': 0.059724949300289154, 'align_loss': 36.77803039550781, 'uniform_loss': -inf}
2024-01-01 15:17:08,310 - INFO - {'mode': 'Train', 'epoch': 0, 'iter': 1000, 'lr': 1e-06, 'Loc acc(%)': 0.005493237824238362, 'MLM loss': 10.365948677062988, 'Contrastive loss': 0.012943311594426632, 'align_loss': 32.35477828979492, 'uniform_loss': -inf}
2024-01-01 15:18:39,260 - INFO - Train: expid = 171360, Epoch = 0, avg_loss = 6.259452873531905, total_loc_acc = 0.0050217609641781055%.
2024-01-01 15:18:39,260 - INFO - epoch complete!
2024-01-01 15:18:39,260 - INFO - evaluating now!
2024-01-01 15:18:39,370 - INFO - {'mode': 'Train', 'epoch': 0, 'iter': 0, 'lr': 1e-06, 'Loc acc(%)': 0.0, 'MLM loss': 10.44034481048584, 'Contrastive loss': 0.0003127278760075569, 'align_loss': 2.304492170424055e-07, 'uniform_loss': -82.77803802490234}
2024-01-01 15:19:27,592 - INFO - Eval: expid = 171360, Epoch = 0, avg_loss = 6.2055642114926695, total_loc_acc = 0.005025251890751024%.
2024-01-01 15:19:27,592 - INFO - Epoch [0/5] (1314)  train_loss: 6.2595, val_loss: 6.2056, lr: 0.000051, 438.22s
2024-01-01 15:19:28,032 - INFO - Saved model at 0
2024-01-01 15:19:28,032 - INFO - Val loss decrease from inf to 6.2056, saving to ./libcity/cache/171360/model_cache/BERTContrastiveLM_bj_epoch0.tar
2024-01-01 15:19:28,313 - INFO - {'mode': 'Train', 'epoch': 1, 'iter': 0, 'lr': 5.075e-05, 'Loc acc(%)': 0.0, 'MLM loss': 10.410286903381348, 'Contrastive loss': 0.054003406316041946, 'align_loss': 35.4012336730957, 'uniform_loss': -inf}
2024-01-01 15:21:54,142 - INFO - {'mode': 'Train', 'epoch': 1, 'iter': 500, 'lr': 5.075e-05, 'Loc acc(%)': 0.02166002425922717, 'MLM loss': 9.696981430053711, 'Contrastive loss': 0.0008419126970693469, 'align_loss': 20.112857818603516, 'uniform_loss': -68.32048034667969}
2024-01-01 15:24:20,601 - INFO - {'mode': 'Train', 'epoch': 1, 'iter': 1000, 'lr': 5.075e-05, 'Loc acc(%)': 0.03597828220056257, 'MLM loss': 9.3201904296875, 'Contrastive loss': 0.011775954626500607, 'align_loss': 21.60915756225586, 'uniform_loss': -63.9183464050293}
2024-01-01 15:25:51,162 - INFO - Train: expid = 171360, Epoch = 1, avg_loss = 5.845152416548591, total_loc_acc = 0.042366897330885465%.
2024-01-01 15:25:51,162 - INFO - epoch complete!
2024-01-01 15:25:51,162 - INFO - evaluating now!
2024-01-01 15:25:51,282 - INFO - {'mode': 'Train', 'epoch': 1, 'iter': 0, 'lr': 5.075e-05, 'Loc acc(%)': 0.0, 'MLM loss': 9.063087463378906, 'Contrastive loss': 6.779958766855998e-06, 'align_loss': 2.9496337106138526e-07, 'uniform_loss': -95.81614685058594}
2024-01-01 15:26:36,741 - INFO - Eval: expid = 171360, Epoch = 1, avg_loss = 5.723852450444818, total_loc_acc = 0.051034729133175125%.
2024-01-01 15:26:36,741 - INFO - Epoch [1/5] (2628)  train_loss: 5.8452, val_loss: 5.7239, lr: 0.000101, 428.71s
2024-01-01 15:26:37,152 - INFO - Saved model at 1
2024-01-01 15:26:37,152 - INFO - Val loss decrease from 6.2056 to 5.7239, saving to ./libcity/cache/171360/model_cache/BERTContrastiveLM_bj_epoch1.tar
2024-01-01 15:26:37,462 - INFO - {'mode': 'Train', 'epoch': 2, 'iter': 0, 'lr': 0.0001005, 'Loc acc(%)': 0.0, 'MLM loss': 9.531110763549805, 'Contrastive loss': 0.013172408565878868, 'align_loss': 24.529434204101562, 'uniform_loss': -inf}
2024-01-01 15:29:03,574 - INFO - {'mode': 'Train', 'epoch': 2, 'iter': 500, 'lr': 0.0001005, 'Loc acc(%)': 0.04918619209443749, 'MLM loss': 9.72819995880127, 'Contrastive loss': 0.017702149227261543, 'align_loss': 29.76273536682129, 'uniform_loss': -inf}
2024-01-01 15:31:33,796 - INFO - {'mode': 'Train', 'epoch': 2, 'iter': 1000, 'lr': 0.0001005, 'Loc acc(%)': 0.046414481318171275, 'MLM loss': 9.313085556030273, 'Contrastive loss': 0.009057450108230114, 'align_loss': 23.39438819885254, 'uniform_loss': -inf}
2024-01-01 15:33:03,707 - INFO - Train: expid = 171360, Epoch = 2, avg_loss = 5.710968536147607, total_loc_acc = 0.04695426151846728%.
2024-01-01 15:33:03,707 - INFO - epoch complete!
2024-01-01 15:33:03,707 - INFO - evaluating now!
2024-01-01 15:33:03,817 - INFO - {'mode': 'Train', 'epoch': 2, 'iter': 0, 'lr': 0.0001005, 'Loc acc(%)': 0.0, 'MLM loss': 9.30645751953125, 'Contrastive loss': 6.746354483766481e-05, 'align_loss': 5.313210635904397e-07, 'uniform_loss': -inf}
2024-01-01 15:33:51,887 - INFO - Eval: expid = 171360, Epoch = 2, avg_loss = 5.715587517986559, total_loc_acc = 0.042907622412922765%.
2024-01-01 15:33:51,887 - INFO - Epoch [2/5] (3942)  train_loss: 5.7110, val_loss: 5.7156, lr: 0.000150, 434.74s
2024-01-01 15:33:52,247 - INFO - Saved model at 2
2024-01-01 15:33:52,247 - INFO - Val loss decrease from 5.7239 to 5.7156, saving to ./libcity/cache/171360/model_cache/BERTContrastiveLM_bj_epoch2.tar
2024-01-01 15:33:52,524 - INFO - {'mode': 'Train', 'epoch': 3, 'iter': 0, 'lr': 0.00015025000000000002, 'Loc acc(%)': 0.0, 'MLM loss': 9.61330795288086, 'Contrastive loss': 0.0004672172653954476, 'align_loss': 18.95572853088379, 'uniform_loss': -inf}
2024-01-01 15:36:15,035 - INFO - {'mode': 'Train', 'epoch': 3, 'iter': 500, 'lr': 0.00015025000000000002, 'Loc acc(%)': 0.08349996703948669, 'MLM loss': 9.334888458251953, 'Contrastive loss': 0.00017963135906029493, 'align_loss': 21.564842224121094, 'uniform_loss': -inf}
2024-01-01 15:38:41,572 - INFO - {'mode': 'Train', 'epoch': 3, 'iter': 1000, 'lr': 0.00015025000000000002, 'Loc acc(%)': 0.08153551202097886, 'MLM loss': 9.118098258972168, 'Contrastive loss': 0.0034866738133132458, 'align_loss': 20.309345245361328, 'uniform_loss': -inf}
2024-01-01 15:40:13,136 - INFO - Train: expid = 171360, Epoch = 3, avg_loss = 5.6903774495175625, total_loc_acc = 0.0903433045573178%.
2024-01-01 15:40:13,136 - INFO - epoch complete!
2024-01-01 15:40:13,136 - INFO - evaluating now!
2024-01-01 15:40:13,246 - INFO - {'mode': 'Train', 'epoch': 3, 'iter': 0, 'lr': 0.00015025000000000002, 'Loc acc(%)': 0.0, 'MLM loss': 9.080851554870605, 'Contrastive loss': 0.02744152396917343, 'align_loss': 9.71674722904936e-08, 'uniform_loss': -24.529212951660156}
2024-01-01 15:40:59,507 - INFO - Eval: expid = 171360, Epoch = 3, avg_loss = 5.652802877774522, total_loc_acc = 0.13282926330847042%.
2024-01-01 15:40:59,507 - INFO - Epoch [3/5] (5256)  train_loss: 5.6904, val_loss: 5.6528, lr: 0.000019, 427.26s
2024-01-01 15:40:59,857 - INFO - Saved model at 3
2024-01-01 15:40:59,857 - INFO - Val loss decrease from 5.7156 to 5.6528, saving to ./libcity/cache/171360/model_cache/BERTContrastiveLM_bj_epoch3.tar
2024-01-01 15:41:00,157 - INFO - {'mode': 'Train', 'epoch': 4, 'iter': 0, 'lr': 1.9098300562505266e-05, 'Loc acc(%)': 0.0, 'MLM loss': 9.301689147949219, 'Contrastive loss': 0.00099398463498801, 'align_loss': 20.4119873046875, 'uniform_loss': -inf}
2024-01-01 15:43:25,487 - INFO - {'mode': 'Train', 'epoch': 4, 'iter': 500, 'lr': 1.9098300562505266e-05, 'Loc acc(%)': 0.13326633604963625, 'MLM loss': 8.908760070800781, 'Contrastive loss': 9.356013470096514e-05, 'align_loss': 15.956321716308594, 'uniform_loss': -inf}
2024-01-01 15:45:51,516 - INFO - {'mode': 'Train', 'epoch': 4, 'iter': 1000, 'lr': 1.9098300562505266e-05, 'Loc acc(%)': 0.16574100498320637, 'MLM loss': 9.330318450927734, 'Contrastive loss': 0.013762889429926872, 'align_loss': 14.073728561401367, 'uniform_loss': -52.635746002197266}
2024-01-01 15:47:22,573 - INFO - Train: expid = 171360, Epoch = 4, avg_loss = 5.600069991348359, total_loc_acc = 0.17485150171505062%.
2024-01-01 15:47:22,573 - INFO - epoch complete!
2024-01-01 15:47:22,573 - INFO - evaluating now!
2024-01-01 15:47:22,683 - INFO - {'mode': 'Train', 'epoch': 4, 'iter': 0, 'lr': 1.9098300562505266e-05, 'Loc acc(%)': 1.2987012987012987, 'MLM loss': 8.963601112365723, 'Contrastive loss': 0.00016588666767347604, 'align_loss': 2.2182092607181403e-07, 'uniform_loss': -48.48133850097656}
2024-01-01 15:48:09,217 - INFO - Eval: expid = 171360, Epoch = 4, avg_loss = 5.6047156978415575, total_loc_acc = 0.18370627407955503%.
2024-01-01 15:48:09,217 - INFO - Epoch [4/5] (6570)  train_loss: 5.6001, val_loss: 5.6047, lr: 0.000020, 429.36s
2024-01-01 15:48:09,568 - INFO - Saved model at 4
2024-01-01 15:48:09,568 - INFO - Val loss decrease from 5.6528 to 5.6047, saving to ./libcity/cache/171360/model_cache/BERTContrastiveLM_bj_epoch4.tar
2024-01-01 15:48:09,568 - INFO - Trained totally 5 epochs, average train time is 384.635s, average eval time is 47.021s
2024-01-01 15:48:09,728 - INFO - Loaded model at 4
2024-01-01 15:48:09,826 - INFO - Save png at ./libcity/cache/171360/171360_loss.png
2024-01-01 15:48:09,896 - INFO - Save png at ./libcity/cache/171360/171360_acc.png
2024-01-01 15:48:09,966 - INFO - Save png at ./libcity/cache/171360/171360_lr.png
2024-01-01 15:48:10,267 - INFO - Saved model at ./libcity/cache/171360/model_cache/171360_BERTContrastiveLM_bj.pt
2024-01-01 15:48:10,267 - INFO - Start evaluating ...
2024-01-01 15:48:10,398 - INFO - {'mode': 'Train', 'epoch': 0, 'iter': 0, 'lr': 2e-05, 'Loc acc(%)': 0.0, 'MLM loss': 9.575873374938965, 'Contrastive loss': 0.00019326536857988685, 'align_loss': 2.010996240642271e-07, 'uniform_loss': -55.61564254760742}
2024-01-01 15:49:02,478 - INFO - Test: expid = 171360, Epoch = 0, avg_loss = 5.605272541307423, total_loc_acc = 0.1775191696047733%.
2024-01-01 15:49:02,478 - INFO - Evaluate result is {
 "Precision@1": 0.0017751916960477329,
 "Recall@1": 0.0017751916960477329,
 "F1@1": 0.0017751916960477329,
 "MRR@1": 0.0017751916960477329,
 "MAP@1": 0.0017751916960477329,
 "NDCG@1": 0.0017751916960477329,
 "Precision@5": 0.0014053600927044553,
 "Recall@5": 0.007026800463522276,
 "F1@5": 0.0023422668211740922,
 "MRR@5": 0.0035302481159134418,
 "MAP@5": 0.0035302481159134418,
 "NDCG@5": 0.004390976288147361,
 "Precision@10": 0.0012229098350551048,
 "Recall@10": 0.012229098350551049,
 "F1@10": 0.0022234724273729176,
 "MRR@10": 0.0042172249949808584,
 "MAP@10": 0.0042172249949808584,
 "NDCG@10": 0.006065878014622484
}
2024-01-01 15:49:02,478 - INFO - Evaluate result is saved at ./libcity/cache/171360/evaluate_cache\171360_2024_01_01_15_49_02_BERTContrastiveLM_bj.json
2024-01-01 15:49:02,488 - INFO - Evaluate result is saved at ./libcity/cache/171360/evaluate_cache\171360_2024_01_01_15_49_02_BERTContrastiveLM_bj.csv
2024-01-01 15:49:02,498 - INFO - 
    Precision    Recall        F1       MRR      NDCG
1    0.001775  0.001775  0.001775  0.001775  0.001775
5    0.001405  0.007027  0.002342  0.003530  0.004391
10   0.001223  0.012229  0.002223  0.004217  0.006066
2024-01-01 15:49:02,498 - INFO - Test time 52.231364488601685s.
