2024-01-02 15:12:56,374 - INFO - Log directory: ./libcity/log
2024-01-02 15:12:56,374 - INFO - Begin pretrain-pipeline, task=trajectory_embedding, model_name=BERTContrastiveLM, dataset_name=bj, exp_id=210080
2024-01-02 15:12:56,374 - INFO - {'task': 'trajectory_embedding', 'model': 'BERTContrastiveLM', 'dataset': 'bj', 'saved_model': True, 'train': True, 'gpu': True, 'gpu_id': 0, 'distribution': 'geometric', 'avg_mask_len': 2, 'contra_ratio': 0.4, 'mlm_ratio': 0.6, 'split': True, 'n_layers': 6, 'd_model': 256, 'attn_heads': 8, 'max_epoch': 30, 'batch_size': 8, 'grad_accmu_steps': 1, 'learning_rate': 0.0002, 'roadnetwork': 'bj_roadmap_edge_bj_True_1_merge', 'geo_file': 'bj_roadmap_edge_bj_True_1_merge_withdegree', 'rel_file': 'bj_roadmap_edge_bj_True_1_merge_withdegree', 'merge': True, 'min_freq': 1, 'seq_len': 128, 'test_every': 50, 'temperature': 0.05, 'contra_loss_type': 'simclr', 'classify_label': 'vflag', 'type_ln': 'post', 'add_cls': True, 'add_time_in_day': True, 'add_day_in_week': True, 'add_pe': True, 'add_temporal_bias': True, 'temporal_bias_dim': 64, 'use_mins_interval': False, 'add_gat': True, 'gat_heads_per_layer': [8, 16, 1], 'gat_features_per_layer': [16, 16, 256], 'gat_dropout': 0.1, 'gat_K': 1, 'gat_avg_last': True, 'load_trans_prob': True, 'append_degree2gcn': True, 'normal_feature': False, 'pooling': 'cls', 'dataset_class': 'ContrastiveSplitLMDataset', 'executor': 'ContrastiveSplitMLMExecutor', 'evaluator': 'ClassificationEvaluator', 'num_workers': 0, 'vocab_path': None, 'masking_ratio': 0.15, 'masking_mode': 'together', 'mlp_ratio': 4, 'pretrain_road_emb': None, 'future_mask': False, 'load_node2vec': False, 'dropout': 0.1, 'drop_path': 0.3, 'attn_drop': 0.1, 'seed': 0, 'learner': 'adamw', 'lr_eta_min': 0, 'lr_warmup_epoch': 4, 'lr_warmup_init': 1e-06, 'lr_decay': True, 'lr_scheduler': 'cosinelr', 'lr_decay_ratio': 0.1, 't_in_epochs': True, 'clip_grad_norm': True, 'max_grad_norm': 5, 'use_early_stop': True, 'patience': 10, 'log_batch': 500, 'log_every': 1, 'l2_reg': None, 'n_views': 2, 'similarity': 'cosine', 'data_argument1': [], 'data_argument2': [], 'cutoff_row_rate': 0.2, 'cutoff_column_rate': 0.2, 'cutoff_random_rate': 0.2, 'sample_rate': 0.2, 'align_w': 1.0, 'unif_w': 1.0, 'align_alpha': 2, 'unif_t': 2, 'train_align_uniform': False, 'test_align_uniform': True, 'norm_align_uniform': False, 'bidir_adj_mx': False, 'out_data_argument1': None, 'out_data_argument2': None, 'metrics': ['Precision', 'Recall', 'F1', 'MRR', 'NDCG'], 'save_modes': ['csv', 'json'], 'topk': [1, 5, 10], 'device': device(type='cuda', index=0), 'exp_id': 210080}
2024-01-02 15:12:56,886 - INFO - Loading Vocab from raw_data/vocab_bj_True_1_merge.pkl
2024-01-02 15:12:56,886 - INFO - vocab_path=raw_data/vocab_bj_True_1_merge.pkl, usr_num=3, vocab_size=386
2024-01-02 15:12:56,890 - INFO - Loaded file bj_roadmap_edge_bj_True_1_merge_withdegree.geo, num_nodes=382
2024-01-02 15:12:56,892 - INFO - Loaded file bj_roadmap_edge_bj_True_1_merge_withdegree.rel, shape=(382, 382), edges=276.0
2024-01-02 15:12:56,895 - INFO - node_features: (382, 27)
2024-01-02 15:12:57,822 - INFO - node_features_encoded: torch.Size([386, 27])
2024-01-02 15:12:57,824 - INFO - edge_index: torch.Size([2, 662])
2024-01-02 15:12:57,824 - INFO - Trajectory loc-transfer prob shape=torch.Size([662, 1])
2024-01-02 15:12:57,824 - INFO - Loading Dataset!
2024-01-02 15:12:57,825 - INFO - Load dataset from raw_data/bj/cache_bj_train_True_True_1.pkl
2024-01-02 15:12:57,825 - INFO - Init TrajectoryProcessingDatasetSplitLM!
2024-01-02 15:12:57,826 - INFO - Load dataset from raw_data/bj/cache_bj_train_True_True_1.pkl, raw_data/bj/cache_bj_train_True_True_1.pkl
2024-01-02 15:12:57,827 - INFO - Load dataset from raw_data/bj/cache_bj_eval_True_True_1.pkl
2024-01-02 15:12:57,827 - INFO - Init TrajectoryProcessingDatasetSplitLM!
2024-01-02 15:12:57,828 - INFO - Load dataset from raw_data/bj/cache_bj_eval_True_True_1.pkl, raw_data/bj/cache_bj_eval_True_True_1.pkl
2024-01-02 15:12:57,829 - INFO - Load dataset from raw_data/bj/cache_bj_test_True_True_1.pkl
2024-01-02 15:12:57,829 - INFO - Init TrajectoryProcessingDatasetSplitLM!
2024-01-02 15:12:57,829 - INFO - Load dataset from raw_data/bj/cache_bj_test_True_True_1.pkl, raw_data/bj/cache_bj_test_True_True_1.pkl
2024-01-02 15:12:57,830 - INFO - Size of dataset: 3/1/1
2024-01-02 15:12:57,830 - INFO - Creating Dataloader!
2024-01-02 15:12:57,832 - INFO - Building BERTContrastiveLM model
2024-01-02 15:12:57,870 - INFO - Building BERTPooler model
2024-01-02 15:12:58,436 - INFO - BERTContrastiveLM(
  (bert): BERT(
    (embedding): BERTEmbedding(
      (token_embedding): GAT(
        (gat_net): Sequential(
          (0): GATLayerImp3(
            (linear_proj): Linear(in_features=27, out_features=128, bias=False)
            (linear_proj_tran_prob): Linear(in_features=1, out_features=128, bias=False)
            (skip_proj): Linear(in_features=27, out_features=128, bias=False)
            (leakyReLU): LeakyReLU(negative_slope=0.2)
            (softmax): Softmax(dim=-1)
            (activation): ELU(alpha=1.0)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (1): GATLayerImp3(
            (linear_proj): Linear(in_features=128, out_features=256, bias=False)
            (linear_proj_tran_prob): Linear(in_features=1, out_features=256, bias=False)
            (skip_proj): Linear(in_features=128, out_features=256, bias=False)
            (leakyReLU): LeakyReLU(negative_slope=0.2)
            (softmax): Softmax(dim=-1)
            (activation): ELU(alpha=1.0)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (2): GATLayerImp3(
            (linear_proj): Linear(in_features=256, out_features=256, bias=False)
            (linear_proj_tran_prob): Linear(in_features=1, out_features=256, bias=False)
            (skip_proj): Linear(in_features=256, out_features=256, bias=False)
            (leakyReLU): LeakyReLU(negative_slope=0.2)
            (softmax): Softmax(dim=-1)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (position_embedding): PositionalEmbedding()
      (daytime_embedding): Embedding(1441, 256, padding_idx=0)
      (weekday_embedding): Embedding(8, 256, padding_idx=0)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (transformer_blocks): ModuleList(
      (0): TransformerBlock(
        (attention): MultiHeadedAttention(
          (linear_layers): ModuleList(
            (0): Linear(in_features=256, out_features=256, bias=True)
            (1): Linear(in_features=256, out_features=256, bias=True)
            (2): Linear(in_features=256, out_features=256, bias=True)
          )
          (dropout): Dropout(p=0.1, inplace=False)
          (proj): Linear(in_features=256, out_features=256, bias=True)
          (proj_drop): Dropout(p=0.1, inplace=False)
          (temporal_mat_bias_1): Linear(in_features=1, out_features=64, bias=True)
          (temporal_mat_bias_2): Linear(in_features=64, out_features=1, bias=True)
        )
        (mlp): Mlp(
          (fc1): Linear(in_features=256, out_features=1024, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=1024, out_features=256, bias=True)
          (drop): Dropout(p=0.1, inplace=False)
        )
        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (drop_path): Identity()
      )
      (1): TransformerBlock(
        (attention): MultiHeadedAttention(
          (linear_layers): ModuleList(
            (0): Linear(in_features=256, out_features=256, bias=True)
            (1): Linear(in_features=256, out_features=256, bias=True)
            (2): Linear(in_features=256, out_features=256, bias=True)
          )
          (dropout): Dropout(p=0.1, inplace=False)
          (proj): Linear(in_features=256, out_features=256, bias=True)
          (proj_drop): Dropout(p=0.1, inplace=False)
          (temporal_mat_bias_1): Linear(in_features=1, out_features=64, bias=True)
          (temporal_mat_bias_2): Linear(in_features=64, out_features=1, bias=True)
        )
        (mlp): Mlp(
          (fc1): Linear(in_features=256, out_features=1024, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=1024, out_features=256, bias=True)
          (drop): Dropout(p=0.1, inplace=False)
        )
        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath()
      )
      (2): TransformerBlock(
        (attention): MultiHeadedAttention(
          (linear_layers): ModuleList(
            (0): Linear(in_features=256, out_features=256, bias=True)
            (1): Linear(in_features=256, out_features=256, bias=True)
            (2): Linear(in_features=256, out_features=256, bias=True)
          )
          (dropout): Dropout(p=0.1, inplace=False)
          (proj): Linear(in_features=256, out_features=256, bias=True)
          (proj_drop): Dropout(p=0.1, inplace=False)
          (temporal_mat_bias_1): Linear(in_features=1, out_features=64, bias=True)
          (temporal_mat_bias_2): Linear(in_features=64, out_features=1, bias=True)
        )
        (mlp): Mlp(
          (fc1): Linear(in_features=256, out_features=1024, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=1024, out_features=256, bias=True)
          (drop): Dropout(p=0.1, inplace=False)
        )
        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath()
      )
      (3): TransformerBlock(
        (attention): MultiHeadedAttention(
          (linear_layers): ModuleList(
            (0): Linear(in_features=256, out_features=256, bias=True)
            (1): Linear(in_features=256, out_features=256, bias=True)
            (2): Linear(in_features=256, out_features=256, bias=True)
          )
          (dropout): Dropout(p=0.1, inplace=False)
          (proj): Linear(in_features=256, out_features=256, bias=True)
          (proj_drop): Dropout(p=0.1, inplace=False)
          (temporal_mat_bias_1): Linear(in_features=1, out_features=64, bias=True)
          (temporal_mat_bias_2): Linear(in_features=64, out_features=1, bias=True)
        )
        (mlp): Mlp(
          (fc1): Linear(in_features=256, out_features=1024, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=1024, out_features=256, bias=True)
          (drop): Dropout(p=0.1, inplace=False)
        )
        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath()
      )
      (4): TransformerBlock(
        (attention): MultiHeadedAttention(
          (linear_layers): ModuleList(
            (0): Linear(in_features=256, out_features=256, bias=True)
            (1): Linear(in_features=256, out_features=256, bias=True)
            (2): Linear(in_features=256, out_features=256, bias=True)
          )
          (dropout): Dropout(p=0.1, inplace=False)
          (proj): Linear(in_features=256, out_features=256, bias=True)
          (proj_drop): Dropout(p=0.1, inplace=False)
          (temporal_mat_bias_1): Linear(in_features=1, out_features=64, bias=True)
          (temporal_mat_bias_2): Linear(in_features=64, out_features=1, bias=True)
        )
        (mlp): Mlp(
          (fc1): Linear(in_features=256, out_features=1024, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=1024, out_features=256, bias=True)
          (drop): Dropout(p=0.1, inplace=False)
        )
        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath()
      )
      (5): TransformerBlock(
        (attention): MultiHeadedAttention(
          (linear_layers): ModuleList(
            (0): Linear(in_features=256, out_features=256, bias=True)
            (1): Linear(in_features=256, out_features=256, bias=True)
            (2): Linear(in_features=256, out_features=256, bias=True)
          )
          (dropout): Dropout(p=0.1, inplace=False)
          (proj): Linear(in_features=256, out_features=256, bias=True)
          (proj_drop): Dropout(p=0.1, inplace=False)
          (temporal_mat_bias_1): Linear(in_features=1, out_features=64, bias=True)
          (temporal_mat_bias_2): Linear(in_features=64, out_features=1, bias=True)
        )
        (mlp): Mlp(
          (fc1): Linear(in_features=256, out_features=1024, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=1024, out_features=256, bias=True)
          (drop): Dropout(p=0.1, inplace=False)
        )
        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath()
      )
    )
  )
  (mask_l): MaskedLanguageModel(
    (linear): Linear(in_features=256, out_features=386, bias=True)
    (softmax): LogSoftmax(dim=-1)
  )
  (pooler): BERTPooler(
    (linear): MLPLayer(
      (dense): Linear(in_features=256, out_features=256, bias=True)
      (activation): Tanh()
    )
  )
)
2024-01-02 15:12:58,439 - INFO - bert.embedding.token_embedding.gat_net.0.scoring_fn_target	torch.Size([1, 8, 16])	cuda:0	True
2024-01-02 15:12:58,439 - INFO - bert.embedding.token_embedding.gat_net.0.scoring_fn_source	torch.Size([1, 8, 16])	cuda:0	True
2024-01-02 15:12:58,439 - INFO - bert.embedding.token_embedding.gat_net.0.scoring_trans_prob	torch.Size([1, 8, 16])	cuda:0	True
2024-01-02 15:12:58,439 - INFO - bert.embedding.token_embedding.gat_net.0.bias	torch.Size([128])	cuda:0	True
2024-01-02 15:12:58,439 - INFO - bert.embedding.token_embedding.gat_net.0.linear_proj.weight	torch.Size([128, 27])	cuda:0	True
2024-01-02 15:12:58,439 - INFO - bert.embedding.token_embedding.gat_net.0.linear_proj_tran_prob.weight	torch.Size([128, 1])	cuda:0	True
2024-01-02 15:12:58,440 - INFO - bert.embedding.token_embedding.gat_net.0.skip_proj.weight	torch.Size([128, 27])	cuda:0	True
2024-01-02 15:12:58,440 - INFO - bert.embedding.token_embedding.gat_net.1.scoring_fn_target	torch.Size([1, 16, 16])	cuda:0	True
2024-01-02 15:12:58,440 - INFO - bert.embedding.token_embedding.gat_net.1.scoring_fn_source	torch.Size([1, 16, 16])	cuda:0	True
2024-01-02 15:12:58,440 - INFO - bert.embedding.token_embedding.gat_net.1.scoring_trans_prob	torch.Size([1, 16, 16])	cuda:0	True
2024-01-02 15:12:58,440 - INFO - bert.embedding.token_embedding.gat_net.1.bias	torch.Size([256])	cuda:0	True
2024-01-02 15:12:58,440 - INFO - bert.embedding.token_embedding.gat_net.1.linear_proj.weight	torch.Size([256, 128])	cuda:0	True
2024-01-02 15:12:58,440 - INFO - bert.embedding.token_embedding.gat_net.1.linear_proj_tran_prob.weight	torch.Size([256, 1])	cuda:0	True
2024-01-02 15:12:58,441 - INFO - bert.embedding.token_embedding.gat_net.1.skip_proj.weight	torch.Size([256, 128])	cuda:0	True
2024-01-02 15:12:58,441 - INFO - bert.embedding.token_embedding.gat_net.2.scoring_fn_target	torch.Size([1, 1, 256])	cuda:0	True
2024-01-02 15:12:58,441 - INFO - bert.embedding.token_embedding.gat_net.2.scoring_fn_source	torch.Size([1, 1, 256])	cuda:0	True
2024-01-02 15:12:58,441 - INFO - bert.embedding.token_embedding.gat_net.2.scoring_trans_prob	torch.Size([1, 1, 256])	cuda:0	True
2024-01-02 15:12:58,441 - INFO - bert.embedding.token_embedding.gat_net.2.bias	torch.Size([256])	cuda:0	True
2024-01-02 15:12:58,441 - INFO - bert.embedding.token_embedding.gat_net.2.linear_proj.weight	torch.Size([256, 256])	cuda:0	True
2024-01-02 15:12:58,442 - INFO - bert.embedding.token_embedding.gat_net.2.linear_proj_tran_prob.weight	torch.Size([256, 1])	cuda:0	True
2024-01-02 15:12:58,442 - INFO - bert.embedding.token_embedding.gat_net.2.skip_proj.weight	torch.Size([256, 256])	cuda:0	True
2024-01-02 15:12:58,442 - INFO - bert.embedding.daytime_embedding.weight	torch.Size([1441, 256])	cuda:0	True
2024-01-02 15:12:58,442 - INFO - bert.embedding.weekday_embedding.weight	torch.Size([8, 256])	cuda:0	True
2024-01-02 15:12:58,442 - INFO - bert.transformer_blocks.0.attention.linear_layers.0.weight	torch.Size([256, 256])	cuda:0	True
2024-01-02 15:12:58,442 - INFO - bert.transformer_blocks.0.attention.linear_layers.0.bias	torch.Size([256])	cuda:0	True
2024-01-02 15:12:58,442 - INFO - bert.transformer_blocks.0.attention.linear_layers.1.weight	torch.Size([256, 256])	cuda:0	True
2024-01-02 15:12:58,442 - INFO - bert.transformer_blocks.0.attention.linear_layers.1.bias	torch.Size([256])	cuda:0	True
2024-01-02 15:12:58,443 - INFO - bert.transformer_blocks.0.attention.linear_layers.2.weight	torch.Size([256, 256])	cuda:0	True
2024-01-02 15:12:58,443 - INFO - bert.transformer_blocks.0.attention.linear_layers.2.bias	torch.Size([256])	cuda:0	True
2024-01-02 15:12:58,443 - INFO - bert.transformer_blocks.0.attention.proj.weight	torch.Size([256, 256])	cuda:0	True
2024-01-02 15:12:58,443 - INFO - bert.transformer_blocks.0.attention.proj.bias	torch.Size([256])	cuda:0	True
2024-01-02 15:12:58,443 - INFO - bert.transformer_blocks.0.attention.temporal_mat_bias_1.weight	torch.Size([64, 1])	cuda:0	True
2024-01-02 15:12:58,443 - INFO - bert.transformer_blocks.0.attention.temporal_mat_bias_1.bias	torch.Size([64])	cuda:0	True
2024-01-02 15:12:58,443 - INFO - bert.transformer_blocks.0.attention.temporal_mat_bias_2.weight	torch.Size([1, 64])	cuda:0	True
2024-01-02 15:12:58,443 - INFO - bert.transformer_blocks.0.attention.temporal_mat_bias_2.bias	torch.Size([1])	cuda:0	True
2024-01-02 15:12:58,444 - INFO - bert.transformer_blocks.0.mlp.fc1.weight	torch.Size([1024, 256])	cuda:0	True
2024-01-02 15:12:58,444 - INFO - bert.transformer_blocks.0.mlp.fc1.bias	torch.Size([1024])	cuda:0	True
2024-01-02 15:12:58,444 - INFO - bert.transformer_blocks.0.mlp.fc2.weight	torch.Size([256, 1024])	cuda:0	True
2024-01-02 15:12:58,444 - INFO - bert.transformer_blocks.0.mlp.fc2.bias	torch.Size([256])	cuda:0	True
2024-01-02 15:12:58,444 - INFO - bert.transformer_blocks.0.norm1.weight	torch.Size([256])	cuda:0	True
2024-01-02 15:12:58,444 - INFO - bert.transformer_blocks.0.norm1.bias	torch.Size([256])	cuda:0	True
2024-01-02 15:12:58,444 - INFO - bert.transformer_blocks.0.norm2.weight	torch.Size([256])	cuda:0	True
2024-01-02 15:12:58,444 - INFO - bert.transformer_blocks.0.norm2.bias	torch.Size([256])	cuda:0	True
2024-01-02 15:12:58,444 - INFO - bert.transformer_blocks.1.attention.linear_layers.0.weight	torch.Size([256, 256])	cuda:0	True
2024-01-02 15:12:58,445 - INFO - bert.transformer_blocks.1.attention.linear_layers.0.bias	torch.Size([256])	cuda:0	True
2024-01-02 15:12:58,445 - INFO - bert.transformer_blocks.1.attention.linear_layers.1.weight	torch.Size([256, 256])	cuda:0	True
2024-01-02 15:12:58,445 - INFO - bert.transformer_blocks.1.attention.linear_layers.1.bias	torch.Size([256])	cuda:0	True
2024-01-02 15:12:58,445 - INFO - bert.transformer_blocks.1.attention.linear_layers.2.weight	torch.Size([256, 256])	cuda:0	True
2024-01-02 15:12:58,445 - INFO - bert.transformer_blocks.1.attention.linear_layers.2.bias	torch.Size([256])	cuda:0	True
2024-01-02 15:12:58,445 - INFO - bert.transformer_blocks.1.attention.proj.weight	torch.Size([256, 256])	cuda:0	True
2024-01-02 15:12:58,445 - INFO - bert.transformer_blocks.1.attention.proj.bias	torch.Size([256])	cuda:0	True
2024-01-02 15:12:58,445 - INFO - bert.transformer_blocks.1.attention.temporal_mat_bias_1.weight	torch.Size([64, 1])	cuda:0	True
2024-01-02 15:12:58,446 - INFO - bert.transformer_blocks.1.attention.temporal_mat_bias_1.bias	torch.Size([64])	cuda:0	True
2024-01-02 15:12:58,446 - INFO - bert.transformer_blocks.1.attention.temporal_mat_bias_2.weight	torch.Size([1, 64])	cuda:0	True
2024-01-02 15:12:58,446 - INFO - bert.transformer_blocks.1.attention.temporal_mat_bias_2.bias	torch.Size([1])	cuda:0	True
2024-01-02 15:12:58,446 - INFO - bert.transformer_blocks.1.mlp.fc1.weight	torch.Size([1024, 256])	cuda:0	True
2024-01-02 15:12:58,446 - INFO - bert.transformer_blocks.1.mlp.fc1.bias	torch.Size([1024])	cuda:0	True
2024-01-02 15:12:58,446 - INFO - bert.transformer_blocks.1.mlp.fc2.weight	torch.Size([256, 1024])	cuda:0	True
2024-01-02 15:12:58,447 - INFO - bert.transformer_blocks.1.mlp.fc2.bias	torch.Size([256])	cuda:0	True
2024-01-02 15:12:58,447 - INFO - bert.transformer_blocks.1.norm1.weight	torch.Size([256])	cuda:0	True
2024-01-02 15:12:58,447 - INFO - bert.transformer_blocks.1.norm1.bias	torch.Size([256])	cuda:0	True
2024-01-02 15:12:58,447 - INFO - bert.transformer_blocks.1.norm2.weight	torch.Size([256])	cuda:0	True
2024-01-02 15:12:58,447 - INFO - bert.transformer_blocks.1.norm2.bias	torch.Size([256])	cuda:0	True
2024-01-02 15:12:58,447 - INFO - bert.transformer_blocks.2.attention.linear_layers.0.weight	torch.Size([256, 256])	cuda:0	True
2024-01-02 15:12:58,447 - INFO - bert.transformer_blocks.2.attention.linear_layers.0.bias	torch.Size([256])	cuda:0	True
2024-01-02 15:12:58,447 - INFO - bert.transformer_blocks.2.attention.linear_layers.1.weight	torch.Size([256, 256])	cuda:0	True
2024-01-02 15:12:58,447 - INFO - bert.transformer_blocks.2.attention.linear_layers.1.bias	torch.Size([256])	cuda:0	True
2024-01-02 15:12:58,448 - INFO - bert.transformer_blocks.2.attention.linear_layers.2.weight	torch.Size([256, 256])	cuda:0	True
2024-01-02 15:12:58,448 - INFO - bert.transformer_blocks.2.attention.linear_layers.2.bias	torch.Size([256])	cuda:0	True
2024-01-02 15:12:58,448 - INFO - bert.transformer_blocks.2.attention.proj.weight	torch.Size([256, 256])	cuda:0	True
2024-01-02 15:12:58,448 - INFO - bert.transformer_blocks.2.attention.proj.bias	torch.Size([256])	cuda:0	True
2024-01-02 15:12:58,448 - INFO - bert.transformer_blocks.2.attention.temporal_mat_bias_1.weight	torch.Size([64, 1])	cuda:0	True
2024-01-02 15:12:58,448 - INFO - bert.transformer_blocks.2.attention.temporal_mat_bias_1.bias	torch.Size([64])	cuda:0	True
2024-01-02 15:12:58,448 - INFO - bert.transformer_blocks.2.attention.temporal_mat_bias_2.weight	torch.Size([1, 64])	cuda:0	True
2024-01-02 15:12:58,448 - INFO - bert.transformer_blocks.2.attention.temporal_mat_bias_2.bias	torch.Size([1])	cuda:0	True
2024-01-02 15:12:58,449 - INFO - bert.transformer_blocks.2.mlp.fc1.weight	torch.Size([1024, 256])	cuda:0	True
2024-01-02 15:12:58,449 - INFO - bert.transformer_blocks.2.mlp.fc1.bias	torch.Size([1024])	cuda:0	True
2024-01-02 15:12:58,449 - INFO - bert.transformer_blocks.2.mlp.fc2.weight	torch.Size([256, 1024])	cuda:0	True
2024-01-02 15:12:58,449 - INFO - bert.transformer_blocks.2.mlp.fc2.bias	torch.Size([256])	cuda:0	True
2024-01-02 15:12:58,449 - INFO - bert.transformer_blocks.2.norm1.weight	torch.Size([256])	cuda:0	True
2024-01-02 15:12:58,449 - INFO - bert.transformer_blocks.2.norm1.bias	torch.Size([256])	cuda:0	True
2024-01-02 15:12:58,449 - INFO - bert.transformer_blocks.2.norm2.weight	torch.Size([256])	cuda:0	True
2024-01-02 15:12:58,449 - INFO - bert.transformer_blocks.2.norm2.bias	torch.Size([256])	cuda:0	True
2024-01-02 15:12:58,449 - INFO - bert.transformer_blocks.3.attention.linear_layers.0.weight	torch.Size([256, 256])	cuda:0	True
2024-01-02 15:12:58,450 - INFO - bert.transformer_blocks.3.attention.linear_layers.0.bias	torch.Size([256])	cuda:0	True
2024-01-02 15:12:58,450 - INFO - bert.transformer_blocks.3.attention.linear_layers.1.weight	torch.Size([256, 256])	cuda:0	True
2024-01-02 15:12:58,450 - INFO - bert.transformer_blocks.3.attention.linear_layers.1.bias	torch.Size([256])	cuda:0	True
2024-01-02 15:12:58,450 - INFO - bert.transformer_blocks.3.attention.linear_layers.2.weight	torch.Size([256, 256])	cuda:0	True
2024-01-02 15:12:58,450 - INFO - bert.transformer_blocks.3.attention.linear_layers.2.bias	torch.Size([256])	cuda:0	True
2024-01-02 15:12:58,450 - INFO - bert.transformer_blocks.3.attention.proj.weight	torch.Size([256, 256])	cuda:0	True
2024-01-02 15:12:58,450 - INFO - bert.transformer_blocks.3.attention.proj.bias	torch.Size([256])	cuda:0	True
2024-01-02 15:12:58,450 - INFO - bert.transformer_blocks.3.attention.temporal_mat_bias_1.weight	torch.Size([64, 1])	cuda:0	True
2024-01-02 15:12:58,450 - INFO - bert.transformer_blocks.3.attention.temporal_mat_bias_1.bias	torch.Size([64])	cuda:0	True
2024-01-02 15:12:58,451 - INFO - bert.transformer_blocks.3.attention.temporal_mat_bias_2.weight	torch.Size([1, 64])	cuda:0	True
2024-01-02 15:12:58,451 - INFO - bert.transformer_blocks.3.attention.temporal_mat_bias_2.bias	torch.Size([1])	cuda:0	True
2024-01-02 15:12:58,451 - INFO - bert.transformer_blocks.3.mlp.fc1.weight	torch.Size([1024, 256])	cuda:0	True
2024-01-02 15:12:58,451 - INFO - bert.transformer_blocks.3.mlp.fc1.bias	torch.Size([1024])	cuda:0	True
2024-01-02 15:12:58,451 - INFO - bert.transformer_blocks.3.mlp.fc2.weight	torch.Size([256, 1024])	cuda:0	True
2024-01-02 15:12:58,451 - INFO - bert.transformer_blocks.3.mlp.fc2.bias	torch.Size([256])	cuda:0	True
2024-01-02 15:12:58,451 - INFO - bert.transformer_blocks.3.norm1.weight	torch.Size([256])	cuda:0	True
2024-01-02 15:12:58,452 - INFO - bert.transformer_blocks.3.norm1.bias	torch.Size([256])	cuda:0	True
2024-01-02 15:12:58,452 - INFO - bert.transformer_blocks.3.norm2.weight	torch.Size([256])	cuda:0	True
2024-01-02 15:12:58,452 - INFO - bert.transformer_blocks.3.norm2.bias	torch.Size([256])	cuda:0	True
2024-01-02 15:12:58,452 - INFO - bert.transformer_blocks.4.attention.linear_layers.0.weight	torch.Size([256, 256])	cuda:0	True
2024-01-02 15:12:58,452 - INFO - bert.transformer_blocks.4.attention.linear_layers.0.bias	torch.Size([256])	cuda:0	True
2024-01-02 15:12:58,452 - INFO - bert.transformer_blocks.4.attention.linear_layers.1.weight	torch.Size([256, 256])	cuda:0	True
2024-01-02 15:12:58,452 - INFO - bert.transformer_blocks.4.attention.linear_layers.1.bias	torch.Size([256])	cuda:0	True
2024-01-02 15:12:58,453 - INFO - bert.transformer_blocks.4.attention.linear_layers.2.weight	torch.Size([256, 256])	cuda:0	True
2024-01-02 15:12:58,453 - INFO - bert.transformer_blocks.4.attention.linear_layers.2.bias	torch.Size([256])	cuda:0	True
2024-01-02 15:12:58,453 - INFO - bert.transformer_blocks.4.attention.proj.weight	torch.Size([256, 256])	cuda:0	True
2024-01-02 15:12:58,453 - INFO - bert.transformer_blocks.4.attention.proj.bias	torch.Size([256])	cuda:0	True
2024-01-02 15:12:58,453 - INFO - bert.transformer_blocks.4.attention.temporal_mat_bias_1.weight	torch.Size([64, 1])	cuda:0	True
2024-01-02 15:12:58,453 - INFO - bert.transformer_blocks.4.attention.temporal_mat_bias_1.bias	torch.Size([64])	cuda:0	True
2024-01-02 15:12:58,453 - INFO - bert.transformer_blocks.4.attention.temporal_mat_bias_2.weight	torch.Size([1, 64])	cuda:0	True
2024-01-02 15:12:58,454 - INFO - bert.transformer_blocks.4.attention.temporal_mat_bias_2.bias	torch.Size([1])	cuda:0	True
2024-01-02 15:12:58,454 - INFO - bert.transformer_blocks.4.mlp.fc1.weight	torch.Size([1024, 256])	cuda:0	True
2024-01-02 15:12:58,454 - INFO - bert.transformer_blocks.4.mlp.fc1.bias	torch.Size([1024])	cuda:0	True
2024-01-02 15:12:58,454 - INFO - bert.transformer_blocks.4.mlp.fc2.weight	torch.Size([256, 1024])	cuda:0	True
2024-01-02 15:12:58,454 - INFO - bert.transformer_blocks.4.mlp.fc2.bias	torch.Size([256])	cuda:0	True
2024-01-02 15:12:58,454 - INFO - bert.transformer_blocks.4.norm1.weight	torch.Size([256])	cuda:0	True
2024-01-02 15:12:58,454 - INFO - bert.transformer_blocks.4.norm1.bias	torch.Size([256])	cuda:0	True
2024-01-02 15:12:58,454 - INFO - bert.transformer_blocks.4.norm2.weight	torch.Size([256])	cuda:0	True
2024-01-02 15:12:58,455 - INFO - bert.transformer_blocks.4.norm2.bias	torch.Size([256])	cuda:0	True
2024-01-02 15:12:58,455 - INFO - bert.transformer_blocks.5.attention.linear_layers.0.weight	torch.Size([256, 256])	cuda:0	True
2024-01-02 15:12:58,455 - INFO - bert.transformer_blocks.5.attention.linear_layers.0.bias	torch.Size([256])	cuda:0	True
2024-01-02 15:12:58,455 - INFO - bert.transformer_blocks.5.attention.linear_layers.1.weight	torch.Size([256, 256])	cuda:0	True
2024-01-02 15:12:58,455 - INFO - bert.transformer_blocks.5.attention.linear_layers.1.bias	torch.Size([256])	cuda:0	True
2024-01-02 15:12:58,455 - INFO - bert.transformer_blocks.5.attention.linear_layers.2.weight	torch.Size([256, 256])	cuda:0	True
2024-01-02 15:12:58,455 - INFO - bert.transformer_blocks.5.attention.linear_layers.2.bias	torch.Size([256])	cuda:0	True
2024-01-02 15:12:58,455 - INFO - bert.transformer_blocks.5.attention.proj.weight	torch.Size([256, 256])	cuda:0	True
2024-01-02 15:12:58,455 - INFO - bert.transformer_blocks.5.attention.proj.bias	torch.Size([256])	cuda:0	True
2024-01-02 15:12:58,456 - INFO - bert.transformer_blocks.5.attention.temporal_mat_bias_1.weight	torch.Size([64, 1])	cuda:0	True
2024-01-02 15:12:58,456 - INFO - bert.transformer_blocks.5.attention.temporal_mat_bias_1.bias	torch.Size([64])	cuda:0	True
2024-01-02 15:12:58,456 - INFO - bert.transformer_blocks.5.attention.temporal_mat_bias_2.weight	torch.Size([1, 64])	cuda:0	True
2024-01-02 15:12:58,456 - INFO - bert.transformer_blocks.5.attention.temporal_mat_bias_2.bias	torch.Size([1])	cuda:0	True
2024-01-02 15:12:58,456 - INFO - bert.transformer_blocks.5.mlp.fc1.weight	torch.Size([1024, 256])	cuda:0	True
2024-01-02 15:12:58,456 - INFO - bert.transformer_blocks.5.mlp.fc1.bias	torch.Size([1024])	cuda:0	True
2024-01-02 15:12:58,456 - INFO - bert.transformer_blocks.5.mlp.fc2.weight	torch.Size([256, 1024])	cuda:0	True
2024-01-02 15:12:58,456 - INFO - bert.transformer_blocks.5.mlp.fc2.bias	torch.Size([256])	cuda:0	True
2024-01-02 15:12:58,456 - INFO - bert.transformer_blocks.5.norm1.weight	torch.Size([256])	cuda:0	True
2024-01-02 15:12:58,457 - INFO - bert.transformer_blocks.5.norm1.bias	torch.Size([256])	cuda:0	True
2024-01-02 15:12:58,457 - INFO - bert.transformer_blocks.5.norm2.weight	torch.Size([256])	cuda:0	True
2024-01-02 15:12:58,457 - INFO - bert.transformer_blocks.5.norm2.bias	torch.Size([256])	cuda:0	True
2024-01-02 15:12:58,457 - INFO - mask_l.linear.weight	torch.Size([386, 256])	cuda:0	True
2024-01-02 15:12:58,457 - INFO - mask_l.linear.bias	torch.Size([386])	cuda:0	True
2024-01-02 15:12:58,457 - INFO - pooler.linear.dense.weight	torch.Size([256, 256])	cuda:0	True
2024-01-02 15:12:58,457 - INFO - pooler.linear.dense.bias	torch.Size([256])	cuda:0	True
2024-01-02 15:12:58,458 - INFO - Total parameter numbers: 5482376
2024-01-02 15:12:58,458 - INFO - You select `adamw` optimizer.
2024-01-02 15:12:58,459 - INFO - You select `cosinelr` lr_scheduler.
2024-01-02 15:12:58,463 - INFO - Start training ...
2024-01-02 15:12:58,463 - INFO - Num_batches: train=1, eval=1
2024-01-02 15:12:59,106 - INFO - {'mode': 'Train', 'epoch': 0, 'iter': 0, 'lr': 1e-06, 'Loc acc(%)': 0.0, 'MLM loss': 6.080909252166748, 'Contrastive loss': 1.5159646272659302, 'align_loss': 32.340843200683594, 'uniform_loss': -64.77069091796875}
2024-01-02 15:12:59,106 - INFO - Train: expid = 210080, Epoch = 0, avg_loss = 4.254931449890137, total_loc_acc = 0.0%.
2024-01-02 15:12:59,107 - INFO - epoch complete!
2024-01-02 15:12:59,108 - INFO - evaluating now!
2024-01-02 15:12:59,140 - INFO - {'mode': 'Train', 'epoch': 0, 'iter': 0, 'lr': 1e-06, 'Loc acc(%)': 0.0, 'MLM loss': 6.209202289581299, 'Contrastive loss': 0.0, 'align_loss': 0.0, 'uniform_loss': nan}
2024-01-02 15:12:59,140 - INFO - Eval: expid = 210080, Epoch = 0, avg_loss = 3.7255215644836426, total_loc_acc = 0.0%.
2024-01-02 15:12:59,140 - INFO - Epoch [0/30] (1)  train_loss: 4.2549, val_loss: 3.7255, lr: 0.000051, 0.68s
2024-01-02 15:12:59,322 - INFO - Saved model at 0
2024-01-02 15:12:59,322 - INFO - Val loss decrease from inf to 3.7255, saving to ./libcity/cache/210080/model_cache/BERTContrastiveLM_bj_epoch0.tar
2024-01-02 15:12:59,433 - INFO - {'mode': 'Train', 'epoch': 1, 'iter': 0, 'lr': 5.075e-05, 'Loc acc(%)': 0.0, 'MLM loss': 6.048948764801025, 'Contrastive loss': 1.382481575012207, 'align_loss': 29.063331604003906, 'uniform_loss': -48.76033020019531}
2024-01-02 15:12:59,434 - INFO - Train: expid = 210080, Epoch = 1, avg_loss = 4.182362079620361, total_loc_acc = 0.0%.
2024-01-02 15:12:59,434 - INFO - epoch complete!
2024-01-02 15:12:59,435 - INFO - evaluating now!
2024-01-02 15:12:59,469 - INFO - {'mode': 'Train', 'epoch': 1, 'iter': 0, 'lr': 5.075e-05, 'Loc acc(%)': 0.0, 'MLM loss': 6.3888139724731445, 'Contrastive loss': 0.0, 'align_loss': 2.070322935310287e-08, 'uniform_loss': nan}
2024-01-02 15:12:59,469 - INFO - Eval: expid = 210080, Epoch = 1, avg_loss = 3.8332884311676025, total_loc_acc = 0.0%.
2024-01-02 15:12:59,469 - INFO - Epoch [1/30] (2)  train_loss: 4.1824, val_loss: 3.8333, lr: 0.000101, 0.15s
2024-01-02 15:12:59,575 - INFO - {'mode': 'Train', 'epoch': 2, 'iter': 0, 'lr': 0.0001005, 'Loc acc(%)': 0.0, 'MLM loss': 6.130916118621826, 'Contrastive loss': 0.8510101437568665, 'align_loss': 30.09681510925293, 'uniform_loss': -66.421142578125}
2024-01-02 15:12:59,576 - INFO - Train: expid = 210080, Epoch = 2, avg_loss = 4.018953800201416, total_loc_acc = 0.0%.
2024-01-02 15:12:59,577 - INFO - epoch complete!
2024-01-02 15:12:59,577 - INFO - evaluating now!
2024-01-02 15:12:59,613 - INFO - {'mode': 'Train', 'epoch': 2, 'iter': 0, 'lr': 0.0001005, 'Loc acc(%)': 0.0, 'MLM loss': 6.253864288330078, 'Contrastive loss': 0.0, 'align_loss': 0.0, 'uniform_loss': nan}
2024-01-02 15:12:59,613 - INFO - Eval: expid = 210080, Epoch = 2, avg_loss = 3.7523186206817627, total_loc_acc = 0.0%.
2024-01-02 15:12:59,614 - INFO - Epoch [2/30] (3)  train_loss: 4.0190, val_loss: 3.7523, lr: 0.000150, 0.14s
2024-01-02 15:12:59,718 - INFO - {'mode': 'Train', 'epoch': 3, 'iter': 0, 'lr': 0.00015025000000000002, 'Loc acc(%)': 0.0, 'MLM loss': 6.295186519622803, 'Contrastive loss': 1.4056905508041382, 'align_loss': 33.67337417602539, 'uniform_loss': -66.3402099609375}
2024-01-02 15:12:59,719 - INFO - Train: expid = 210080, Epoch = 3, avg_loss = 4.339388370513916, total_loc_acc = 0.0%.
2024-01-02 15:12:59,720 - INFO - epoch complete!
2024-01-02 15:12:59,720 - INFO - evaluating now!
2024-01-02 15:12:59,755 - INFO - {'mode': 'Train', 'epoch': 3, 'iter': 0, 'lr': 0.00015025000000000002, 'Loc acc(%)': 0.0, 'MLM loss': 6.303207874298096, 'Contrastive loss': 0.0, 'align_loss': 2.727399817104015e-07, 'uniform_loss': nan}
2024-01-02 15:12:59,755 - INFO - Eval: expid = 210080, Epoch = 3, avg_loss = 3.7819249629974365, total_loc_acc = 0.0%.
2024-01-02 15:12:59,755 - INFO - Epoch [3/30] (4)  train_loss: 4.3394, val_loss: 3.7819, lr: 0.000191, 0.14s
2024-01-02 15:12:59,867 - INFO - {'mode': 'Train', 'epoch': 4, 'iter': 0, 'lr': 0.0001913545457642601, 'Loc acc(%)': 0.0, 'MLM loss': 6.057535648345947, 'Contrastive loss': 1.5460344552993774, 'align_loss': 31.253570556640625, 'uniform_loss': -66.21949768066406}
2024-01-02 15:12:59,867 - INFO - Train: expid = 210080, Epoch = 4, avg_loss = 4.252935409545898, total_loc_acc = 0.0%.
2024-01-02 15:12:59,868 - INFO - epoch complete!
2024-01-02 15:12:59,868 - INFO - evaluating now!
2024-01-02 15:12:59,903 - INFO - {'mode': 'Train', 'epoch': 4, 'iter': 0, 'lr': 0.0001913545457642601, 'Loc acc(%)': 0.0, 'MLM loss': 6.417288780212402, 'Contrastive loss': 0.0, 'align_loss': 0.0, 'uniform_loss': nan}
2024-01-02 15:12:59,904 - INFO - Eval: expid = 210080, Epoch = 4, avg_loss = 3.8503735065460205, total_loc_acc = 0.0%.
2024-01-02 15:12:59,904 - INFO - Epoch [4/30] (5)  train_loss: 4.2529, val_loss: 3.8504, lr: 0.000187, 0.15s
2024-01-02 15:13:00,012 - INFO - {'mode': 'Train', 'epoch': 5, 'iter': 0, 'lr': 0.00018660254037844388, 'Loc acc(%)': 0.0, 'MLM loss': 6.084648132324219, 'Contrastive loss': 1.4017316102981567, 'align_loss': 28.689489364624023, 'uniform_loss': -55.650325775146484}
2024-01-02 15:13:00,012 - INFO - Train: expid = 210080, Epoch = 5, avg_loss = 4.21148157119751, total_loc_acc = 0.0%.
2024-01-02 15:13:00,014 - INFO - epoch complete!
2024-01-02 15:13:00,014 - INFO - evaluating now!
2024-01-02 15:13:00,051 - INFO - {'mode': 'Train', 'epoch': 5, 'iter': 0, 'lr': 0.00018660254037844388, 'Loc acc(%)': 0.0, 'MLM loss': 6.549488067626953, 'Contrastive loss': 0.0, 'align_loss': 0.0, 'uniform_loss': nan}
2024-01-02 15:13:00,052 - INFO - Eval: expid = 210080, Epoch = 5, avg_loss = 3.9296929836273193, total_loc_acc = 0.0%.
2024-01-02 15:13:00,052 - INFO - Epoch [5/30] (6)  train_loss: 4.2115, val_loss: 3.9297, lr: 0.000181, 0.15s
2024-01-02 15:13:00,163 - INFO - {'mode': 'Train', 'epoch': 6, 'iter': 0, 'lr': 0.00018090169943749476, 'Loc acc(%)': 0.0, 'MLM loss': 6.100588798522949, 'Contrastive loss': 1.8770610094070435, 'align_loss': 33.30110168457031, 'uniform_loss': -48.965816497802734}
2024-01-02 15:13:00,164 - INFO - Train: expid = 210080, Epoch = 6, avg_loss = 4.411177635192871, total_loc_acc = 0.0%.
2024-01-02 15:13:00,165 - INFO - epoch complete!
2024-01-02 15:13:00,165 - INFO - evaluating now!
2024-01-02 15:13:00,196 - INFO - {'mode': 'Train', 'epoch': 6, 'iter': 0, 'lr': 0.00018090169943749476, 'Loc acc(%)': 0.0, 'MLM loss': 6.0041680335998535, 'Contrastive loss': 0.0, 'align_loss': 0.0, 'uniform_loss': nan}
2024-01-02 15:13:00,197 - INFO - Eval: expid = 210080, Epoch = 6, avg_loss = 3.6025009155273438, total_loc_acc = 0.0%.
2024-01-02 15:13:00,197 - INFO - Epoch [6/30] (7)  train_loss: 4.4112, val_loss: 3.6025, lr: 0.000174, 0.14s
2024-01-02 15:13:00,379 - INFO - Saved model at 6
2024-01-02 15:13:00,379 - INFO - Val loss decrease from 3.7255 to 3.6025, saving to ./libcity/cache/210080/model_cache/BERTContrastiveLM_bj_epoch6.tar
2024-01-02 15:13:00,490 - INFO - {'mode': 'Train', 'epoch': 7, 'iter': 0, 'lr': 0.00017431448254773944, 'Loc acc(%)': 0.0, 'MLM loss': 6.005574703216553, 'Contrastive loss': 2.1246001720428467, 'align_loss': 34.282814025878906, 'uniform_loss': -55.19542694091797}
2024-01-02 15:13:00,490 - INFO - Train: expid = 210080, Epoch = 7, avg_loss = 4.453185081481934, total_loc_acc = 0.0%.
2024-01-02 15:13:00,491 - INFO - epoch complete!
2024-01-02 15:13:00,492 - INFO - evaluating now!
2024-01-02 15:13:00,524 - INFO - {'mode': 'Train', 'epoch': 7, 'iter': 0, 'lr': 0.00017431448254773944, 'Loc acc(%)': 0.0, 'MLM loss': 6.140170574188232, 'Contrastive loss': 0.0, 'align_loss': 0.0, 'uniform_loss': nan}
2024-01-02 15:13:00,524 - INFO - Eval: expid = 210080, Epoch = 7, avg_loss = 3.6841025352478027, total_loc_acc = 0.0%.
2024-01-02 15:13:00,525 - INFO - Epoch [7/30] (8)  train_loss: 4.4532, val_loss: 3.6841, lr: 0.000167, 0.15s
2024-01-02 15:13:00,624 - INFO - {'mode': 'Train', 'epoch': 8, 'iter': 0, 'lr': 0.00016691306063588583, 'Loc acc(%)': 0.0, 'MLM loss': 6.010636806488037, 'Contrastive loss': 1.7736835479736328, 'align_loss': 33.8441162109375, 'uniform_loss': -67.48753356933594}
2024-01-02 15:13:00,624 - INFO - Train: expid = 210080, Epoch = 8, avg_loss = 4.315855503082275, total_loc_acc = 0.0%.
2024-01-02 15:13:00,625 - INFO - epoch complete!
2024-01-02 15:13:00,625 - INFO - evaluating now!
2024-01-02 15:13:00,659 - INFO - {'mode': 'Train', 'epoch': 8, 'iter': 0, 'lr': 0.00016691306063588583, 'Loc acc(%)': 0.0, 'MLM loss': 6.263782501220703, 'Contrastive loss': 0.0, 'align_loss': 0.0, 'uniform_loss': nan}
2024-01-02 15:13:00,659 - INFO - Eval: expid = 210080, Epoch = 8, avg_loss = 3.7582695484161377, total_loc_acc = 0.0%.
2024-01-02 15:13:00,660 - INFO - Epoch [8/30] (9)  train_loss: 4.3159, val_loss: 3.7583, lr: 0.000159, 0.13s
2024-01-02 15:13:00,759 - INFO - {'mode': 'Train', 'epoch': 9, 'iter': 0, 'lr': 0.00015877852522924732, 'Loc acc(%)': 2.5, 'MLM loss': 5.946674346923828, 'Contrastive loss': 2.6861732006073, 'align_loss': 38.95793533325195, 'uniform_loss': -50.09043884277344}
2024-01-02 15:13:00,759 - INFO - Train: expid = 210080, Epoch = 9, avg_loss = 4.642474174499512, total_loc_acc = 2.5%.
2024-01-02 15:13:00,760 - INFO - epoch complete!
2024-01-02 15:13:00,760 - INFO - evaluating now!
2024-01-02 15:13:00,794 - INFO - {'mode': 'Train', 'epoch': 9, 'iter': 0, 'lr': 0.00015877852522924732, 'Loc acc(%)': 0.0, 'MLM loss': 6.399398326873779, 'Contrastive loss': 0.0, 'align_loss': 0.0, 'uniform_loss': nan}
2024-01-02 15:13:00,795 - INFO - Eval: expid = 210080, Epoch = 9, avg_loss = 3.839639186859131, total_loc_acc = 0.0%.
2024-01-02 15:13:00,795 - INFO - Epoch [9/30] (10)  train_loss: 4.6425, val_loss: 3.8396, lr: 0.000150, 0.13s
2024-01-02 15:13:00,910 - INFO - {'mode': 'Train', 'epoch': 10, 'iter': 0, 'lr': 0.00015000000000000001, 'Loc acc(%)': 0.0, 'MLM loss': 5.966390609741211, 'Contrastive loss': 1.529526710510254, 'align_loss': 33.897552490234375, 'uniform_loss': -73.62812805175781}
2024-01-02 15:13:00,910 - INFO - Train: expid = 210080, Epoch = 10, avg_loss = 4.19164514541626, total_loc_acc = 0.0%.
2024-01-02 15:13:00,911 - INFO - epoch complete!
2024-01-02 15:13:00,912 - INFO - evaluating now!
2024-01-02 15:13:00,944 - INFO - {'mode': 'Train', 'epoch': 10, 'iter': 0, 'lr': 0.00015000000000000001, 'Loc acc(%)': 0.0, 'MLM loss': 6.468156814575195, 'Contrastive loss': 0.0, 'align_loss': 0.0, 'uniform_loss': nan}
2024-01-02 15:13:00,945 - INFO - Eval: expid = 210080, Epoch = 10, avg_loss = 3.880894184112549, total_loc_acc = 0.0%.
2024-01-02 15:13:00,945 - INFO - Epoch [10/30] (11)  train_loss: 4.1916, val_loss: 3.8809, lr: 0.000141, 0.15s
2024-01-02 15:13:01,050 - INFO - {'mode': 'Train', 'epoch': 11, 'iter': 0, 'lr': 0.00014067366430758004, 'Loc acc(%)': 0.0, 'MLM loss': 5.7144341468811035, 'Contrastive loss': 0.5562987923622131, 'align_loss': 24.527690887451172, 'uniform_loss': -67.76393127441406}
2024-01-02 15:13:01,051 - INFO - Train: expid = 210080, Epoch = 11, avg_loss = 3.6511802673339844, total_loc_acc = 0.0%.
2024-01-02 15:13:01,052 - INFO - epoch complete!
2024-01-02 15:13:01,052 - INFO - evaluating now!
2024-01-02 15:13:01,087 - INFO - {'mode': 'Train', 'epoch': 11, 'iter': 0, 'lr': 0.00014067366430758004, 'Loc acc(%)': 0.0, 'MLM loss': 6.245360851287842, 'Contrastive loss': 0.0, 'align_loss': 0.0, 'uniform_loss': nan}
2024-01-02 15:13:01,087 - INFO - Eval: expid = 210080, Epoch = 11, avg_loss = 3.7472167015075684, total_loc_acc = 0.0%.
2024-01-02 15:13:01,087 - INFO - Epoch [11/30] (12)  train_loss: 3.6512, val_loss: 3.7472, lr: 0.000131, 0.14s
2024-01-02 15:13:01,191 - INFO - {'mode': 'Train', 'epoch': 12, 'iter': 0, 'lr': 0.00013090169943749476, 'Loc acc(%)': 2.7027027027027026, 'MLM loss': 6.071624755859375, 'Contrastive loss': 0.7023653388023376, 'align_loss': 26.573911666870117, 'uniform_loss': -70.64543151855469}
2024-01-02 15:13:01,192 - INFO - Train: expid = 210080, Epoch = 12, avg_loss = 3.9239211082458496, total_loc_acc = 2.7027027027027026%.
2024-01-02 15:13:01,193 - INFO - epoch complete!
2024-01-02 15:13:01,193 - INFO - evaluating now!
2024-01-02 15:13:01,225 - INFO - {'mode': 'Train', 'epoch': 12, 'iter': 0, 'lr': 0.00013090169943749476, 'Loc acc(%)': 0.0, 'MLM loss': 6.543419361114502, 'Contrastive loss': 0.0, 'align_loss': 0.0, 'uniform_loss': nan}
2024-01-02 15:13:01,225 - INFO - Eval: expid = 210080, Epoch = 12, avg_loss = 3.9260518550872803, total_loc_acc = 0.0%.
2024-01-02 15:13:01,225 - INFO - Epoch [12/30] (13)  train_loss: 3.9239, val_loss: 3.9261, lr: 0.000121, 0.14s
2024-01-02 15:13:01,334 - INFO - {'mode': 'Train', 'epoch': 13, 'iter': 0, 'lr': 0.00012079116908177593, 'Loc acc(%)': 0.0, 'MLM loss': 5.835423469543457, 'Contrastive loss': 0.3178353011608124, 'align_loss': 27.356571197509766, 'uniform_loss': -85.1900405883789}
2024-01-02 15:13:01,335 - INFO - Train: expid = 210080, Epoch = 13, avg_loss = 3.6283884048461914, total_loc_acc = 0.0%.
2024-01-02 15:13:01,336 - INFO - epoch complete!
2024-01-02 15:13:01,336 - INFO - evaluating now!
2024-01-02 15:13:01,367 - INFO - {'mode': 'Train', 'epoch': 13, 'iter': 0, 'lr': 0.00012079116908177593, 'Loc acc(%)': 0.0, 'MLM loss': 6.405543327331543, 'Contrastive loss': 0.0, 'align_loss': 9.845906561167794e-07, 'uniform_loss': nan}
2024-01-02 15:13:01,368 - INFO - Eval: expid = 210080, Epoch = 13, avg_loss = 3.8433260917663574, total_loc_acc = 0.0%.
2024-01-02 15:13:01,368 - INFO - Epoch [13/30] (14)  train_loss: 3.6284, val_loss: 3.8433, lr: 0.000110, 0.14s
2024-01-02 15:13:01,470 - INFO - {'mode': 'Train', 'epoch': 14, 'iter': 0, 'lr': 0.00011045284632676536, 'Loc acc(%)': 2.941176470588235, 'MLM loss': 5.762869358062744, 'Contrastive loss': 0.629705011844635, 'align_loss': 32.336639404296875, 'uniform_loss': -69.53012084960938}
2024-01-02 15:13:01,470 - INFO - Train: expid = 210080, Epoch = 14, avg_loss = 3.709603786468506, total_loc_acc = 2.941176470588235%.
2024-01-02 15:13:01,471 - INFO - epoch complete!
2024-01-02 15:13:01,471 - INFO - evaluating now!
2024-01-02 15:13:01,502 - INFO - {'mode': 'Train', 'epoch': 14, 'iter': 0, 'lr': 0.00011045284632676536, 'Loc acc(%)': 0.0, 'MLM loss': 6.387652397155762, 'Contrastive loss': 0.0, 'align_loss': 0.0, 'uniform_loss': nan}
2024-01-02 15:13:01,503 - INFO - Eval: expid = 210080, Epoch = 14, avg_loss = 3.8325915336608887, total_loc_acc = 0.0%.
2024-01-02 15:13:01,503 - INFO - Epoch [14/30] (15)  train_loss: 3.7096, val_loss: 3.8326, lr: 0.000100, 0.14s
2024-01-02 15:13:01,601 - INFO - {'mode': 'Train', 'epoch': 15, 'iter': 0, 'lr': 0.00010000000000000003, 'Loc acc(%)': 0.0, 'MLM loss': 6.079440593719482, 'Contrastive loss': 0.06200240179896355, 'align_loss': 30.435422897338867, 'uniform_loss': -inf}
2024-01-02 15:13:01,602 - INFO - Train: expid = 210080, Epoch = 15, avg_loss = 3.6724655628204346, total_loc_acc = 0.0%.
2024-01-02 15:13:01,603 - INFO - epoch complete!
2024-01-02 15:13:01,603 - INFO - evaluating now!
2024-01-02 15:13:01,634 - INFO - {'mode': 'Train', 'epoch': 15, 'iter': 0, 'lr': 0.00010000000000000003, 'Loc acc(%)': 0.0, 'MLM loss': 6.497738361358643, 'Contrastive loss': 0.0, 'align_loss': 1.3329226611347167e-08, 'uniform_loss': nan}
2024-01-02 15:13:01,634 - INFO - Eval: expid = 210080, Epoch = 15, avg_loss = 3.8986432552337646, total_loc_acc = 0.0%.
2024-01-02 15:13:01,635 - INFO - Epoch [15/30] (16)  train_loss: 3.6725, val_loss: 3.8986, lr: 0.000090, 0.13s
2024-01-02 15:13:01,742 - INFO - {'mode': 'Train', 'epoch': 16, 'iter': 0, 'lr': 8.954715367323468e-05, 'Loc acc(%)': 0.0, 'MLM loss': 5.846414089202881, 'Contrastive loss': 0.02475987933576107, 'align_loss': 22.082427978515625, 'uniform_loss': -inf}
2024-01-02 15:13:01,743 - INFO - Train: expid = 210080, Epoch = 16, avg_loss = 3.5177524089813232, total_loc_acc = 0.0%.
2024-01-02 15:13:01,744 - INFO - epoch complete!
2024-01-02 15:13:01,744 - INFO - evaluating now!
2024-01-02 15:13:01,774 - INFO - {'mode': 'Train', 'epoch': 16, 'iter': 0, 'lr': 8.954715367323468e-05, 'Loc acc(%)': 0.0, 'MLM loss': 6.694282054901123, 'Contrastive loss': 0.0, 'align_loss': 2.420085820631357e-07, 'uniform_loss': nan}
2024-01-02 15:13:01,775 - INFO - Eval: expid = 210080, Epoch = 16, avg_loss = 4.0165696144104, total_loc_acc = 0.0%.
2024-01-02 15:13:01,775 - INFO - Epoch [16/30] (17)  train_loss: 3.5178, val_loss: 4.0166, lr: 0.000079, 0.14s
2024-01-02 15:13:01,775 - WARNING - Early stopping at epoch: 16
2024-01-02 15:13:01,775 - INFO - Trained totally 17 epochs, average train time is 0.139s, average eval time is 0.034s
2024-01-02 15:13:01,868 - INFO - Loaded model at 6
2024-01-02 15:13:01,949 - INFO - Save png at ./libcity/cache/210080/210080_loss.png
2024-01-02 15:13:02,015 - INFO - Save png at ./libcity/cache/210080/210080_acc.png
2024-01-02 15:13:02,079 - INFO - Save png at ./libcity/cache/210080/210080_lr.png
2024-01-02 15:13:02,214 - INFO - Saved model at ./libcity/cache/210080/model_cache/210080_BERTContrastiveLM_bj.pt
2024-01-02 15:13:02,214 - INFO - Start evaluating ...
2024-01-02 15:13:02,248 - INFO - {'mode': 'Train', 'epoch': 0, 'iter': 0, 'lr': 0.00017431448254773944, 'Loc acc(%)': 0.0, 'MLM loss': 6.054050922393799, 'Contrastive loss': 0.0, 'align_loss': 0.0, 'uniform_loss': nan}
2024-01-02 15:13:02,248 - INFO - Test: expid = 210080, Epoch = 0, avg_loss = 3.6324307918548584, total_loc_acc = 0.0%.
2024-01-02 15:13:02,249 - INFO - Evaluate result is {
 "Precision@1": 0.0,
 "Recall@1": 0.0,
 "F1@1": 0.0,
 "MRR@1": 0.0,
 "MAP@1": 0.0,
 "NDCG@1": 0.0,
 "Precision@5": 0.0,
 "Recall@5": 0.0,
 "F1@5": 0.0,
 "MRR@5": 0.0,
 "MAP@5": 0.0,
 "NDCG@5": 0.0,
 "Precision@10": 0.0,
 "Recall@10": 0.0,
 "F1@10": 0.0,
 "MRR@10": 0.0,
 "MAP@10": 0.0,
 "NDCG@10": 0.0
}
2024-01-02 15:13:02,250 - INFO - Evaluate result is saved at ./libcity/cache/210080/evaluate_cache\210080_2024_01_02_15_13_02_BERTContrastiveLM_bj.json
2024-01-02 15:13:02,253 - INFO - Evaluate result is saved at ./libcity/cache/210080/evaluate_cache\210080_2024_01_02_15_13_02_BERTContrastiveLM_bj.csv
2024-01-02 15:13:02,259 - INFO - 
    Precision  Recall   F1  MRR  NDCG
1         0.0     0.0  0.0  0.0   0.0
5         0.0     0.0  0.0  0.0   0.0
10        0.0     0.0  0.0  0.0   0.0
2024-01-02 15:13:02,259 - INFO - Test time 0.0441746711730957s.
