2024-01-01 13:36:15,417 - INFO - Log directory: ./libcity/log
2024-01-01 13:36:15,417 - INFO - Begin pretrain-pipeline, task=trajectory_embedding, model_name=LinearETA, dataset_name=bj, exp_id=166657
2024-01-01 13:36:15,417 - INFO - {'task': 'trajectory_embedding', 'model': 'LinearETA', 'dataset': 'bj', 'saved_model': True, 'train': True, 'gpu': True, 'gpu_id': 0, 'pretrain_path': 'libcity/cache/247850/model_cache/247850_BERTContrastiveLM_bj.pt', 'n_layers': 6, 'd_model': 256, 'attn_heads': 8, 'max_epoch': 30, 'batch_size': 8, 'grad_accmu_steps': 1, 'learning_rate': 0.0002, 'roadnetwork': 'bj_roadmap_edge_bj_True_1_merge', 'geo_file': 'bj_roadmap_edge_bj_True_1_merge_withdegree', 'rel_file': 'bj_roadmap_edge_bj_True_1_merge_withdegree', 'merge': True, 'min_freq': 1, 'seq_len': 128, 'test_every': 50, 'temperature': 0.05, 'contra_loss_type': 'simclr', 'classify_label': 'vflag', 'type_ln': 'post', 'add_cls': True, 'add_time_in_day': True, 'add_day_in_week': True, 'add_pe': True, 'add_temporal_bias': True, 'temporal_bias_dim': 64, 'use_mins_interval': False, 'add_gat': True, 'gat_heads_per_layer': [8, 16, 1], 'gat_features_per_layer': [16, 16, 256], 'gat_dropout': 0.1, 'gat_K': 1, 'gat_avg_last': True, 'load_trans_prob': True, 'append_degree2gcn': True, 'normal_feature': False, 'pooling': 'cls', 'dataset_class': 'ETADataset', 'executor': 'ETAExecutor', 'evaluator': 'RegressionEvaluator', 'num_workers': 0, 'vocab_path': None, 'mlp_ratio': 4, 'pretrain_road_emb': None, 'future_mask': False, 'load_node2vec': False, 'dropout': 0.1, 'drop_path': 0.3, 'attn_drop': 0.1, 'seed': 0, 'learner': 'adamw', 'lr_eta_min': 0, 'lr_warmup_epoch': 4, 'lr_warmup_init': 1e-06, 'lr_decay': True, 'lr_scheduler': 'cosinelr', 'lr_decay_ratio': 0.1, 't_in_epochs': True, 'clip_grad_norm': True, 'max_grad_norm': 5, 'use_early_stop': True, 'patience': 10, 'log_batch': 500, 'log_every': 1, 'l2_reg': None, 'bidir_adj_mx': False, 'freeze': False, 'metrics': ['MAE', 'RMSE', 'MAPE', 'R2', 'EVAR'], 'save_modes': ['csv', 'json'], 'device': device(type='cuda', index=0), 'exp_id': 166657}
2024-01-01 13:36:15,902 - INFO - Loading Vocab from raw_data/vocab_bj_True_1_merge.pkl
2024-01-01 13:36:15,902 - INFO - vocab_path=raw_data/vocab_bj_True_1_merge.pkl, usr_num=75, vocab_size=27313
2024-01-01 13:36:15,949 - INFO - Loaded file bj_roadmap_edge_bj_True_1_merge_withdegree.geo, num_nodes=27309
2024-01-01 13:36:17,310 - INFO - Loaded file bj_roadmap_edge_bj_True_1_merge_withdegree.rel, shape=(27309, 27309), edges=51196.0
2024-01-01 13:36:17,310 - INFO - node_features: (27309, 42)
2024-01-01 13:36:18,267 - INFO - node_features_encoded: torch.Size([27313, 42])
2024-01-01 13:36:18,407 - INFO - edge_index: torch.Size([2, 78471])
2024-01-01 13:36:18,407 - INFO - Trajectory loc-transfer prob shape=torch.Size([78471, 1])
2024-01-01 13:36:18,423 - INFO - Loading Dataset!
2024-01-01 13:36:19,143 - INFO - Load dataset from raw_data/bj/cache_bj_train_True_True_1.pkl
2024-01-01 13:36:19,378 - INFO - Load dataset from raw_data/bj/cache_bj_eval_True_True_1.pkl
2024-01-01 13:36:19,659 - INFO - Load dataset from raw_data/bj/cache_bj_test_True_True_1.pkl
2024-01-01 13:36:19,659 - INFO - Size of dataset: 10510/3504/3504
2024-01-01 13:36:19,659 - INFO - Creating Dataloader!
2024-01-01 13:36:19,659 - INFO - Building Downstream LinearETA model
2024-01-01 13:36:19,659 - INFO - Building BERTDownstream model
2024-01-01 13:36:20,270 - INFO - LinearETA(
  (model): BERTDownstream(
    (bert): BERT(
      (embedding): BERTEmbedding(
        (token_embedding): GAT(
          (gat_net): Sequential(
            (0): GATLayerImp3(
              (linear_proj): Linear(in_features=42, out_features=128, bias=False)
              (linear_proj_tran_prob): Linear(in_features=1, out_features=128, bias=False)
              (skip_proj): Linear(in_features=42, out_features=128, bias=False)
              (leakyReLU): LeakyReLU(negative_slope=0.2)
              (softmax): Softmax(dim=-1)
              (activation): ELU(alpha=1.0)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (1): GATLayerImp3(
              (linear_proj): Linear(in_features=128, out_features=256, bias=False)
              (linear_proj_tran_prob): Linear(in_features=1, out_features=256, bias=False)
              (skip_proj): Linear(in_features=128, out_features=256, bias=False)
              (leakyReLU): LeakyReLU(negative_slope=0.2)
              (softmax): Softmax(dim=-1)
              (activation): ELU(alpha=1.0)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (2): GATLayerImp3(
              (linear_proj): Linear(in_features=256, out_features=256, bias=False)
              (linear_proj_tran_prob): Linear(in_features=1, out_features=256, bias=False)
              (skip_proj): Linear(in_features=256, out_features=256, bias=False)
              (leakyReLU): LeakyReLU(negative_slope=0.2)
              (softmax): Softmax(dim=-1)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
        )
        (position_embedding): PositionalEmbedding()
        (daytime_embedding): Embedding(1441, 256, padding_idx=0)
        (weekday_embedding): Embedding(8, 256, padding_idx=0)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (transformer_blocks): ModuleList(
        (0): TransformerBlock(
          (attention): MultiHeadedAttention(
            (linear_layers): ModuleList(
              (0): Linear(in_features=256, out_features=256, bias=True)
              (1): Linear(in_features=256, out_features=256, bias=True)
              (2): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout): Dropout(p=0.1, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0.1, inplace=False)
            (temporal_mat_bias_1): Linear(in_features=1, out_features=64, bias=True)
            (temporal_mat_bias_2): Linear(in_features=64, out_features=1, bias=True)
          )
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0.1, inplace=False)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (drop_path): Identity()
        )
        (1): TransformerBlock(
          (attention): MultiHeadedAttention(
            (linear_layers): ModuleList(
              (0): Linear(in_features=256, out_features=256, bias=True)
              (1): Linear(in_features=256, out_features=256, bias=True)
              (2): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout): Dropout(p=0.1, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0.1, inplace=False)
            (temporal_mat_bias_1): Linear(in_features=1, out_features=64, bias=True)
            (temporal_mat_bias_2): Linear(in_features=64, out_features=1, bias=True)
          )
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0.1, inplace=False)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (drop_path): DropPath()
        )
        (2): TransformerBlock(
          (attention): MultiHeadedAttention(
            (linear_layers): ModuleList(
              (0): Linear(in_features=256, out_features=256, bias=True)
              (1): Linear(in_features=256, out_features=256, bias=True)
              (2): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout): Dropout(p=0.1, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0.1, inplace=False)
            (temporal_mat_bias_1): Linear(in_features=1, out_features=64, bias=True)
            (temporal_mat_bias_2): Linear(in_features=64, out_features=1, bias=True)
          )
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0.1, inplace=False)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (drop_path): DropPath()
        )
        (3): TransformerBlock(
          (attention): MultiHeadedAttention(
            (linear_layers): ModuleList(
              (0): Linear(in_features=256, out_features=256, bias=True)
              (1): Linear(in_features=256, out_features=256, bias=True)
              (2): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout): Dropout(p=0.1, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0.1, inplace=False)
            (temporal_mat_bias_1): Linear(in_features=1, out_features=64, bias=True)
            (temporal_mat_bias_2): Linear(in_features=64, out_features=1, bias=True)
          )
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0.1, inplace=False)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (drop_path): DropPath()
        )
        (4): TransformerBlock(
          (attention): MultiHeadedAttention(
            (linear_layers): ModuleList(
              (0): Linear(in_features=256, out_features=256, bias=True)
              (1): Linear(in_features=256, out_features=256, bias=True)
              (2): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout): Dropout(p=0.1, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0.1, inplace=False)
            (temporal_mat_bias_1): Linear(in_features=1, out_features=64, bias=True)
            (temporal_mat_bias_2): Linear(in_features=64, out_features=1, bias=True)
          )
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0.1, inplace=False)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (drop_path): DropPath()
        )
        (5): TransformerBlock(
          (attention): MultiHeadedAttention(
            (linear_layers): ModuleList(
              (0): Linear(in_features=256, out_features=256, bias=True)
              (1): Linear(in_features=256, out_features=256, bias=True)
              (2): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout): Dropout(p=0.1, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0.1, inplace=False)
            (temporal_mat_bias_1): Linear(in_features=1, out_features=64, bias=True)
            (temporal_mat_bias_2): Linear(in_features=64, out_features=1, bias=True)
          )
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0.1, inplace=False)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (drop_path): DropPath()
        )
      )
    )
  )
  (linear): Linear(in_features=256, out_features=1, bias=True)
)
2024-01-01 13:36:20,270 - INFO - model.bert.embedding.token_embedding.gat_net.0.scoring_fn_target	torch.Size([1, 8, 16])	cuda:0	True
2024-01-01 13:36:20,270 - INFO - model.bert.embedding.token_embedding.gat_net.0.scoring_fn_source	torch.Size([1, 8, 16])	cuda:0	True
2024-01-01 13:36:20,270 - INFO - model.bert.embedding.token_embedding.gat_net.0.scoring_trans_prob	torch.Size([1, 8, 16])	cuda:0	True
2024-01-01 13:36:20,270 - INFO - model.bert.embedding.token_embedding.gat_net.0.bias	torch.Size([128])	cuda:0	True
2024-01-01 13:36:20,270 - INFO - model.bert.embedding.token_embedding.gat_net.0.linear_proj.weight	torch.Size([128, 42])	cuda:0	True
2024-01-01 13:36:20,270 - INFO - model.bert.embedding.token_embedding.gat_net.0.linear_proj_tran_prob.weight	torch.Size([128, 1])	cuda:0	True
2024-01-01 13:36:20,270 - INFO - model.bert.embedding.token_embedding.gat_net.0.skip_proj.weight	torch.Size([128, 42])	cuda:0	True
2024-01-01 13:36:20,270 - INFO - model.bert.embedding.token_embedding.gat_net.1.scoring_fn_target	torch.Size([1, 16, 16])	cuda:0	True
2024-01-01 13:36:20,270 - INFO - model.bert.embedding.token_embedding.gat_net.1.scoring_fn_source	torch.Size([1, 16, 16])	cuda:0	True
2024-01-01 13:36:20,270 - INFO - model.bert.embedding.token_embedding.gat_net.1.scoring_trans_prob	torch.Size([1, 16, 16])	cuda:0	True
2024-01-01 13:36:20,270 - INFO - model.bert.embedding.token_embedding.gat_net.1.bias	torch.Size([256])	cuda:0	True
2024-01-01 13:36:20,270 - INFO - model.bert.embedding.token_embedding.gat_net.1.linear_proj.weight	torch.Size([256, 128])	cuda:0	True
2024-01-01 13:36:20,270 - INFO - model.bert.embedding.token_embedding.gat_net.1.linear_proj_tran_prob.weight	torch.Size([256, 1])	cuda:0	True
2024-01-01 13:36:20,270 - INFO - model.bert.embedding.token_embedding.gat_net.1.skip_proj.weight	torch.Size([256, 128])	cuda:0	True
2024-01-01 13:36:20,270 - INFO - model.bert.embedding.token_embedding.gat_net.2.scoring_fn_target	torch.Size([1, 1, 256])	cuda:0	True
2024-01-01 13:36:20,270 - INFO - model.bert.embedding.token_embedding.gat_net.2.scoring_fn_source	torch.Size([1, 1, 256])	cuda:0	True
2024-01-01 13:36:20,270 - INFO - model.bert.embedding.token_embedding.gat_net.2.scoring_trans_prob	torch.Size([1, 1, 256])	cuda:0	True
2024-01-01 13:36:20,270 - INFO - model.bert.embedding.token_embedding.gat_net.2.bias	torch.Size([256])	cuda:0	True
2024-01-01 13:36:20,270 - INFO - model.bert.embedding.token_embedding.gat_net.2.linear_proj.weight	torch.Size([256, 256])	cuda:0	True
2024-01-01 13:36:20,270 - INFO - model.bert.embedding.token_embedding.gat_net.2.linear_proj_tran_prob.weight	torch.Size([256, 1])	cuda:0	True
2024-01-01 13:36:20,270 - INFO - model.bert.embedding.token_embedding.gat_net.2.skip_proj.weight	torch.Size([256, 256])	cuda:0	True
2024-01-01 13:36:20,270 - INFO - model.bert.embedding.daytime_embedding.weight	torch.Size([1441, 256])	cuda:0	True
2024-01-01 13:36:20,285 - INFO - model.bert.embedding.weekday_embedding.weight	torch.Size([8, 256])	cuda:0	True
2024-01-01 13:36:20,285 - INFO - model.bert.transformer_blocks.0.attention.linear_layers.0.weight	torch.Size([256, 256])	cuda:0	True
2024-01-01 13:36:20,285 - INFO - model.bert.transformer_blocks.0.attention.linear_layers.0.bias	torch.Size([256])	cuda:0	True
2024-01-01 13:36:20,285 - INFO - model.bert.transformer_blocks.0.attention.linear_layers.1.weight	torch.Size([256, 256])	cuda:0	True
2024-01-01 13:36:20,285 - INFO - model.bert.transformer_blocks.0.attention.linear_layers.1.bias	torch.Size([256])	cuda:0	True
2024-01-01 13:36:20,285 - INFO - model.bert.transformer_blocks.0.attention.linear_layers.2.weight	torch.Size([256, 256])	cuda:0	True
2024-01-01 13:36:20,285 - INFO - model.bert.transformer_blocks.0.attention.linear_layers.2.bias	torch.Size([256])	cuda:0	True
2024-01-01 13:36:20,285 - INFO - model.bert.transformer_blocks.0.attention.proj.weight	torch.Size([256, 256])	cuda:0	True
2024-01-01 13:36:20,285 - INFO - model.bert.transformer_blocks.0.attention.proj.bias	torch.Size([256])	cuda:0	True
2024-01-01 13:36:20,285 - INFO - model.bert.transformer_blocks.0.attention.temporal_mat_bias_1.weight	torch.Size([64, 1])	cuda:0	True
2024-01-01 13:36:20,285 - INFO - model.bert.transformer_blocks.0.attention.temporal_mat_bias_1.bias	torch.Size([64])	cuda:0	True
2024-01-01 13:36:20,285 - INFO - model.bert.transformer_blocks.0.attention.temporal_mat_bias_2.weight	torch.Size([1, 64])	cuda:0	True
2024-01-01 13:36:20,285 - INFO - model.bert.transformer_blocks.0.attention.temporal_mat_bias_2.bias	torch.Size([1])	cuda:0	True
2024-01-01 13:36:20,285 - INFO - model.bert.transformer_blocks.0.mlp.fc1.weight	torch.Size([1024, 256])	cuda:0	True
2024-01-01 13:36:20,285 - INFO - model.bert.transformer_blocks.0.mlp.fc1.bias	torch.Size([1024])	cuda:0	True
2024-01-01 13:36:20,285 - INFO - model.bert.transformer_blocks.0.mlp.fc2.weight	torch.Size([256, 1024])	cuda:0	True
2024-01-01 13:36:20,285 - INFO - model.bert.transformer_blocks.0.mlp.fc2.bias	torch.Size([256])	cuda:0	True
2024-01-01 13:36:20,285 - INFO - model.bert.transformer_blocks.0.norm1.weight	torch.Size([256])	cuda:0	True
2024-01-01 13:36:20,285 - INFO - model.bert.transformer_blocks.0.norm1.bias	torch.Size([256])	cuda:0	True
2024-01-01 13:36:20,285 - INFO - model.bert.transformer_blocks.0.norm2.weight	torch.Size([256])	cuda:0	True
2024-01-01 13:36:20,285 - INFO - model.bert.transformer_blocks.0.norm2.bias	torch.Size([256])	cuda:0	True
2024-01-01 13:36:20,285 - INFO - model.bert.transformer_blocks.1.attention.linear_layers.0.weight	torch.Size([256, 256])	cuda:0	True
2024-01-01 13:36:20,285 - INFO - model.bert.transformer_blocks.1.attention.linear_layers.0.bias	torch.Size([256])	cuda:0	True
2024-01-01 13:36:20,285 - INFO - model.bert.transformer_blocks.1.attention.linear_layers.1.weight	torch.Size([256, 256])	cuda:0	True
2024-01-01 13:36:20,285 - INFO - model.bert.transformer_blocks.1.attention.linear_layers.1.bias	torch.Size([256])	cuda:0	True
2024-01-01 13:36:20,285 - INFO - model.bert.transformer_blocks.1.attention.linear_layers.2.weight	torch.Size([256, 256])	cuda:0	True
2024-01-01 13:36:20,285 - INFO - model.bert.transformer_blocks.1.attention.linear_layers.2.bias	torch.Size([256])	cuda:0	True
2024-01-01 13:36:20,285 - INFO - model.bert.transformer_blocks.1.attention.proj.weight	torch.Size([256, 256])	cuda:0	True
2024-01-01 13:36:20,285 - INFO - model.bert.transformer_blocks.1.attention.proj.bias	torch.Size([256])	cuda:0	True
2024-01-01 13:36:20,285 - INFO - model.bert.transformer_blocks.1.attention.temporal_mat_bias_1.weight	torch.Size([64, 1])	cuda:0	True
2024-01-01 13:36:20,285 - INFO - model.bert.transformer_blocks.1.attention.temporal_mat_bias_1.bias	torch.Size([64])	cuda:0	True
2024-01-01 13:36:20,285 - INFO - model.bert.transformer_blocks.1.attention.temporal_mat_bias_2.weight	torch.Size([1, 64])	cuda:0	True
2024-01-01 13:36:20,285 - INFO - model.bert.transformer_blocks.1.attention.temporal_mat_bias_2.bias	torch.Size([1])	cuda:0	True
2024-01-01 13:36:20,285 - INFO - model.bert.transformer_blocks.1.mlp.fc1.weight	torch.Size([1024, 256])	cuda:0	True
2024-01-01 13:36:20,285 - INFO - model.bert.transformer_blocks.1.mlp.fc1.bias	torch.Size([1024])	cuda:0	True
2024-01-01 13:36:20,285 - INFO - model.bert.transformer_blocks.1.mlp.fc2.weight	torch.Size([256, 1024])	cuda:0	True
2024-01-01 13:36:20,285 - INFO - model.bert.transformer_blocks.1.mlp.fc2.bias	torch.Size([256])	cuda:0	True
2024-01-01 13:36:20,285 - INFO - model.bert.transformer_blocks.1.norm1.weight	torch.Size([256])	cuda:0	True
2024-01-01 13:36:20,285 - INFO - model.bert.transformer_blocks.1.norm1.bias	torch.Size([256])	cuda:0	True
2024-01-01 13:36:20,285 - INFO - model.bert.transformer_blocks.1.norm2.weight	torch.Size([256])	cuda:0	True
2024-01-01 13:36:20,285 - INFO - model.bert.transformer_blocks.1.norm2.bias	torch.Size([256])	cuda:0	True
2024-01-01 13:36:20,285 - INFO - model.bert.transformer_blocks.2.attention.linear_layers.0.weight	torch.Size([256, 256])	cuda:0	True
2024-01-01 13:36:20,285 - INFO - model.bert.transformer_blocks.2.attention.linear_layers.0.bias	torch.Size([256])	cuda:0	True
2024-01-01 13:36:20,285 - INFO - model.bert.transformer_blocks.2.attention.linear_layers.1.weight	torch.Size([256, 256])	cuda:0	True
2024-01-01 13:36:20,285 - INFO - model.bert.transformer_blocks.2.attention.linear_layers.1.bias	torch.Size([256])	cuda:0	True
2024-01-01 13:36:20,285 - INFO - model.bert.transformer_blocks.2.attention.linear_layers.2.weight	torch.Size([256, 256])	cuda:0	True
2024-01-01 13:36:20,285 - INFO - model.bert.transformer_blocks.2.attention.linear_layers.2.bias	torch.Size([256])	cuda:0	True
2024-01-01 13:36:20,285 - INFO - model.bert.transformer_blocks.2.attention.proj.weight	torch.Size([256, 256])	cuda:0	True
2024-01-01 13:36:20,285 - INFO - model.bert.transformer_blocks.2.attention.proj.bias	torch.Size([256])	cuda:0	True
2024-01-01 13:36:20,285 - INFO - model.bert.transformer_blocks.2.attention.temporal_mat_bias_1.weight	torch.Size([64, 1])	cuda:0	True
2024-01-01 13:36:20,285 - INFO - model.bert.transformer_blocks.2.attention.temporal_mat_bias_1.bias	torch.Size([64])	cuda:0	True
2024-01-01 13:36:20,285 - INFO - model.bert.transformer_blocks.2.attention.temporal_mat_bias_2.weight	torch.Size([1, 64])	cuda:0	True
2024-01-01 13:36:20,285 - INFO - model.bert.transformer_blocks.2.attention.temporal_mat_bias_2.bias	torch.Size([1])	cuda:0	True
2024-01-01 13:36:20,285 - INFO - model.bert.transformer_blocks.2.mlp.fc1.weight	torch.Size([1024, 256])	cuda:0	True
2024-01-01 13:36:20,285 - INFO - model.bert.transformer_blocks.2.mlp.fc1.bias	torch.Size([1024])	cuda:0	True
2024-01-01 13:36:20,285 - INFO - model.bert.transformer_blocks.2.mlp.fc2.weight	torch.Size([256, 1024])	cuda:0	True
2024-01-01 13:36:20,285 - INFO - model.bert.transformer_blocks.2.mlp.fc2.bias	torch.Size([256])	cuda:0	True
2024-01-01 13:36:20,285 - INFO - model.bert.transformer_blocks.2.norm1.weight	torch.Size([256])	cuda:0	True
2024-01-01 13:36:20,285 - INFO - model.bert.transformer_blocks.2.norm1.bias	torch.Size([256])	cuda:0	True
2024-01-01 13:36:20,285 - INFO - model.bert.transformer_blocks.2.norm2.weight	torch.Size([256])	cuda:0	True
2024-01-01 13:36:20,285 - INFO - model.bert.transformer_blocks.2.norm2.bias	torch.Size([256])	cuda:0	True
2024-01-01 13:36:20,285 - INFO - model.bert.transformer_blocks.3.attention.linear_layers.0.weight	torch.Size([256, 256])	cuda:0	True
2024-01-01 13:36:20,285 - INFO - model.bert.transformer_blocks.3.attention.linear_layers.0.bias	torch.Size([256])	cuda:0	True
2024-01-01 13:36:20,285 - INFO - model.bert.transformer_blocks.3.attention.linear_layers.1.weight	torch.Size([256, 256])	cuda:0	True
2024-01-01 13:36:20,285 - INFO - model.bert.transformer_blocks.3.attention.linear_layers.1.bias	torch.Size([256])	cuda:0	True
2024-01-01 13:36:20,285 - INFO - model.bert.transformer_blocks.3.attention.linear_layers.2.weight	torch.Size([256, 256])	cuda:0	True
2024-01-01 13:36:20,285 - INFO - model.bert.transformer_blocks.3.attention.linear_layers.2.bias	torch.Size([256])	cuda:0	True
2024-01-01 13:36:20,285 - INFO - model.bert.transformer_blocks.3.attention.proj.weight	torch.Size([256, 256])	cuda:0	True
2024-01-01 13:36:20,285 - INFO - model.bert.transformer_blocks.3.attention.proj.bias	torch.Size([256])	cuda:0	True
2024-01-01 13:36:20,285 - INFO - model.bert.transformer_blocks.3.attention.temporal_mat_bias_1.weight	torch.Size([64, 1])	cuda:0	True
2024-01-01 13:36:20,285 - INFO - model.bert.transformer_blocks.3.attention.temporal_mat_bias_1.bias	torch.Size([64])	cuda:0	True
2024-01-01 13:36:20,285 - INFO - model.bert.transformer_blocks.3.attention.temporal_mat_bias_2.weight	torch.Size([1, 64])	cuda:0	True
2024-01-01 13:36:20,285 - INFO - model.bert.transformer_blocks.3.attention.temporal_mat_bias_2.bias	torch.Size([1])	cuda:0	True
2024-01-01 13:36:20,285 - INFO - model.bert.transformer_blocks.3.mlp.fc1.weight	torch.Size([1024, 256])	cuda:0	True
2024-01-01 13:36:20,285 - INFO - model.bert.transformer_blocks.3.mlp.fc1.bias	torch.Size([1024])	cuda:0	True
2024-01-01 13:36:20,285 - INFO - model.bert.transformer_blocks.3.mlp.fc2.weight	torch.Size([256, 1024])	cuda:0	True
2024-01-01 13:36:20,285 - INFO - model.bert.transformer_blocks.3.mlp.fc2.bias	torch.Size([256])	cuda:0	True
2024-01-01 13:36:20,285 - INFO - model.bert.transformer_blocks.3.norm1.weight	torch.Size([256])	cuda:0	True
2024-01-01 13:36:20,285 - INFO - model.bert.transformer_blocks.3.norm1.bias	torch.Size([256])	cuda:0	True
2024-01-01 13:36:20,285 - INFO - model.bert.transformer_blocks.3.norm2.weight	torch.Size([256])	cuda:0	True
2024-01-01 13:36:20,285 - INFO - model.bert.transformer_blocks.3.norm2.bias	torch.Size([256])	cuda:0	True
2024-01-01 13:36:20,285 - INFO - model.bert.transformer_blocks.4.attention.linear_layers.0.weight	torch.Size([256, 256])	cuda:0	True
2024-01-01 13:36:20,285 - INFO - model.bert.transformer_blocks.4.attention.linear_layers.0.bias	torch.Size([256])	cuda:0	True
2024-01-01 13:36:20,285 - INFO - model.bert.transformer_blocks.4.attention.linear_layers.1.weight	torch.Size([256, 256])	cuda:0	True
2024-01-01 13:36:20,285 - INFO - model.bert.transformer_blocks.4.attention.linear_layers.1.bias	torch.Size([256])	cuda:0	True
2024-01-01 13:36:20,285 - INFO - model.bert.transformer_blocks.4.attention.linear_layers.2.weight	torch.Size([256, 256])	cuda:0	True
2024-01-01 13:36:20,285 - INFO - model.bert.transformer_blocks.4.attention.linear_layers.2.bias	torch.Size([256])	cuda:0	True
2024-01-01 13:36:20,285 - INFO - model.bert.transformer_blocks.4.attention.proj.weight	torch.Size([256, 256])	cuda:0	True
2024-01-01 13:36:20,285 - INFO - model.bert.transformer_blocks.4.attention.proj.bias	torch.Size([256])	cuda:0	True
2024-01-01 13:36:20,285 - INFO - model.bert.transformer_blocks.4.attention.temporal_mat_bias_1.weight	torch.Size([64, 1])	cuda:0	True
2024-01-01 13:36:20,285 - INFO - model.bert.transformer_blocks.4.attention.temporal_mat_bias_1.bias	torch.Size([64])	cuda:0	True
2024-01-01 13:36:20,285 - INFO - model.bert.transformer_blocks.4.attention.temporal_mat_bias_2.weight	torch.Size([1, 64])	cuda:0	True
2024-01-01 13:36:20,285 - INFO - model.bert.transformer_blocks.4.attention.temporal_mat_bias_2.bias	torch.Size([1])	cuda:0	True
2024-01-01 13:36:20,285 - INFO - model.bert.transformer_blocks.4.mlp.fc1.weight	torch.Size([1024, 256])	cuda:0	True
2024-01-01 13:36:20,285 - INFO - model.bert.transformer_blocks.4.mlp.fc1.bias	torch.Size([1024])	cuda:0	True
2024-01-01 13:36:20,285 - INFO - model.bert.transformer_blocks.4.mlp.fc2.weight	torch.Size([256, 1024])	cuda:0	True
2024-01-01 13:36:20,285 - INFO - model.bert.transformer_blocks.4.mlp.fc2.bias	torch.Size([256])	cuda:0	True
2024-01-01 13:36:20,285 - INFO - model.bert.transformer_blocks.4.norm1.weight	torch.Size([256])	cuda:0	True
2024-01-01 13:36:20,285 - INFO - model.bert.transformer_blocks.4.norm1.bias	torch.Size([256])	cuda:0	True
2024-01-01 13:36:20,285 - INFO - model.bert.transformer_blocks.4.norm2.weight	torch.Size([256])	cuda:0	True
2024-01-01 13:36:20,285 - INFO - model.bert.transformer_blocks.4.norm2.bias	torch.Size([256])	cuda:0	True
2024-01-01 13:36:20,285 - INFO - model.bert.transformer_blocks.5.attention.linear_layers.0.weight	torch.Size([256, 256])	cuda:0	True
2024-01-01 13:36:20,285 - INFO - model.bert.transformer_blocks.5.attention.linear_layers.0.bias	torch.Size([256])	cuda:0	True
2024-01-01 13:36:20,285 - INFO - model.bert.transformer_blocks.5.attention.linear_layers.1.weight	torch.Size([256, 256])	cuda:0	True
2024-01-01 13:36:20,285 - INFO - model.bert.transformer_blocks.5.attention.linear_layers.1.bias	torch.Size([256])	cuda:0	True
2024-01-01 13:36:20,285 - INFO - model.bert.transformer_blocks.5.attention.linear_layers.2.weight	torch.Size([256, 256])	cuda:0	True
2024-01-01 13:36:20,285 - INFO - model.bert.transformer_blocks.5.attention.linear_layers.2.bias	torch.Size([256])	cuda:0	True
2024-01-01 13:36:20,285 - INFO - model.bert.transformer_blocks.5.attention.proj.weight	torch.Size([256, 256])	cuda:0	True
2024-01-01 13:36:20,285 - INFO - model.bert.transformer_blocks.5.attention.proj.bias	torch.Size([256])	cuda:0	True
2024-01-01 13:36:20,285 - INFO - model.bert.transformer_blocks.5.attention.temporal_mat_bias_1.weight	torch.Size([64, 1])	cuda:0	True
2024-01-01 13:36:20,285 - INFO - model.bert.transformer_blocks.5.attention.temporal_mat_bias_1.bias	torch.Size([64])	cuda:0	True
2024-01-01 13:36:20,285 - INFO - model.bert.transformer_blocks.5.attention.temporal_mat_bias_2.weight	torch.Size([1, 64])	cuda:0	True
2024-01-01 13:36:20,285 - INFO - model.bert.transformer_blocks.5.attention.temporal_mat_bias_2.bias	torch.Size([1])	cuda:0	True
2024-01-01 13:36:20,285 - INFO - model.bert.transformer_blocks.5.mlp.fc1.weight	torch.Size([1024, 256])	cuda:0	True
2024-01-01 13:36:20,285 - INFO - model.bert.transformer_blocks.5.mlp.fc1.bias	torch.Size([1024])	cuda:0	True
2024-01-01 13:36:20,285 - INFO - model.bert.transformer_blocks.5.mlp.fc2.weight	torch.Size([256, 1024])	cuda:0	True
2024-01-01 13:36:20,285 - INFO - model.bert.transformer_blocks.5.mlp.fc2.bias	torch.Size([256])	cuda:0	True
2024-01-01 13:36:20,285 - INFO - model.bert.transformer_blocks.5.norm1.weight	torch.Size([256])	cuda:0	True
2024-01-01 13:36:20,285 - INFO - model.bert.transformer_blocks.5.norm1.bias	torch.Size([256])	cuda:0	True
2024-01-01 13:36:20,285 - INFO - model.bert.transformer_blocks.5.norm2.weight	torch.Size([256])	cuda:0	True
2024-01-01 13:36:20,285 - INFO - model.bert.transformer_blocks.5.norm2.bias	torch.Size([256])	cuda:0	True
2024-01-01 13:36:20,285 - INFO - linear.weight	torch.Size([1, 256])	cuda:0	True
2024-01-01 13:36:20,285 - INFO - linear.bias	torch.Size([1])	cuda:0	True
2024-01-01 13:36:20,285 - INFO - Total parameter numbers: 5321479
2024-01-01 13:36:20,285 - INFO - You select `adamw` optimizer.
2024-01-01 13:36:20,301 - INFO - You select `cosinelr` lr_scheduler.
2024-01-01 13:36:20,442 - INFO - Load Pretrained-Model from libcity/cache/247850/model_cache/247850_BERTContrastiveLM_bj.pt
2024-01-01 13:36:20,457 - INFO - Start training ...
2024-01-01 13:36:20,457 - INFO - Num_batches: train=1314, eval=438
2024-01-01 13:36:21,602 - INFO - {'mode': 'Train', 'epoch': 0, 'iter': 0, 'lr': 1e-06, 'loss': 0.5656102895736694}
2024-01-01 13:37:10,052 - INFO - {'mode': 'Train', 'epoch': 0, 'iter': 500, 'lr': 1e-06, 'loss': 1.2197003364562988}
2024-01-01 13:38:01,600 - INFO - {'mode': 'Train', 'epoch': 0, 'iter': 1000, 'lr': 1e-06, 'loss': 0.3270953595638275}
2024-01-01 13:38:32,811 - INFO - Train: expid = 166657, Epoch = 0, avg_loss = 0.806900206083462.
2024-01-01 13:38:32,811 - INFO - epoch complete!
2024-01-01 13:38:32,811 - INFO - evaluating now!
2024-01-01 13:38:32,841 - INFO - {'mode': 'Eval', 'epoch': 0, 'iter': 0, 'lr': 1e-06, 'loss': 0.036218803375959396}
2024-01-01 13:38:45,112 - INFO - Eval: expid = 166657, Epoch = 0, avg_loss = 0.36332326452566743.
2024-01-01 13:38:45,112 - INFO - Epoch [0/30] (1314)  train_loss: 0.8069, val_loss: 0.3633, lr: 0.000051, 144.65s
2024-01-01 13:38:45,302 - INFO - Saved model at 0
2024-01-01 13:38:45,302 - INFO - Val loss decrease from inf to 0.3633, saving to ./libcity/cache/166657/model_cache/LinearETA_bj_epoch0.tar
2024-01-01 13:38:45,402 - INFO - {'mode': 'Train', 'epoch': 1, 'iter': 0, 'lr': 5.075e-05, 'loss': 0.18075710535049438}
2024-01-01 13:39:36,122 - INFO - {'mode': 'Train', 'epoch': 1, 'iter': 500, 'lr': 5.075e-05, 'loss': 0.0467287115752697}
2024-01-01 13:40:28,397 - INFO - {'mode': 'Train', 'epoch': 1, 'iter': 1000, 'lr': 5.075e-05, 'loss': 0.022157110273838043}
2024-01-01 13:41:01,928 - INFO - Train: expid = 166657, Epoch = 1, avg_loss = 0.2593448554060212.
2024-01-01 13:41:01,928 - INFO - epoch complete!
2024-01-01 13:41:01,928 - INFO - evaluating now!
2024-01-01 13:41:01,956 - INFO - {'mode': 'Eval', 'epoch': 1, 'iter': 0, 'lr': 5.075e-05, 'loss': 0.009153486229479313}
2024-01-01 13:41:15,096 - INFO - Eval: expid = 166657, Epoch = 1, avg_loss = 0.09521943481652849.
2024-01-01 13:41:15,096 - INFO - Epoch [1/30] (2628)  train_loss: 0.2593, val_loss: 0.0952, lr: 0.000101, 149.79s
2024-01-01 13:41:15,272 - INFO - Saved model at 1
2024-01-01 13:41:15,272 - INFO - Val loss decrease from 0.3633 to 0.0952, saving to ./libcity/cache/166657/model_cache/LinearETA_bj_epoch1.tar
2024-01-01 13:41:15,368 - INFO - {'mode': 'Train', 'epoch': 2, 'iter': 0, 'lr': 0.0001005, 'loss': 0.0056264810264110565}
2024-01-01 13:42:05,943 - INFO - {'mode': 'Train', 'epoch': 2, 'iter': 500, 'lr': 0.0001005, 'loss': 0.02519647218286991}
2024-01-01 13:42:58,454 - INFO - {'mode': 'Train', 'epoch': 2, 'iter': 1000, 'lr': 0.0001005, 'loss': 0.03958548232913017}
2024-01-01 13:43:31,616 - INFO - Train: expid = 166657, Epoch = 2, avg_loss = 0.15701442494151935.
2024-01-01 13:43:31,616 - INFO - epoch complete!
2024-01-01 13:43:31,616 - INFO - evaluating now!
2024-01-01 13:43:31,646 - INFO - {'mode': 'Eval', 'epoch': 2, 'iter': 0, 'lr': 0.0001005, 'loss': 0.00592005392536521}
2024-01-01 13:43:44,961 - INFO - Eval: expid = 166657, Epoch = 2, avg_loss = 0.1806815015186649.
2024-01-01 13:43:44,961 - INFO - Epoch [2/30] (3942)  train_loss: 0.1570, val_loss: 0.1807, lr: 0.000150, 149.69s
2024-01-01 13:43:45,070 - INFO - {'mode': 'Train', 'epoch': 3, 'iter': 0, 'lr': 0.00015025000000000002, 'loss': 0.014902686700224876}
2024-01-01 13:44:36,159 - INFO - {'mode': 'Train', 'epoch': 3, 'iter': 500, 'lr': 0.00015025000000000002, 'loss': 0.06278904527425766}
2024-01-01 13:45:27,344 - INFO - {'mode': 'Train', 'epoch': 3, 'iter': 1000, 'lr': 0.00015025000000000002, 'loss': 0.02078108862042427}
2024-01-01 13:45:59,280 - INFO - Train: expid = 166657, Epoch = 3, avg_loss = 0.19269693231668242.
2024-01-01 13:45:59,280 - INFO - epoch complete!
2024-01-01 13:45:59,290 - INFO - evaluating now!
2024-01-01 13:45:59,320 - INFO - {'mode': 'Eval', 'epoch': 3, 'iter': 0, 'lr': 0.00015025000000000002, 'loss': 0.015275400131940842}
2024-01-01 13:46:12,401 - INFO - Eval: expid = 166657, Epoch = 3, avg_loss = 0.0830526062825346.
2024-01-01 13:46:12,401 - INFO - Epoch [3/30] (5256)  train_loss: 0.1927, val_loss: 0.0831, lr: 0.000191, 147.44s
2024-01-01 13:46:12,582 - INFO - Saved model at 3
2024-01-01 13:46:12,582 - INFO - Val loss decrease from 0.0952 to 0.0831, saving to ./libcity/cache/166657/model_cache/LinearETA_bj_epoch3.tar
2024-01-01 13:46:12,682 - INFO - {'mode': 'Train', 'epoch': 4, 'iter': 0, 'lr': 0.0001913545457642601, 'loss': 0.021114222705364227}
2024-01-01 13:47:03,968 - INFO - {'mode': 'Train', 'epoch': 4, 'iter': 500, 'lr': 0.0001913545457642601, 'loss': 0.00437374971807003}
2024-01-01 13:47:54,739 - INFO - {'mode': 'Train', 'epoch': 4, 'iter': 1000, 'lr': 0.0001913545457642601, 'loss': 0.015451114624738693}
2024-01-01 13:48:26,724 - INFO - Train: expid = 166657, Epoch = 4, avg_loss = 0.184892488245123.
2024-01-01 13:48:26,724 - INFO - epoch complete!
2024-01-01 13:48:26,724 - INFO - evaluating now!
2024-01-01 13:48:26,754 - INFO - {'mode': 'Eval', 'epoch': 4, 'iter': 0, 'lr': 0.0001913545457642601, 'loss': 0.053485214710235596}
2024-01-01 13:48:40,034 - INFO - Eval: expid = 166657, Epoch = 4, avg_loss = 0.1478712751473498.
2024-01-01 13:48:40,034 - INFO - Epoch [4/30] (6570)  train_loss: 0.1849, val_loss: 0.1479, lr: 0.000187, 147.45s
2024-01-01 13:48:40,134 - INFO - {'mode': 'Train', 'epoch': 5, 'iter': 0, 'lr': 0.00018660254037844388, 'loss': 0.0172729529440403}
2024-01-01 13:49:30,629 - INFO - {'mode': 'Train', 'epoch': 5, 'iter': 500, 'lr': 0.00018660254037844388, 'loss': 0.017889462411403656}
2024-01-01 13:50:21,615 - INFO - {'mode': 'Train', 'epoch': 5, 'iter': 1000, 'lr': 0.00018660254037844388, 'loss': 0.009246948175132275}
2024-01-01 13:50:55,201 - INFO - Train: expid = 166657, Epoch = 5, avg_loss = 0.31536021041776896.
2024-01-01 13:50:55,201 - INFO - epoch complete!
2024-01-01 13:50:55,201 - INFO - evaluating now!
2024-01-01 13:50:55,227 - INFO - {'mode': 'Eval', 'epoch': 5, 'iter': 0, 'lr': 0.00018660254037844388, 'loss': 0.01123591884970665}
2024-01-01 13:51:08,101 - INFO - Eval: expid = 166657, Epoch = 5, avg_loss = 0.23682623819889795.
2024-01-01 13:51:08,101 - INFO - Epoch [5/30] (7884)  train_loss: 0.3154, val_loss: 0.2368, lr: 0.000181, 148.07s
2024-01-01 13:51:08,231 - INFO - {'mode': 'Train', 'epoch': 6, 'iter': 0, 'lr': 0.00018090169943749476, 'loss': 0.06353572756052017}
2024-01-01 13:51:59,573 - INFO - {'mode': 'Train', 'epoch': 6, 'iter': 500, 'lr': 0.00018090169943749476, 'loss': 0.011646755039691925}
2024-01-01 13:52:53,184 - INFO - {'mode': 'Train', 'epoch': 6, 'iter': 1000, 'lr': 0.00018090169943749476, 'loss': 0.010583896189928055}
2024-01-01 13:53:26,517 - INFO - Train: expid = 166657, Epoch = 6, avg_loss = 0.3291439032968095.
2024-01-01 13:53:26,518 - INFO - epoch complete!
2024-01-01 13:53:26,518 - INFO - evaluating now!
2024-01-01 13:53:26,553 - INFO - {'mode': 'Eval', 'epoch': 6, 'iter': 0, 'lr': 0.00018090169943749476, 'loss': 0.009263619780540466}
2024-01-01 13:53:40,828 - INFO - Eval: expid = 166657, Epoch = 6, avg_loss = 0.2657885868043839.
2024-01-01 13:53:40,828 - INFO - Epoch [6/30] (9198)  train_loss: 0.3291, val_loss: 0.2658, lr: 0.000174, 152.73s
2024-01-01 13:53:40,936 - INFO - {'mode': 'Train', 'epoch': 7, 'iter': 0, 'lr': 0.00017431448254773944, 'loss': 0.022949952632188797}
2024-01-01 13:54:33,284 - INFO - {'mode': 'Train', 'epoch': 7, 'iter': 500, 'lr': 0.00017431448254773944, 'loss': 0.004549669101834297}
2024-01-01 13:55:25,477 - INFO - {'mode': 'Train', 'epoch': 7, 'iter': 1000, 'lr': 0.00017431448254773944, 'loss': 0.010434133931994438}
2024-01-01 13:55:58,507 - INFO - Train: expid = 166657, Epoch = 7, avg_loss = 0.3259568909041364.
2024-01-01 13:55:58,507 - INFO - epoch complete!
2024-01-01 13:55:58,507 - INFO - evaluating now!
2024-01-01 13:55:58,537 - INFO - {'mode': 'Eval', 'epoch': 7, 'iter': 0, 'lr': 0.00017431448254773944, 'loss': 0.002763977739959955}
2024-01-01 13:56:11,879 - INFO - Eval: expid = 166657, Epoch = 7, avg_loss = 0.24775955106382105.
2024-01-01 13:56:11,879 - INFO - Epoch [7/30] (10512)  train_loss: 0.3260, val_loss: 0.2478, lr: 0.000167, 151.05s
2024-01-01 13:56:11,989 - INFO - {'mode': 'Train', 'epoch': 8, 'iter': 0, 'lr': 0.00016691306063588583, 'loss': 0.0197363942861557}
2024-01-01 13:57:03,681 - INFO - {'mode': 'Train', 'epoch': 8, 'iter': 500, 'lr': 0.00016691306063588583, 'loss': 0.002932865172624588}
2024-01-01 13:57:57,460 - INFO - {'mode': 'Train', 'epoch': 8, 'iter': 1000, 'lr': 0.00016691306063588583, 'loss': 0.018757004290819168}
2024-01-01 13:58:30,702 - INFO - Train: expid = 166657, Epoch = 8, avg_loss = 0.2565672517492286.
2024-01-01 13:58:30,702 - INFO - epoch complete!
2024-01-01 13:58:30,702 - INFO - evaluating now!
2024-01-01 13:58:30,732 - INFO - {'mode': 'Eval', 'epoch': 8, 'iter': 0, 'lr': 0.00016691306063588583, 'loss': 0.01284318882972002}
2024-01-01 13:58:44,725 - INFO - Eval: expid = 166657, Epoch = 8, avg_loss = 0.1239663261949982.
2024-01-01 13:58:44,725 - INFO - Epoch [8/30] (11826)  train_loss: 0.2566, val_loss: 0.1240, lr: 0.000159, 152.85s
2024-01-01 13:58:44,852 - INFO - {'mode': 'Train', 'epoch': 9, 'iter': 0, 'lr': 0.00015877852522924732, 'loss': 0.01755804941058159}
2024-01-01 13:59:38,498 - INFO - {'mode': 'Train', 'epoch': 9, 'iter': 500, 'lr': 0.00015877852522924732, 'loss': 0.004412960261106491}
2024-01-01 14:00:32,436 - INFO - {'mode': 'Train', 'epoch': 9, 'iter': 1000, 'lr': 0.00015877852522924732, 'loss': 0.008824709802865982}
2024-01-01 14:01:07,084 - INFO - Train: expid = 166657, Epoch = 9, avg_loss = 0.2219319910800827.
2024-01-01 14:01:07,084 - INFO - epoch complete!
2024-01-01 14:01:07,085 - INFO - evaluating now!
2024-01-01 14:01:07,128 - INFO - {'mode': 'Eval', 'epoch': 9, 'iter': 0, 'lr': 0.00015877852522924732, 'loss': 0.01921047270298004}
2024-01-01 14:01:21,456 - INFO - Eval: expid = 166657, Epoch = 9, avg_loss = 0.11770693393922241.
2024-01-01 14:01:21,464 - INFO - Epoch [9/30] (13140)  train_loss: 0.2219, val_loss: 0.1177, lr: 0.000150, 156.74s
2024-01-01 14:01:21,593 - INFO - {'mode': 'Train', 'epoch': 10, 'iter': 0, 'lr': 0.00015000000000000001, 'loss': 0.03330915421247482}
2024-01-01 14:02:15,624 - INFO - {'mode': 'Train', 'epoch': 10, 'iter': 500, 'lr': 0.00015000000000000001, 'loss': 2.639594793319702}
2024-01-01 14:03:10,102 - INFO - {'mode': 'Train', 'epoch': 10, 'iter': 1000, 'lr': 0.00015000000000000001, 'loss': 0.020429832860827446}
2024-01-01 14:03:46,114 - INFO - Train: expid = 166657, Epoch = 10, avg_loss = 0.22056739291745095.
2024-01-01 14:03:46,114 - INFO - epoch complete!
2024-01-01 14:03:46,115 - INFO - evaluating now!
2024-01-01 14:03:46,145 - INFO - {'mode': 'Eval', 'epoch': 10, 'iter': 0, 'lr': 0.00015000000000000001, 'loss': 0.011029163375496864}
2024-01-01 14:04:00,293 - INFO - Eval: expid = 166657, Epoch = 10, avg_loss = 0.15064305223718064.
2024-01-01 14:04:00,293 - INFO - Epoch [10/30] (14454)  train_loss: 0.2206, val_loss: 0.1506, lr: 0.000141, 158.83s
2024-01-01 14:04:00,405 - INFO - {'mode': 'Train', 'epoch': 11, 'iter': 0, 'lr': 0.00014067366430758004, 'loss': 0.0124746672809124}
2024-01-01 14:04:54,104 - INFO - {'mode': 'Train', 'epoch': 11, 'iter': 500, 'lr': 0.00014067366430758004, 'loss': 0.015926269814372063}
2024-01-01 14:05:48,671 - INFO - {'mode': 'Train', 'epoch': 11, 'iter': 1000, 'lr': 0.00014067366430758004, 'loss': 0.01689307764172554}
2024-01-01 14:06:22,869 - INFO - Train: expid = 166657, Epoch = 11, avg_loss = 0.25009356236819014.
2024-01-01 14:06:22,869 - INFO - epoch complete!
2024-01-01 14:06:22,869 - INFO - evaluating now!
2024-01-01 14:06:22,916 - INFO - {'mode': 'Eval', 'epoch': 11, 'iter': 0, 'lr': 0.00014067366430758004, 'loss': 0.9881330728530884}
2024-01-01 14:06:37,635 - INFO - Eval: expid = 166657, Epoch = 11, avg_loss = 0.16271328091776618.
2024-01-01 14:06:37,635 - INFO - Epoch [11/30] (15768)  train_loss: 0.2501, val_loss: 0.1627, lr: 0.000131, 157.34s
2024-01-01 14:06:37,737 - INFO - {'mode': 'Train', 'epoch': 12, 'iter': 0, 'lr': 0.00013090169943749476, 'loss': 0.04791751876473427}
2024-01-01 14:07:31,936 - INFO - {'mode': 'Train', 'epoch': 12, 'iter': 500, 'lr': 0.00013090169943749476, 'loss': 0.005670378915965557}
2024-01-01 14:08:27,186 - INFO - {'mode': 'Train', 'epoch': 12, 'iter': 1000, 'lr': 0.00013090169943749476, 'loss': 0.004595748148858547}
2024-01-01 14:09:01,698 - INFO - Train: expid = 166657, Epoch = 12, avg_loss = 0.22001191076550536.
2024-01-01 14:09:01,699 - INFO - epoch complete!
2024-01-01 14:09:01,699 - INFO - evaluating now!
2024-01-01 14:09:01,732 - INFO - {'mode': 'Eval', 'epoch': 12, 'iter': 0, 'lr': 0.00013090169943749476, 'loss': 3.291393995285034}
2024-01-01 14:09:15,625 - INFO - Eval: expid = 166657, Epoch = 12, avg_loss = 0.17627954803812254.
2024-01-01 14:09:15,635 - INFO - Epoch [12/30] (17082)  train_loss: 0.2200, val_loss: 0.1763, lr: 0.000121, 158.00s
2024-01-01 14:09:15,735 - INFO - {'mode': 'Train', 'epoch': 13, 'iter': 0, 'lr': 0.00012079116908177593, 'loss': 0.003082095645368099}
2024-01-01 14:10:08,344 - INFO - {'mode': 'Train', 'epoch': 13, 'iter': 500, 'lr': 0.00012079116908177593, 'loss': 0.03644261881709099}
2024-01-01 14:11:03,653 - INFO - {'mode': 'Train', 'epoch': 13, 'iter': 1000, 'lr': 0.00012079116908177593, 'loss': 0.007186576724052429}
2024-01-01 14:11:38,172 - INFO - Train: expid = 166657, Epoch = 13, avg_loss = 0.21639332662697183.
2024-01-01 14:11:38,172 - INFO - epoch complete!
2024-01-01 14:11:38,172 - INFO - evaluating now!
2024-01-01 14:11:38,217 - INFO - {'mode': 'Eval', 'epoch': 13, 'iter': 0, 'lr': 0.00012079116908177593, 'loss': 0.006876930128782988}
2024-01-01 14:11:53,274 - INFO - Eval: expid = 166657, Epoch = 13, avg_loss = 0.0936309754147862.
2024-01-01 14:11:53,274 - INFO - Epoch [13/30] (18396)  train_loss: 0.2164, val_loss: 0.0936, lr: 0.000110, 157.64s
2024-01-01 14:11:53,275 - WARNING - Early stopping at epoch: 13
2024-01-01 14:11:53,275 - INFO - Trained totally 14 epochs, average train time is 138.576s, average eval time is 13.729s
2024-01-01 14:11:53,468 - INFO - Loaded model at 3
2024-01-01 14:11:53,603 - INFO - Save png at ./libcity/cache/166657/166657_loss.png
2024-01-01 14:11:53,675 - INFO - Save png at ./libcity/cache/166657/166657_lr.png
2024-01-01 14:11:53,832 - INFO - Saved model at ./libcity/cache/166657/model_cache/166657_LinearETA_bj.pt
2024-01-01 14:11:53,832 - INFO - Start evaluating ...
2024-01-01 14:11:53,880 - INFO - {'mode': 'Test', 'epoch': 0, 'iter': 0, 'lr': 0.0001913545457642601, 'loss': 0.049249522387981415}
2024-01-01 14:12:10,197 - INFO - Test: expid = 166657, Epoch = 0, avg_loss = 0.1488210269841089.
2024-01-01 14:12:10,203 - INFO - Evaluate result is {"MAE": 0.14401910929477105, "RMSE": 0.21627951224576936, "MAPE": 0.7127987568577131, "R2": -1.0195608700963963, "EVAR": 0.1841692471340911}
2024-01-01 14:12:10,203 - INFO - Evaluate result is saved at ./libcity/cache/166657/evaluate_cache\166657_2024_01_01_14_12_10_LinearETA_bj.json
2024-01-01 14:12:10,203 - INFO - 
{
 "MAE": 0.14401910929477105,
 "RMSE": 0.21627951224576936,
 "MAPE": 0.7127987568577131,
 "R2": -1.0195608700963963,
 "EVAR": 0.1841692471340911
}
2024-01-01 14:12:10,248 - INFO - Evaluate result is saved at ./libcity/cache/166657/evaluate_cache\166657_2024_01_01_14_12_10_LinearETA_bj.csv
2024-01-01 14:12:10,266 - INFO - 
        MAE     RMSE      MAPE        R2      EVAR
1  0.144019  0.21628  0.712799 -1.019561  0.184169
2024-01-01 14:12:10,266 - INFO - Test time 16.433923959732056s.
