2023-12-31 01:13:45,922 - INFO - Log directory: ./libcity/log
2023-12-31 01:13:45,923 - INFO - Begin pretrain-pipeline, task=trajectory_embedding, model_name=LinearETA, dataset_name=bj, exp_id=945775
2023-12-31 01:13:45,923 - INFO - {'task': 'trajectory_embedding', 'model': 'LinearETA', 'dataset': 'bj', 'saved_model': True, 'train': True, 'gpu': True, 'gpu_id': 0, 'distribution': 'geometric', 'avg_mask_len': 2, 'contra_ratio': 0.4, 'mlm_ratio': 0.6, 'split': True, 'config': 'bj', 'dataset_class': 'ETADataset', 'executor': 'ETAExecutor', 'evaluator': 'RegressionEvaluator', 'num_workers': 0, 'vocab_path': None, 'min_freq': 1, 'd_model': 768, 'mlp_ratio': 4, 'pretrain_road_emb': None, 'n_layers': 12, 'attn_heads': 12, 'seq_len': 128, 'future_mask': False, 'load_node2vec': False, 'dropout': 0.1, 'drop_path': 0.3, 'attn_drop': 0.1, 'type_ln': 'post', 'pooling': 'cls', 'add_cls': True, 'add_time_in_day': True, 'add_day_in_week': True, 'add_pe': True, 'add_temporal_bias': True, 'temporal_bias_dim': 64, 'use_mins_interval': False, 'add_gat': True, 'gat_K': 3, 'gat_avg_last': True, 'load_trans_prob': True, 'gat_heads_per_layer': [8, 1], 'gat_features_per_layer': [16, 256], 'gat_dropout': 0.6, 'append_degree2gcn': True, 'normal_feature': True, 'seed': 0, 'batch_size': 32, 'grad_accmu_steps': 32, 'max_epoch': 20, 'learner': 'adamw', 'learning_rate': 0.0002, 'lr_eta_min': 0, 'lr_warmup_epoch': 4, 'lr_warmup_init': 1e-06, 'lr_decay': True, 'lr_scheduler': 'cosinelr', 'lr_decay_ratio': 0.1, 't_in_epochs': True, 'clip_grad_norm': True, 'max_grad_norm': 5, 'use_early_stop': True, 'patience': 10, 'test_every': 10, 'log_batch': 500, 'log_every': 1, 'l2_reg': None, 'merge': True, 'bidir_adj_mx': False, 'pretrain_path': None, 'freeze': False, 'metrics': ['MAE', 'RMSE', 'MAPE', 'R2', 'EVAR'], 'save_modes': ['csv', 'json'], 'device': device(type='cuda', index=0), 'exp_id': 945775}
2023-12-31 01:13:46,699 - INFO - Loading Vocab from raw_data/vocab_bj_True_1_merge.pkl
2023-12-31 01:13:46,699 - INFO - vocab_path=raw_data/vocab_bj_True_1_merge.pkl, usr_num=6, vocab_size=11573
