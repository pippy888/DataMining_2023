2024-01-02 08:33:59,981 - INFO - Log directory: ./libcity/log
2024-01-02 08:33:59,981 - INFO - Begin pretrain-pipeline, task=trajectory_embedding, model_name=LinearETA, dataset_name=bj, exp_id=816594
2024-01-02 08:33:59,981 - INFO - {'task': 'trajectory_embedding', 'model': 'LinearETA', 'dataset': 'bj', 'saved_model': True, 'train': True, 'gpu': True, 'gpu_id': 0, 'pretrain_path': 'libcity/cache/454824/model_cache/454824_BERTContrastiveLM_bj.pt', 'n_layers': 6, 'd_model': 256, 'attn_heads': 8, 'max_epoch': 30, 'batch_size': 8, 'grad_accmu_steps': 1, 'learning_rate': 0.0002, 'roadnetwork': 'bj_roadmap_edge_bj_True_1_merge', 'geo_file': 'bj_roadmap_edge_bj_True_1_merge_withdegree', 'rel_file': 'bj_roadmap_edge_bj_True_1_merge_withdegree', 'merge': True, 'min_freq': 1, 'seq_len': 128, 'test_every': 50, 'temperature': 0.05, 'contra_loss_type': 'simclr', 'classify_label': 'vflag', 'type_ln': 'post', 'add_cls': True, 'add_time_in_day': True, 'add_day_in_week': True, 'add_pe': True, 'add_temporal_bias': True, 'temporal_bias_dim': 64, 'use_mins_interval': False, 'add_gat': True, 'gat_heads_per_layer': [8, 16, 1], 'gat_features_per_layer': [16, 16, 256], 'gat_dropout': 0.1, 'gat_K': 1, 'gat_avg_last': True, 'load_trans_prob': True, 'append_degree2gcn': True, 'normal_feature': False, 'pooling': 'cls', 'dataset_class': 'ETADataset', 'executor': 'ETAExecutor', 'evaluator': 'RegressionEvaluator', 'num_workers': 0, 'vocab_path': None, 'mlp_ratio': 4, 'pretrain_road_emb': None, 'future_mask': False, 'load_node2vec': False, 'dropout': 0.1, 'drop_path': 0.3, 'attn_drop': 0.1, 'seed': 0, 'learner': 'adamw', 'lr_eta_min': 0, 'lr_warmup_epoch': 4, 'lr_warmup_init': 1e-06, 'lr_decay': True, 'lr_scheduler': 'cosinelr', 'lr_decay_ratio': 0.1, 't_in_epochs': True, 'clip_grad_norm': True, 'max_grad_norm': 5, 'use_early_stop': True, 'patience': 10, 'log_batch': 500, 'log_every': 1, 'l2_reg': None, 'bidir_adj_mx': False, 'freeze': False, 'metrics': ['MAE', 'RMSE', 'MAPE', 'R2', 'EVAR'], 'save_modes': ['csv', 'json'], 'device': device(type='cuda', index=0), 'exp_id': 816594}
2024-01-02 08:34:00,491 - INFO - Loading Vocab from raw_data/vocab_bj_True_1_merge.pkl
2024-01-02 08:34:00,506 - INFO - vocab_path=raw_data/vocab_bj_True_1_merge.pkl, usr_num=75, vocab_size=27313
2024-01-02 08:34:00,553 - INFO - Loaded file bj_roadmap_edge_bj_True_1_merge_withdegree.geo, num_nodes=27309
2024-01-02 08:34:01,876 - INFO - Loaded file bj_roadmap_edge_bj_True_1_merge_withdegree.rel, shape=(27309, 27309), edges=51196.0
2024-01-02 08:34:01,876 - INFO - node_features: (27309, 42)
2024-01-02 08:34:02,840 - INFO - node_features_encoded: torch.Size([27313, 42])
2024-01-02 08:34:02,980 - INFO - edge_index: torch.Size([2, 78471])
2024-01-02 08:34:02,996 - INFO - Trajectory loc-transfer prob shape=torch.Size([78471, 1])
2024-01-02 08:34:02,996 - INFO - Loading Dataset!
2024-01-02 08:34:03,740 - INFO - Load dataset from raw_data/bj/cache_bj_train_True_True_1.pkl
2024-01-02 08:34:03,959 - INFO - Load dataset from raw_data/bj/cache_bj_eval_True_True_1.pkl
2024-01-02 08:34:04,204 - INFO - Load dataset from raw_data/bj/cache_bj_test_True_True_1.pkl
2024-01-02 08:34:04,204 - INFO - Size of dataset: 10510/3504/3504
2024-01-02 08:34:04,204 - INFO - Creating Dataloader!
2024-01-02 08:34:04,204 - INFO - Building Downstream LinearETA model
2024-01-02 08:34:04,204 - INFO - Building BERTDownstream model
2024-01-02 08:34:04,829 - INFO - LinearETA(
  (model): BERTDownstream(
    (bert): BERT(
      (embedding): BERTEmbedding(
        (token_embedding): GAT(
          (gat_net): Sequential(
            (0): GATLayerImp3(
              (linear_proj): Linear(in_features=42, out_features=128, bias=False)
              (linear_proj_tran_prob): Linear(in_features=1, out_features=128, bias=False)
              (skip_proj): Linear(in_features=42, out_features=128, bias=False)
              (leakyReLU): LeakyReLU(negative_slope=0.2)
              (softmax): Softmax(dim=-1)
              (activation): ELU(alpha=1.0)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (1): GATLayerImp3(
              (linear_proj): Linear(in_features=128, out_features=256, bias=False)
              (linear_proj_tran_prob): Linear(in_features=1, out_features=256, bias=False)
              (skip_proj): Linear(in_features=128, out_features=256, bias=False)
              (leakyReLU): LeakyReLU(negative_slope=0.2)
              (softmax): Softmax(dim=-1)
              (activation): ELU(alpha=1.0)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (2): GATLayerImp3(
              (linear_proj): Linear(in_features=256, out_features=256, bias=False)
              (linear_proj_tran_prob): Linear(in_features=1, out_features=256, bias=False)
              (skip_proj): Linear(in_features=256, out_features=256, bias=False)
              (leakyReLU): LeakyReLU(negative_slope=0.2)
              (softmax): Softmax(dim=-1)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
        )
        (position_embedding): PositionalEmbedding()
        (daytime_embedding): Embedding(1441, 256, padding_idx=0)
        (weekday_embedding): Embedding(8, 256, padding_idx=0)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (transformer_blocks): ModuleList(
        (0): TransformerBlock(
          (attention): MultiHeadedAttention(
            (linear_layers): ModuleList(
              (0): Linear(in_features=256, out_features=256, bias=True)
              (1): Linear(in_features=256, out_features=256, bias=True)
              (2): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout): Dropout(p=0.1, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0.1, inplace=False)
            (temporal_mat_bias_1): Linear(in_features=1, out_features=64, bias=True)
            (temporal_mat_bias_2): Linear(in_features=64, out_features=1, bias=True)
          )
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0.1, inplace=False)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (drop_path): Identity()
        )
        (1): TransformerBlock(
          (attention): MultiHeadedAttention(
            (linear_layers): ModuleList(
              (0): Linear(in_features=256, out_features=256, bias=True)
              (1): Linear(in_features=256, out_features=256, bias=True)
              (2): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout): Dropout(p=0.1, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0.1, inplace=False)
            (temporal_mat_bias_1): Linear(in_features=1, out_features=64, bias=True)
            (temporal_mat_bias_2): Linear(in_features=64, out_features=1, bias=True)
          )
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0.1, inplace=False)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (drop_path): DropPath()
        )
        (2): TransformerBlock(
          (attention): MultiHeadedAttention(
            (linear_layers): ModuleList(
              (0): Linear(in_features=256, out_features=256, bias=True)
              (1): Linear(in_features=256, out_features=256, bias=True)
              (2): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout): Dropout(p=0.1, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0.1, inplace=False)
            (temporal_mat_bias_1): Linear(in_features=1, out_features=64, bias=True)
            (temporal_mat_bias_2): Linear(in_features=64, out_features=1, bias=True)
          )
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0.1, inplace=False)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (drop_path): DropPath()
        )
        (3): TransformerBlock(
          (attention): MultiHeadedAttention(
            (linear_layers): ModuleList(
              (0): Linear(in_features=256, out_features=256, bias=True)
              (1): Linear(in_features=256, out_features=256, bias=True)
              (2): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout): Dropout(p=0.1, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0.1, inplace=False)
            (temporal_mat_bias_1): Linear(in_features=1, out_features=64, bias=True)
            (temporal_mat_bias_2): Linear(in_features=64, out_features=1, bias=True)
          )
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0.1, inplace=False)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (drop_path): DropPath()
        )
        (4): TransformerBlock(
          (attention): MultiHeadedAttention(
            (linear_layers): ModuleList(
              (0): Linear(in_features=256, out_features=256, bias=True)
              (1): Linear(in_features=256, out_features=256, bias=True)
              (2): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout): Dropout(p=0.1, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0.1, inplace=False)
            (temporal_mat_bias_1): Linear(in_features=1, out_features=64, bias=True)
            (temporal_mat_bias_2): Linear(in_features=64, out_features=1, bias=True)
          )
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0.1, inplace=False)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (drop_path): DropPath()
        )
        (5): TransformerBlock(
          (attention): MultiHeadedAttention(
            (linear_layers): ModuleList(
              (0): Linear(in_features=256, out_features=256, bias=True)
              (1): Linear(in_features=256, out_features=256, bias=True)
              (2): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout): Dropout(p=0.1, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0.1, inplace=False)
            (temporal_mat_bias_1): Linear(in_features=1, out_features=64, bias=True)
            (temporal_mat_bias_2): Linear(in_features=64, out_features=1, bias=True)
          )
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0.1, inplace=False)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (drop_path): DropPath()
        )
      )
    )
  )
  (linear): Linear(in_features=256, out_features=1, bias=True)
)
2024-01-02 08:34:04,829 - INFO - model.bert.embedding.token_embedding.gat_net.0.scoring_fn_target	torch.Size([1, 8, 16])	cuda:0	True
2024-01-02 08:34:04,829 - INFO - model.bert.embedding.token_embedding.gat_net.0.scoring_fn_source	torch.Size([1, 8, 16])	cuda:0	True
2024-01-02 08:34:04,829 - INFO - model.bert.embedding.token_embedding.gat_net.0.scoring_trans_prob	torch.Size([1, 8, 16])	cuda:0	True
2024-01-02 08:34:04,829 - INFO - model.bert.embedding.token_embedding.gat_net.0.bias	torch.Size([128])	cuda:0	True
2024-01-02 08:34:04,829 - INFO - model.bert.embedding.token_embedding.gat_net.0.linear_proj.weight	torch.Size([128, 42])	cuda:0	True
2024-01-02 08:34:04,829 - INFO - model.bert.embedding.token_embedding.gat_net.0.linear_proj_tran_prob.weight	torch.Size([128, 1])	cuda:0	True
2024-01-02 08:34:04,829 - INFO - model.bert.embedding.token_embedding.gat_net.0.skip_proj.weight	torch.Size([128, 42])	cuda:0	True
2024-01-02 08:34:04,829 - INFO - model.bert.embedding.token_embedding.gat_net.1.scoring_fn_target	torch.Size([1, 16, 16])	cuda:0	True
2024-01-02 08:34:04,829 - INFO - model.bert.embedding.token_embedding.gat_net.1.scoring_fn_source	torch.Size([1, 16, 16])	cuda:0	True
2024-01-02 08:34:04,829 - INFO - model.bert.embedding.token_embedding.gat_net.1.scoring_trans_prob	torch.Size([1, 16, 16])	cuda:0	True
2024-01-02 08:34:04,829 - INFO - model.bert.embedding.token_embedding.gat_net.1.bias	torch.Size([256])	cuda:0	True
2024-01-02 08:34:04,829 - INFO - model.bert.embedding.token_embedding.gat_net.1.linear_proj.weight	torch.Size([256, 128])	cuda:0	True
2024-01-02 08:34:04,844 - INFO - model.bert.embedding.token_embedding.gat_net.1.linear_proj_tran_prob.weight	torch.Size([256, 1])	cuda:0	True
2024-01-02 08:34:04,844 - INFO - model.bert.embedding.token_embedding.gat_net.1.skip_proj.weight	torch.Size([256, 128])	cuda:0	True
2024-01-02 08:34:04,844 - INFO - model.bert.embedding.token_embedding.gat_net.2.scoring_fn_target	torch.Size([1, 1, 256])	cuda:0	True
2024-01-02 08:34:04,844 - INFO - model.bert.embedding.token_embedding.gat_net.2.scoring_fn_source	torch.Size([1, 1, 256])	cuda:0	True
2024-01-02 08:34:04,844 - INFO - model.bert.embedding.token_embedding.gat_net.2.scoring_trans_prob	torch.Size([1, 1, 256])	cuda:0	True
2024-01-02 08:34:04,844 - INFO - model.bert.embedding.token_embedding.gat_net.2.bias	torch.Size([256])	cuda:0	True
2024-01-02 08:34:04,844 - INFO - model.bert.embedding.token_embedding.gat_net.2.linear_proj.weight	torch.Size([256, 256])	cuda:0	True
2024-01-02 08:34:04,844 - INFO - model.bert.embedding.token_embedding.gat_net.2.linear_proj_tran_prob.weight	torch.Size([256, 1])	cuda:0	True
2024-01-02 08:34:04,844 - INFO - model.bert.embedding.token_embedding.gat_net.2.skip_proj.weight	torch.Size([256, 256])	cuda:0	True
2024-01-02 08:34:04,844 - INFO - model.bert.embedding.daytime_embedding.weight	torch.Size([1441, 256])	cuda:0	True
2024-01-02 08:34:04,844 - INFO - model.bert.embedding.weekday_embedding.weight	torch.Size([8, 256])	cuda:0	True
2024-01-02 08:34:04,844 - INFO - model.bert.transformer_blocks.0.attention.linear_layers.0.weight	torch.Size([256, 256])	cuda:0	True
2024-01-02 08:34:04,844 - INFO - model.bert.transformer_blocks.0.attention.linear_layers.0.bias	torch.Size([256])	cuda:0	True
2024-01-02 08:34:04,844 - INFO - model.bert.transformer_blocks.0.attention.linear_layers.1.weight	torch.Size([256, 256])	cuda:0	True
2024-01-02 08:34:04,844 - INFO - model.bert.transformer_blocks.0.attention.linear_layers.1.bias	torch.Size([256])	cuda:0	True
2024-01-02 08:34:04,844 - INFO - model.bert.transformer_blocks.0.attention.linear_layers.2.weight	torch.Size([256, 256])	cuda:0	True
2024-01-02 08:34:04,844 - INFO - model.bert.transformer_blocks.0.attention.linear_layers.2.bias	torch.Size([256])	cuda:0	True
2024-01-02 08:34:04,844 - INFO - model.bert.transformer_blocks.0.attention.proj.weight	torch.Size([256, 256])	cuda:0	True
2024-01-02 08:34:04,844 - INFO - model.bert.transformer_blocks.0.attention.proj.bias	torch.Size([256])	cuda:0	True
2024-01-02 08:34:04,844 - INFO - model.bert.transformer_blocks.0.attention.temporal_mat_bias_1.weight	torch.Size([64, 1])	cuda:0	True
2024-01-02 08:34:04,844 - INFO - model.bert.transformer_blocks.0.attention.temporal_mat_bias_1.bias	torch.Size([64])	cuda:0	True
2024-01-02 08:34:04,844 - INFO - model.bert.transformer_blocks.0.attention.temporal_mat_bias_2.weight	torch.Size([1, 64])	cuda:0	True
2024-01-02 08:34:04,844 - INFO - model.bert.transformer_blocks.0.attention.temporal_mat_bias_2.bias	torch.Size([1])	cuda:0	True
2024-01-02 08:34:04,844 - INFO - model.bert.transformer_blocks.0.mlp.fc1.weight	torch.Size([1024, 256])	cuda:0	True
2024-01-02 08:34:04,844 - INFO - model.bert.transformer_blocks.0.mlp.fc1.bias	torch.Size([1024])	cuda:0	True
2024-01-02 08:34:04,844 - INFO - model.bert.transformer_blocks.0.mlp.fc2.weight	torch.Size([256, 1024])	cuda:0	True
2024-01-02 08:34:04,844 - INFO - model.bert.transformer_blocks.0.mlp.fc2.bias	torch.Size([256])	cuda:0	True
2024-01-02 08:34:04,844 - INFO - model.bert.transformer_blocks.0.norm1.weight	torch.Size([256])	cuda:0	True
2024-01-02 08:34:04,844 - INFO - model.bert.transformer_blocks.0.norm1.bias	torch.Size([256])	cuda:0	True
2024-01-02 08:34:04,844 - INFO - model.bert.transformer_blocks.0.norm2.weight	torch.Size([256])	cuda:0	True
2024-01-02 08:34:04,844 - INFO - model.bert.transformer_blocks.0.norm2.bias	torch.Size([256])	cuda:0	True
2024-01-02 08:34:04,844 - INFO - model.bert.transformer_blocks.1.attention.linear_layers.0.weight	torch.Size([256, 256])	cuda:0	True
2024-01-02 08:34:04,844 - INFO - model.bert.transformer_blocks.1.attention.linear_layers.0.bias	torch.Size([256])	cuda:0	True
2024-01-02 08:34:04,844 - INFO - model.bert.transformer_blocks.1.attention.linear_layers.1.weight	torch.Size([256, 256])	cuda:0	True
2024-01-02 08:34:04,844 - INFO - model.bert.transformer_blocks.1.attention.linear_layers.1.bias	torch.Size([256])	cuda:0	True
2024-01-02 08:34:04,844 - INFO - model.bert.transformer_blocks.1.attention.linear_layers.2.weight	torch.Size([256, 256])	cuda:0	True
2024-01-02 08:34:04,844 - INFO - model.bert.transformer_blocks.1.attention.linear_layers.2.bias	torch.Size([256])	cuda:0	True
2024-01-02 08:34:04,844 - INFO - model.bert.transformer_blocks.1.attention.proj.weight	torch.Size([256, 256])	cuda:0	True
2024-01-02 08:34:04,844 - INFO - model.bert.transformer_blocks.1.attention.proj.bias	torch.Size([256])	cuda:0	True
2024-01-02 08:34:04,844 - INFO - model.bert.transformer_blocks.1.attention.temporal_mat_bias_1.weight	torch.Size([64, 1])	cuda:0	True
2024-01-02 08:34:04,844 - INFO - model.bert.transformer_blocks.1.attention.temporal_mat_bias_1.bias	torch.Size([64])	cuda:0	True
2024-01-02 08:34:04,844 - INFO - model.bert.transformer_blocks.1.attention.temporal_mat_bias_2.weight	torch.Size([1, 64])	cuda:0	True
2024-01-02 08:34:04,844 - INFO - model.bert.transformer_blocks.1.attention.temporal_mat_bias_2.bias	torch.Size([1])	cuda:0	True
2024-01-02 08:34:04,844 - INFO - model.bert.transformer_blocks.1.mlp.fc1.weight	torch.Size([1024, 256])	cuda:0	True
2024-01-02 08:34:04,844 - INFO - model.bert.transformer_blocks.1.mlp.fc1.bias	torch.Size([1024])	cuda:0	True
2024-01-02 08:34:04,844 - INFO - model.bert.transformer_blocks.1.mlp.fc2.weight	torch.Size([256, 1024])	cuda:0	True
2024-01-02 08:34:04,844 - INFO - model.bert.transformer_blocks.1.mlp.fc2.bias	torch.Size([256])	cuda:0	True
2024-01-02 08:34:04,844 - INFO - model.bert.transformer_blocks.1.norm1.weight	torch.Size([256])	cuda:0	True
2024-01-02 08:34:04,844 - INFO - model.bert.transformer_blocks.1.norm1.bias	torch.Size([256])	cuda:0	True
2024-01-02 08:34:04,844 - INFO - model.bert.transformer_blocks.1.norm2.weight	torch.Size([256])	cuda:0	True
2024-01-02 08:34:04,844 - INFO - model.bert.transformer_blocks.1.norm2.bias	torch.Size([256])	cuda:0	True
2024-01-02 08:34:04,844 - INFO - model.bert.transformer_blocks.2.attention.linear_layers.0.weight	torch.Size([256, 256])	cuda:0	True
2024-01-02 08:34:04,844 - INFO - model.bert.transformer_blocks.2.attention.linear_layers.0.bias	torch.Size([256])	cuda:0	True
2024-01-02 08:34:04,844 - INFO - model.bert.transformer_blocks.2.attention.linear_layers.1.weight	torch.Size([256, 256])	cuda:0	True
2024-01-02 08:34:04,844 - INFO - model.bert.transformer_blocks.2.attention.linear_layers.1.bias	torch.Size([256])	cuda:0	True
2024-01-02 08:34:04,844 - INFO - model.bert.transformer_blocks.2.attention.linear_layers.2.weight	torch.Size([256, 256])	cuda:0	True
2024-01-02 08:34:04,844 - INFO - model.bert.transformer_blocks.2.attention.linear_layers.2.bias	torch.Size([256])	cuda:0	True
2024-01-02 08:34:04,844 - INFO - model.bert.transformer_blocks.2.attention.proj.weight	torch.Size([256, 256])	cuda:0	True
2024-01-02 08:34:04,844 - INFO - model.bert.transformer_blocks.2.attention.proj.bias	torch.Size([256])	cuda:0	True
2024-01-02 08:34:04,844 - INFO - model.bert.transformer_blocks.2.attention.temporal_mat_bias_1.weight	torch.Size([64, 1])	cuda:0	True
2024-01-02 08:34:04,844 - INFO - model.bert.transformer_blocks.2.attention.temporal_mat_bias_1.bias	torch.Size([64])	cuda:0	True
2024-01-02 08:34:04,844 - INFO - model.bert.transformer_blocks.2.attention.temporal_mat_bias_2.weight	torch.Size([1, 64])	cuda:0	True
2024-01-02 08:34:04,844 - INFO - model.bert.transformer_blocks.2.attention.temporal_mat_bias_2.bias	torch.Size([1])	cuda:0	True
2024-01-02 08:34:04,844 - INFO - model.bert.transformer_blocks.2.mlp.fc1.weight	torch.Size([1024, 256])	cuda:0	True
2024-01-02 08:34:04,844 - INFO - model.bert.transformer_blocks.2.mlp.fc1.bias	torch.Size([1024])	cuda:0	True
2024-01-02 08:34:04,844 - INFO - model.bert.transformer_blocks.2.mlp.fc2.weight	torch.Size([256, 1024])	cuda:0	True
2024-01-02 08:34:04,844 - INFO - model.bert.transformer_blocks.2.mlp.fc2.bias	torch.Size([256])	cuda:0	True
2024-01-02 08:34:04,844 - INFO - model.bert.transformer_blocks.2.norm1.weight	torch.Size([256])	cuda:0	True
2024-01-02 08:34:04,844 - INFO - model.bert.transformer_blocks.2.norm1.bias	torch.Size([256])	cuda:0	True
2024-01-02 08:34:04,844 - INFO - model.bert.transformer_blocks.2.norm2.weight	torch.Size([256])	cuda:0	True
2024-01-02 08:34:04,844 - INFO - model.bert.transformer_blocks.2.norm2.bias	torch.Size([256])	cuda:0	True
2024-01-02 08:34:04,844 - INFO - model.bert.transformer_blocks.3.attention.linear_layers.0.weight	torch.Size([256, 256])	cuda:0	True
2024-01-02 08:34:04,844 - INFO - model.bert.transformer_blocks.3.attention.linear_layers.0.bias	torch.Size([256])	cuda:0	True
2024-01-02 08:34:04,844 - INFO - model.bert.transformer_blocks.3.attention.linear_layers.1.weight	torch.Size([256, 256])	cuda:0	True
2024-01-02 08:34:04,844 - INFO - model.bert.transformer_blocks.3.attention.linear_layers.1.bias	torch.Size([256])	cuda:0	True
2024-01-02 08:34:04,844 - INFO - model.bert.transformer_blocks.3.attention.linear_layers.2.weight	torch.Size([256, 256])	cuda:0	True
2024-01-02 08:34:04,844 - INFO - model.bert.transformer_blocks.3.attention.linear_layers.2.bias	torch.Size([256])	cuda:0	True
2024-01-02 08:34:04,844 - INFO - model.bert.transformer_blocks.3.attention.proj.weight	torch.Size([256, 256])	cuda:0	True
2024-01-02 08:34:04,844 - INFO - model.bert.transformer_blocks.3.attention.proj.bias	torch.Size([256])	cuda:0	True
2024-01-02 08:34:04,844 - INFO - model.bert.transformer_blocks.3.attention.temporal_mat_bias_1.weight	torch.Size([64, 1])	cuda:0	True
2024-01-02 08:34:04,844 - INFO - model.bert.transformer_blocks.3.attention.temporal_mat_bias_1.bias	torch.Size([64])	cuda:0	True
2024-01-02 08:34:04,844 - INFO - model.bert.transformer_blocks.3.attention.temporal_mat_bias_2.weight	torch.Size([1, 64])	cuda:0	True
2024-01-02 08:34:04,844 - INFO - model.bert.transformer_blocks.3.attention.temporal_mat_bias_2.bias	torch.Size([1])	cuda:0	True
2024-01-02 08:34:04,844 - INFO - model.bert.transformer_blocks.3.mlp.fc1.weight	torch.Size([1024, 256])	cuda:0	True
2024-01-02 08:34:04,844 - INFO - model.bert.transformer_blocks.3.mlp.fc1.bias	torch.Size([1024])	cuda:0	True
2024-01-02 08:34:04,844 - INFO - model.bert.transformer_blocks.3.mlp.fc2.weight	torch.Size([256, 1024])	cuda:0	True
2024-01-02 08:34:04,844 - INFO - model.bert.transformer_blocks.3.mlp.fc2.bias	torch.Size([256])	cuda:0	True
2024-01-02 08:34:04,844 - INFO - model.bert.transformer_blocks.3.norm1.weight	torch.Size([256])	cuda:0	True
2024-01-02 08:34:04,844 - INFO - model.bert.transformer_blocks.3.norm1.bias	torch.Size([256])	cuda:0	True
2024-01-02 08:34:04,844 - INFO - model.bert.transformer_blocks.3.norm2.weight	torch.Size([256])	cuda:0	True
2024-01-02 08:34:04,844 - INFO - model.bert.transformer_blocks.3.norm2.bias	torch.Size([256])	cuda:0	True
2024-01-02 08:34:04,844 - INFO - model.bert.transformer_blocks.4.attention.linear_layers.0.weight	torch.Size([256, 256])	cuda:0	True
2024-01-02 08:34:04,844 - INFO - model.bert.transformer_blocks.4.attention.linear_layers.0.bias	torch.Size([256])	cuda:0	True
2024-01-02 08:34:04,844 - INFO - model.bert.transformer_blocks.4.attention.linear_layers.1.weight	torch.Size([256, 256])	cuda:0	True
2024-01-02 08:34:04,844 - INFO - model.bert.transformer_blocks.4.attention.linear_layers.1.bias	torch.Size([256])	cuda:0	True
2024-01-02 08:34:04,844 - INFO - model.bert.transformer_blocks.4.attention.linear_layers.2.weight	torch.Size([256, 256])	cuda:0	True
2024-01-02 08:34:04,844 - INFO - model.bert.transformer_blocks.4.attention.linear_layers.2.bias	torch.Size([256])	cuda:0	True
2024-01-02 08:34:04,844 - INFO - model.bert.transformer_blocks.4.attention.proj.weight	torch.Size([256, 256])	cuda:0	True
2024-01-02 08:34:04,844 - INFO - model.bert.transformer_blocks.4.attention.proj.bias	torch.Size([256])	cuda:0	True
2024-01-02 08:34:04,844 - INFO - model.bert.transformer_blocks.4.attention.temporal_mat_bias_1.weight	torch.Size([64, 1])	cuda:0	True
2024-01-02 08:34:04,844 - INFO - model.bert.transformer_blocks.4.attention.temporal_mat_bias_1.bias	torch.Size([64])	cuda:0	True
2024-01-02 08:34:04,844 - INFO - model.bert.transformer_blocks.4.attention.temporal_mat_bias_2.weight	torch.Size([1, 64])	cuda:0	True
2024-01-02 08:34:04,844 - INFO - model.bert.transformer_blocks.4.attention.temporal_mat_bias_2.bias	torch.Size([1])	cuda:0	True
2024-01-02 08:34:04,844 - INFO - model.bert.transformer_blocks.4.mlp.fc1.weight	torch.Size([1024, 256])	cuda:0	True
2024-01-02 08:34:04,844 - INFO - model.bert.transformer_blocks.4.mlp.fc1.bias	torch.Size([1024])	cuda:0	True
2024-01-02 08:34:04,844 - INFO - model.bert.transformer_blocks.4.mlp.fc2.weight	torch.Size([256, 1024])	cuda:0	True
2024-01-02 08:34:04,844 - INFO - model.bert.transformer_blocks.4.mlp.fc2.bias	torch.Size([256])	cuda:0	True
2024-01-02 08:34:04,844 - INFO - model.bert.transformer_blocks.4.norm1.weight	torch.Size([256])	cuda:0	True
2024-01-02 08:34:04,844 - INFO - model.bert.transformer_blocks.4.norm1.bias	torch.Size([256])	cuda:0	True
2024-01-02 08:34:04,844 - INFO - model.bert.transformer_blocks.4.norm2.weight	torch.Size([256])	cuda:0	True
2024-01-02 08:34:04,844 - INFO - model.bert.transformer_blocks.4.norm2.bias	torch.Size([256])	cuda:0	True
2024-01-02 08:34:04,844 - INFO - model.bert.transformer_blocks.5.attention.linear_layers.0.weight	torch.Size([256, 256])	cuda:0	True
2024-01-02 08:34:04,844 - INFO - model.bert.transformer_blocks.5.attention.linear_layers.0.bias	torch.Size([256])	cuda:0	True
2024-01-02 08:34:04,844 - INFO - model.bert.transformer_blocks.5.attention.linear_layers.1.weight	torch.Size([256, 256])	cuda:0	True
2024-01-02 08:34:04,844 - INFO - model.bert.transformer_blocks.5.attention.linear_layers.1.bias	torch.Size([256])	cuda:0	True
2024-01-02 08:34:04,844 - INFO - model.bert.transformer_blocks.5.attention.linear_layers.2.weight	torch.Size([256, 256])	cuda:0	True
2024-01-02 08:34:04,844 - INFO - model.bert.transformer_blocks.5.attention.linear_layers.2.bias	torch.Size([256])	cuda:0	True
2024-01-02 08:34:04,844 - INFO - model.bert.transformer_blocks.5.attention.proj.weight	torch.Size([256, 256])	cuda:0	True
2024-01-02 08:34:04,844 - INFO - model.bert.transformer_blocks.5.attention.proj.bias	torch.Size([256])	cuda:0	True
2024-01-02 08:34:04,844 - INFO - model.bert.transformer_blocks.5.attention.temporal_mat_bias_1.weight	torch.Size([64, 1])	cuda:0	True
2024-01-02 08:34:04,844 - INFO - model.bert.transformer_blocks.5.attention.temporal_mat_bias_1.bias	torch.Size([64])	cuda:0	True
2024-01-02 08:34:04,844 - INFO - model.bert.transformer_blocks.5.attention.temporal_mat_bias_2.weight	torch.Size([1, 64])	cuda:0	True
2024-01-02 08:34:04,844 - INFO - model.bert.transformer_blocks.5.attention.temporal_mat_bias_2.bias	torch.Size([1])	cuda:0	True
2024-01-02 08:34:04,844 - INFO - model.bert.transformer_blocks.5.mlp.fc1.weight	torch.Size([1024, 256])	cuda:0	True
2024-01-02 08:34:04,844 - INFO - model.bert.transformer_blocks.5.mlp.fc1.bias	torch.Size([1024])	cuda:0	True
2024-01-02 08:34:04,844 - INFO - model.bert.transformer_blocks.5.mlp.fc2.weight	torch.Size([256, 1024])	cuda:0	True
2024-01-02 08:34:04,844 - INFO - model.bert.transformer_blocks.5.mlp.fc2.bias	torch.Size([256])	cuda:0	True
2024-01-02 08:34:04,844 - INFO - model.bert.transformer_blocks.5.norm1.weight	torch.Size([256])	cuda:0	True
2024-01-02 08:34:04,844 - INFO - model.bert.transformer_blocks.5.norm1.bias	torch.Size([256])	cuda:0	True
2024-01-02 08:34:04,844 - INFO - model.bert.transformer_blocks.5.norm2.weight	torch.Size([256])	cuda:0	True
2024-01-02 08:34:04,844 - INFO - model.bert.transformer_blocks.5.norm2.bias	torch.Size([256])	cuda:0	True
2024-01-02 08:34:04,844 - INFO - linear.weight	torch.Size([1, 256])	cuda:0	True
2024-01-02 08:34:04,844 - INFO - linear.bias	torch.Size([1])	cuda:0	True
2024-01-02 08:34:04,860 - INFO - Total parameter numbers: 5321479
2024-01-02 08:34:04,860 - INFO - You select `adamw` optimizer.
2024-01-02 08:34:04,860 - INFO - You select `cosinelr` lr_scheduler.
2024-01-02 08:34:04,907 - INFO - Load Pretrained-Model from libcity/cache/454824/model_cache/454824_BERTContrastiveLM_bj.pt
2024-01-02 08:34:04,938 - INFO - Start training ...
2024-01-02 08:34:04,938 - INFO - Num_batches: train=1314, eval=438
2024-01-02 08:34:05,605 - INFO - {'mode': 'Train', 'epoch': 0, 'iter': 0, 'lr': 1e-06, 'loss': 334.3193664550781}
2024-01-02 08:34:55,727 - INFO - {'mode': 'Train', 'epoch': 0, 'iter': 500, 'lr': 1e-06, 'loss': 272.4094543457031}
2024-01-02 08:35:44,902 - INFO - {'mode': 'Train', 'epoch': 0, 'iter': 1000, 'lr': 1e-06, 'loss': 87.76042175292969}
2024-01-02 08:36:15,260 - INFO - Train: expid = 816594, Epoch = 0, avg_loss = 181.85965250053368.
2024-01-02 08:36:15,260 - INFO - epoch complete!
2024-01-02 08:36:15,260 - INFO - evaluating now!
2024-01-02 08:36:15,290 - INFO - {'mode': 'Eval', 'epoch': 0, 'iter': 0, 'lr': 1e-06, 'loss': 194.87771606445312}
2024-01-02 08:36:27,340 - INFO - Eval: expid = 816594, Epoch = 0, avg_loss = 85.72433093040502.
2024-01-02 08:36:27,340 - INFO - Epoch [0/30] (1314)  train_loss: 181.8597, val_loss: 85.7243, lr: 0.000051, 142.40s
2024-01-02 08:36:27,530 - INFO - Saved model at 0
2024-01-02 08:36:27,530 - INFO - Val loss decrease from inf to 85.7243, saving to ./libcity/cache/816594/model_cache/LinearETA_bj_epoch0.tar
2024-01-02 08:36:27,620 - INFO - {'mode': 'Train', 'epoch': 1, 'iter': 0, 'lr': 5.075e-05, 'loss': 8.809124946594238}
2024-01-02 08:37:15,656 - INFO - {'mode': 'Train', 'epoch': 1, 'iter': 500, 'lr': 5.075e-05, 'loss': 50.38575744628906}
2024-01-02 08:38:04,057 - INFO - {'mode': 'Train', 'epoch': 1, 'iter': 1000, 'lr': 5.075e-05, 'loss': 41.83109664916992}
2024-01-02 08:38:34,453 - INFO - Train: expid = 816594, Epoch = 1, avg_loss = 52.04452995877398.
2024-01-02 08:38:34,453 - INFO - epoch complete!
2024-01-02 08:38:34,453 - INFO - evaluating now!
2024-01-02 08:38:34,483 - INFO - {'mode': 'Eval', 'epoch': 1, 'iter': 0, 'lr': 5.075e-05, 'loss': 15.558624267578125}
2024-01-02 08:38:46,863 - INFO - Eval: expid = 816594, Epoch = 1, avg_loss = 48.35076540899059.
2024-01-02 08:38:46,863 - INFO - Epoch [1/30] (2628)  train_loss: 52.0445, val_loss: 48.3508, lr: 0.000101, 139.33s
2024-01-02 08:38:47,043 - INFO - Saved model at 1
2024-01-02 08:38:47,043 - INFO - Val loss decrease from 85.7243 to 48.3508, saving to ./libcity/cache/816594/model_cache/LinearETA_bj_epoch1.tar
2024-01-02 08:38:47,143 - INFO - {'mode': 'Train', 'epoch': 2, 'iter': 0, 'lr': 0.0001005, 'loss': 18.306190490722656}
2024-01-02 08:39:36,132 - INFO - {'mode': 'Train', 'epoch': 2, 'iter': 500, 'lr': 0.0001005, 'loss': 59.199867248535156}
2024-01-02 08:40:25,290 - INFO - {'mode': 'Train', 'epoch': 2, 'iter': 1000, 'lr': 0.0001005, 'loss': 77.38129425048828}
2024-01-02 08:40:56,101 - INFO - Train: expid = 816594, Epoch = 2, avg_loss = 48.39407611333337.
2024-01-02 08:40:56,101 - INFO - epoch complete!
2024-01-02 08:40:56,101 - INFO - evaluating now!
2024-01-02 08:40:56,131 - INFO - {'mode': 'Eval', 'epoch': 2, 'iter': 0, 'lr': 0.0001005, 'loss': 34.17478942871094}
2024-01-02 08:41:08,681 - INFO - Eval: expid = 816594, Epoch = 2, avg_loss = 48.351725228845254.
2024-01-02 08:41:08,681 - INFO - Epoch [2/30] (3942)  train_loss: 48.3941, val_loss: 48.3517, lr: 0.000150, 141.64s
2024-01-02 08:41:08,791 - INFO - {'mode': 'Train', 'epoch': 3, 'iter': 0, 'lr': 0.00015025000000000002, 'loss': 23.338481903076172}
2024-01-02 08:41:58,141 - INFO - {'mode': 'Train', 'epoch': 3, 'iter': 500, 'lr': 0.00015025000000000002, 'loss': 31.51567268371582}
2024-01-02 08:42:47,658 - INFO - {'mode': 'Train', 'epoch': 3, 'iter': 1000, 'lr': 0.00015025000000000002, 'loss': 26.98111915588379}
2024-01-02 08:43:18,758 - INFO - Train: expid = 816594, Epoch = 3, avg_loss = 48.681460150256825.
2024-01-02 08:43:18,758 - INFO - epoch complete!
2024-01-02 08:43:18,758 - INFO - evaluating now!
2024-01-02 08:43:18,788 - INFO - {'mode': 'Eval', 'epoch': 3, 'iter': 0, 'lr': 0.00015025000000000002, 'loss': 34.37278366088867}
2024-01-02 08:43:31,368 - INFO - Eval: expid = 816594, Epoch = 3, avg_loss = 49.18042431846601.
2024-01-02 08:43:31,368 - INFO - Epoch [3/30] (5256)  train_loss: 48.6815, val_loss: 49.1804, lr: 0.000191, 142.69s
2024-01-02 08:43:31,468 - INFO - {'mode': 'Train', 'epoch': 4, 'iter': 0, 'lr': 0.0001913545457642601, 'loss': 22.126373291015625}
2024-01-02 08:44:21,108 - INFO - {'mode': 'Train', 'epoch': 4, 'iter': 500, 'lr': 0.0001913545457642601, 'loss': 61.99954605102539}
2024-01-02 08:45:10,710 - INFO - {'mode': 'Train', 'epoch': 4, 'iter': 1000, 'lr': 0.0001913545457642601, 'loss': 24.936771392822266}
2024-01-02 08:45:41,830 - INFO - Train: expid = 816594, Epoch = 4, avg_loss = 48.56992467559031.
2024-01-02 08:45:41,830 - INFO - epoch complete!
2024-01-02 08:45:41,830 - INFO - evaluating now!
2024-01-02 08:45:41,860 - INFO - {'mode': 'Eval', 'epoch': 4, 'iter': 0, 'lr': 0.0001913545457642601, 'loss': 25.455230712890625}
2024-01-02 08:45:54,540 - INFO - Eval: expid = 816594, Epoch = 4, avg_loss = 53.06359230165612.
2024-01-02 08:45:54,540 - INFO - Epoch [4/30] (6570)  train_loss: 48.5699, val_loss: 53.0636, lr: 0.000187, 143.17s
2024-01-02 08:45:54,640 - INFO - {'mode': 'Train', 'epoch': 5, 'iter': 0, 'lr': 0.00018660254037844388, 'loss': 27.181346893310547}
2024-01-02 08:46:44,346 - INFO - {'mode': 'Train', 'epoch': 5, 'iter': 500, 'lr': 0.00018660254037844388, 'loss': 9.27893352508545}
2024-01-02 08:47:34,146 - INFO - {'mode': 'Train', 'epoch': 5, 'iter': 1000, 'lr': 0.00018660254037844388, 'loss': 27.746551513671875}
2024-01-02 08:48:05,446 - INFO - Train: expid = 816594, Epoch = 5, avg_loss = 47.50651129460358.
2024-01-02 08:48:05,446 - INFO - epoch complete!
2024-01-02 08:48:05,446 - INFO - evaluating now!
2024-01-02 08:48:05,476 - INFO - {'mode': 'Eval', 'epoch': 5, 'iter': 0, 'lr': 0.00018660254037844388, 'loss': 52.11653137207031}
2024-01-02 08:48:18,136 - INFO - Eval: expid = 816594, Epoch = 5, avg_loss = 51.23399044607328.
2024-01-02 08:48:18,136 - INFO - Epoch [5/30] (7884)  train_loss: 47.5065, val_loss: 51.2340, lr: 0.000181, 143.60s
2024-01-02 08:48:18,236 - INFO - {'mode': 'Train', 'epoch': 6, 'iter': 0, 'lr': 0.00018090169943749476, 'loss': 25.835567474365234}
2024-01-02 08:49:08,083 - INFO - {'mode': 'Train', 'epoch': 6, 'iter': 500, 'lr': 0.00018090169943749476, 'loss': 63.648590087890625}
2024-01-02 08:49:58,033 - INFO - {'mode': 'Train', 'epoch': 6, 'iter': 1000, 'lr': 0.00018090169943749476, 'loss': 54.47760009765625}
2024-01-02 08:50:29,283 - INFO - Train: expid = 816594, Epoch = 6, avg_loss = 47.66745009381469.
2024-01-02 08:50:29,283 - INFO - epoch complete!
2024-01-02 08:50:29,283 - INFO - evaluating now!
2024-01-02 08:50:29,313 - INFO - {'mode': 'Eval', 'epoch': 6, 'iter': 0, 'lr': 0.00018090169943749476, 'loss': 29.324186325073242}
2024-01-02 08:50:42,039 - INFO - Eval: expid = 816594, Epoch = 6, avg_loss = 48.656313059536835.
2024-01-02 08:50:42,039 - INFO - Epoch [6/30] (9198)  train_loss: 47.6675, val_loss: 48.6563, lr: 0.000174, 143.90s
2024-01-02 08:50:42,139 - INFO - {'mode': 'Train', 'epoch': 7, 'iter': 0, 'lr': 0.00017431448254773944, 'loss': 37.00201416015625}
2024-01-02 08:51:32,049 - INFO - {'mode': 'Train', 'epoch': 7, 'iter': 500, 'lr': 0.00017431448254773944, 'loss': 34.05768966674805}
2024-01-02 08:52:22,109 - INFO - {'mode': 'Train', 'epoch': 7, 'iter': 1000, 'lr': 0.00017431448254773944, 'loss': 33.866607666015625}
2024-01-02 08:52:53,461 - INFO - Train: expid = 816594, Epoch = 7, avg_loss = 46.51140075324265.
2024-01-02 08:52:53,461 - INFO - epoch complete!
2024-01-02 08:52:53,461 - INFO - evaluating now!
2024-01-02 08:52:53,491 - INFO - {'mode': 'Eval', 'epoch': 7, 'iter': 0, 'lr': 0.00017431448254773944, 'loss': 31.65582275390625}
2024-01-02 08:53:06,217 - INFO - Eval: expid = 816594, Epoch = 7, avg_loss = 49.90122786177892.
2024-01-02 08:53:06,217 - INFO - Epoch [7/30] (10512)  train_loss: 46.5114, val_loss: 49.9012, lr: 0.000167, 144.18s
2024-01-02 08:53:06,317 - INFO - {'mode': 'Train', 'epoch': 8, 'iter': 0, 'lr': 0.00016691306063588583, 'loss': 39.02519226074219}
2024-01-02 08:53:56,577 - INFO - {'mode': 'Train', 'epoch': 8, 'iter': 500, 'lr': 0.00016691306063588583, 'loss': 22.01293182373047}
2024-01-02 08:54:46,872 - INFO - {'mode': 'Train', 'epoch': 8, 'iter': 1000, 'lr': 0.00016691306063588583, 'loss': 15.95199966430664}
2024-01-02 08:55:18,282 - INFO - Train: expid = 816594, Epoch = 8, avg_loss = 45.6135829468209.
2024-01-02 08:55:18,282 - INFO - epoch complete!
2024-01-02 08:55:18,282 - INFO - evaluating now!
2024-01-02 08:55:18,312 - INFO - {'mode': 'Eval', 'epoch': 8, 'iter': 0, 'lr': 0.00016691306063588583, 'loss': 70.82608795166016}
2024-01-02 08:55:31,072 - INFO - Eval: expid = 816594, Epoch = 8, avg_loss = 46.27720247773819.
2024-01-02 08:55:31,072 - INFO - Epoch [8/30] (11826)  train_loss: 45.6136, val_loss: 46.2772, lr: 0.000159, 144.85s
2024-01-02 08:55:31,252 - INFO - Saved model at 8
2024-01-02 08:55:31,252 - INFO - Val loss decrease from 48.3508 to 46.2772, saving to ./libcity/cache/816594/model_cache/LinearETA_bj_epoch8.tar
2024-01-02 08:55:31,362 - INFO - {'mode': 'Train', 'epoch': 9, 'iter': 0, 'lr': 0.00015877852522924732, 'loss': 48.875667572021484}
2024-01-02 08:56:21,432 - INFO - {'mode': 'Train', 'epoch': 9, 'iter': 500, 'lr': 0.00015877852522924732, 'loss': 59.6424560546875}
2024-01-02 08:57:11,578 - INFO - {'mode': 'Train', 'epoch': 9, 'iter': 1000, 'lr': 0.00015877852522924732, 'loss': 60.21170425415039}
2024-01-02 08:57:42,928 - INFO - Train: expid = 816594, Epoch = 9, avg_loss = 44.43723271185733.
2024-01-02 08:57:42,928 - INFO - epoch complete!
2024-01-02 08:57:42,928 - INFO - evaluating now!
2024-01-02 08:57:42,958 - INFO - {'mode': 'Eval', 'epoch': 9, 'iter': 0, 'lr': 0.00015877852522924732, 'loss': 61.63398361206055}
2024-01-02 08:57:55,658 - INFO - Eval: expid = 816594, Epoch = 9, avg_loss = 46.19657582452852.
2024-01-02 08:57:55,658 - INFO - Epoch [9/30] (13140)  train_loss: 44.4372, val_loss: 46.1966, lr: 0.000150, 144.40s
2024-01-02 08:57:55,838 - INFO - Saved model at 9
2024-01-02 08:57:55,838 - INFO - Val loss decrease from 46.2772 to 46.1966, saving to ./libcity/cache/816594/model_cache/LinearETA_bj_epoch9.tar
2024-01-02 08:57:55,948 - INFO - {'mode': 'Train', 'epoch': 10, 'iter': 0, 'lr': 0.00015000000000000001, 'loss': 33.3011474609375}
2024-01-02 08:58:46,005 - INFO - {'mode': 'Train', 'epoch': 10, 'iter': 500, 'lr': 0.00015000000000000001, 'loss': 46.57741165161133}
2024-01-02 08:59:35,985 - INFO - {'mode': 'Train', 'epoch': 10, 'iter': 1000, 'lr': 0.00015000000000000001, 'loss': 10.378618240356445}
2024-01-02 09:00:07,335 - INFO - Train: expid = 816594, Epoch = 10, avg_loss = 44.120469254657955.
2024-01-02 09:00:07,335 - INFO - epoch complete!
2024-01-02 09:00:07,335 - INFO - evaluating now!
2024-01-02 09:00:07,365 - INFO - {'mode': 'Eval', 'epoch': 10, 'iter': 0, 'lr': 0.00015000000000000001, 'loss': 59.246734619140625}
2024-01-02 09:00:20,155 - INFO - Eval: expid = 816594, Epoch = 10, avg_loss = 47.69295735010818.
2024-01-02 09:00:20,155 - INFO - Epoch [10/30] (14454)  train_loss: 44.1205, val_loss: 47.6930, lr: 0.000141, 144.32s
2024-01-02 09:00:20,265 - INFO - {'mode': 'Train', 'epoch': 11, 'iter': 0, 'lr': 0.00014067366430758004, 'loss': 30.08251190185547}
2024-01-02 09:01:10,271 - INFO - {'mode': 'Train', 'epoch': 11, 'iter': 500, 'lr': 0.00014067366430758004, 'loss': 65.99281311035156}
2024-01-02 09:02:00,331 - INFO - {'mode': 'Train', 'epoch': 11, 'iter': 1000, 'lr': 0.00014067366430758004, 'loss': 105.56112670898438}
2024-01-02 09:02:31,718 - INFO - Train: expid = 816594, Epoch = 11, avg_loss = 43.873065457947476.
2024-01-02 09:02:31,718 - INFO - epoch complete!
2024-01-02 09:02:31,718 - INFO - evaluating now!
2024-01-02 09:02:31,748 - INFO - {'mode': 'Eval', 'epoch': 11, 'iter': 0, 'lr': 0.00014067366430758004, 'loss': 18.971763610839844}
2024-01-02 09:02:44,448 - INFO - Eval: expid = 816594, Epoch = 11, avg_loss = 47.27841441598657.
2024-01-02 09:02:44,448 - INFO - Epoch [11/30] (15768)  train_loss: 43.8731, val_loss: 47.2784, lr: 0.000131, 144.29s
2024-01-02 09:02:44,558 - INFO - {'mode': 'Train', 'epoch': 12, 'iter': 0, 'lr': 0.00013090169943749476, 'loss': 20.879169464111328}
2024-01-02 09:03:34,700 - INFO - {'mode': 'Train', 'epoch': 12, 'iter': 500, 'lr': 0.00013090169943749476, 'loss': 25.43846893310547}
2024-01-02 09:04:24,850 - INFO - {'mode': 'Train', 'epoch': 12, 'iter': 1000, 'lr': 0.00013090169943749476, 'loss': 23.60265350341797}
2024-01-02 09:04:56,338 - INFO - Train: expid = 816594, Epoch = 12, avg_loss = 42.72421760885745.
2024-01-02 09:04:56,338 - INFO - epoch complete!
2024-01-02 09:04:56,338 - INFO - evaluating now!
2024-01-02 09:04:56,368 - INFO - {'mode': 'Eval', 'epoch': 12, 'iter': 0, 'lr': 0.00013090169943749476, 'loss': 52.029823303222656}
2024-01-02 09:05:09,068 - INFO - Eval: expid = 816594, Epoch = 12, avg_loss = 47.32792296257193.
2024-01-02 09:05:09,068 - INFO - Epoch [12/30] (17082)  train_loss: 42.7242, val_loss: 47.3279, lr: 0.000121, 144.62s
2024-01-02 09:05:09,168 - INFO - {'mode': 'Train', 'epoch': 13, 'iter': 0, 'lr': 0.00012079116908177593, 'loss': 23.904525756835938}
2024-01-02 09:05:59,258 - INFO - {'mode': 'Train', 'epoch': 13, 'iter': 500, 'lr': 0.00012079116908177593, 'loss': 18.60039710998535}
2024-01-02 09:06:49,335 - INFO - {'mode': 'Train', 'epoch': 13, 'iter': 1000, 'lr': 0.00012079116908177593, 'loss': 81.55146789550781}
2024-01-02 09:07:20,621 - INFO - Train: expid = 816594, Epoch = 13, avg_loss = 42.14718948803438.
2024-01-02 09:07:20,621 - INFO - epoch complete!
2024-01-02 09:07:20,621 - INFO - evaluating now!
2024-01-02 09:07:20,651 - INFO - {'mode': 'Eval', 'epoch': 13, 'iter': 0, 'lr': 0.00012079116908177593, 'loss': 22.161182403564453}
2024-01-02 09:07:33,371 - INFO - Eval: expid = 816594, Epoch = 13, avg_loss = 45.39336799486587.
2024-01-02 09:07:33,371 - INFO - Epoch [13/30] (18396)  train_loss: 42.1472, val_loss: 45.3934, lr: 0.000110, 144.30s
2024-01-02 09:07:33,561 - INFO - Saved model at 13
2024-01-02 09:07:33,561 - INFO - Val loss decrease from 46.1966 to 45.3934, saving to ./libcity/cache/816594/model_cache/LinearETA_bj_epoch13.tar
2024-01-02 09:07:33,661 - INFO - {'mode': 'Train', 'epoch': 14, 'iter': 0, 'lr': 0.00011045284632676536, 'loss': 14.892972946166992}
2024-01-02 09:08:23,671 - INFO - {'mode': 'Train', 'epoch': 14, 'iter': 500, 'lr': 0.00011045284632676536, 'loss': 18.710098266601562}
2024-01-02 09:09:13,937 - INFO - {'mode': 'Train', 'epoch': 14, 'iter': 1000, 'lr': 0.00011045284632676536, 'loss': 28.643291473388672}
2024-01-02 09:09:45,227 - INFO - Train: expid = 816594, Epoch = 14, avg_loss = 41.29193088284682.
2024-01-02 09:09:45,227 - INFO - epoch complete!
2024-01-02 09:09:45,227 - INFO - evaluating now!
2024-01-02 09:09:45,257 - INFO - {'mode': 'Eval', 'epoch': 14, 'iter': 0, 'lr': 0.00011045284632676536, 'loss': 175.8548583984375}
2024-01-02 09:09:57,997 - INFO - Eval: expid = 816594, Epoch = 14, avg_loss = 46.96349880684456.
2024-01-02 09:09:57,997 - INFO - Epoch [14/30] (19710)  train_loss: 41.2919, val_loss: 46.9635, lr: 0.000100, 144.44s
2024-01-02 09:09:58,107 - INFO - {'mode': 'Train', 'epoch': 15, 'iter': 0, 'lr': 0.00010000000000000003, 'loss': 35.86661911010742}
2024-01-02 09:10:48,254 - INFO - {'mode': 'Train', 'epoch': 15, 'iter': 500, 'lr': 0.00010000000000000003, 'loss': 57.41992950439453}
2024-01-02 09:11:38,364 - INFO - {'mode': 'Train', 'epoch': 15, 'iter': 1000, 'lr': 0.00010000000000000003, 'loss': 42.28922653198242}
2024-01-02 09:12:09,724 - INFO - Train: expid = 816594, Epoch = 15, avg_loss = 40.73216231379704.
2024-01-02 09:12:09,724 - INFO - epoch complete!
2024-01-02 09:12:09,724 - INFO - evaluating now!
2024-01-02 09:12:09,754 - INFO - {'mode': 'Eval', 'epoch': 15, 'iter': 0, 'lr': 0.00010000000000000003, 'loss': 76.10816955566406}
2024-01-02 09:12:22,574 - INFO - Eval: expid = 816594, Epoch = 15, avg_loss = 45.756159590259536.
2024-01-02 09:12:22,574 - INFO - Epoch [15/30] (21024)  train_loss: 40.7322, val_loss: 45.7562, lr: 0.000090, 144.58s
2024-01-02 09:12:22,684 - INFO - {'mode': 'Train', 'epoch': 16, 'iter': 0, 'lr': 8.954715367323468e-05, 'loss': 7.710512161254883}
2024-01-02 09:13:12,847 - INFO - {'mode': 'Train', 'epoch': 16, 'iter': 500, 'lr': 8.954715367323468e-05, 'loss': 18.886396408081055}
2024-01-02 09:14:03,107 - INFO - {'mode': 'Train', 'epoch': 16, 'iter': 1000, 'lr': 8.954715367323468e-05, 'loss': 10.27308464050293}
2024-01-02 09:14:34,474 - INFO - Train: expid = 816594, Epoch = 16, avg_loss = 39.600796361609255.
2024-01-02 09:14:34,474 - INFO - epoch complete!
2024-01-02 09:14:34,474 - INFO - evaluating now!
2024-01-02 09:14:34,504 - INFO - {'mode': 'Eval', 'epoch': 16, 'iter': 0, 'lr': 8.954715367323468e-05, 'loss': 35.8331413269043}
2024-01-02 09:14:47,274 - INFO - Eval: expid = 816594, Epoch = 16, avg_loss = 45.014902413163554.
2024-01-02 09:14:47,274 - INFO - Epoch [16/30] (22338)  train_loss: 39.6008, val_loss: 45.0149, lr: 0.000079, 144.70s
2024-01-02 09:14:47,464 - INFO - Saved model at 16
2024-01-02 09:14:47,464 - INFO - Val loss decrease from 45.3934 to 45.0149, saving to ./libcity/cache/816594/model_cache/LinearETA_bj_epoch16.tar
2024-01-02 09:14:47,564 - INFO - {'mode': 'Train', 'epoch': 17, 'iter': 0, 'lr': 7.920883091822407e-05, 'loss': 25.810083389282227}
2024-01-02 09:15:37,754 - INFO - {'mode': 'Train', 'epoch': 17, 'iter': 500, 'lr': 7.920883091822407e-05, 'loss': 5.294334411621094}
2024-01-02 09:16:27,794 - INFO - {'mode': 'Train', 'epoch': 17, 'iter': 1000, 'lr': 7.920883091822407e-05, 'loss': 10.923797607421875}
2024-01-02 09:16:59,311 - INFO - Train: expid = 816594, Epoch = 17, avg_loss = 39.102934655861894.
2024-01-02 09:16:59,311 - INFO - epoch complete!
2024-01-02 09:16:59,311 - INFO - evaluating now!
2024-01-02 09:16:59,341 - INFO - {'mode': 'Eval', 'epoch': 17, 'iter': 0, 'lr': 7.920883091822407e-05, 'loss': 18.081588745117188}
2024-01-02 09:17:12,161 - INFO - Eval: expid = 816594, Epoch = 17, avg_loss = 45.355005781944485.
2024-01-02 09:17:12,161 - INFO - Epoch [17/30] (23652)  train_loss: 39.1029, val_loss: 45.3550, lr: 0.000069, 144.70s
2024-01-02 09:17:12,261 - INFO - {'mode': 'Train', 'epoch': 18, 'iter': 0, 'lr': 6.909830056250527e-05, 'loss': 20.648345947265625}
2024-01-02 09:18:02,371 - INFO - {'mode': 'Train', 'epoch': 18, 'iter': 500, 'lr': 6.909830056250527e-05, 'loss': 17.16402244567871}
2024-01-02 09:18:52,447 - INFO - {'mode': 'Train', 'epoch': 18, 'iter': 1000, 'lr': 6.909830056250527e-05, 'loss': 11.364843368530273}
2024-01-02 09:19:23,897 - INFO - Train: expid = 816594, Epoch = 18, avg_loss = 38.06074837612947.
2024-01-02 09:19:23,907 - INFO - epoch complete!
2024-01-02 09:19:23,907 - INFO - evaluating now!
2024-01-02 09:19:23,937 - INFO - {'mode': 'Eval', 'epoch': 18, 'iter': 0, 'lr': 6.909830056250527e-05, 'loss': 95.62355041503906}
2024-01-02 09:19:36,687 - INFO - Eval: expid = 816594, Epoch = 18, avg_loss = 46.83302094402923.
2024-01-02 09:19:36,697 - INFO - Epoch [18/30] (24966)  train_loss: 38.0607, val_loss: 46.8330, lr: 0.000059, 144.54s
2024-01-02 09:19:36,797 - INFO - {'mode': 'Train', 'epoch': 19, 'iter': 0, 'lr': 5.932633569241999e-05, 'loss': 23.4545955657959}
2024-01-02 09:20:26,847 - INFO - {'mode': 'Train', 'epoch': 19, 'iter': 500, 'lr': 5.932633569241999e-05, 'loss': 79.41798400878906}
2024-01-02 09:21:16,894 - INFO - {'mode': 'Train', 'epoch': 19, 'iter': 1000, 'lr': 5.932633569241999e-05, 'loss': 16.562829971313477}
2024-01-02 09:21:48,428 - INFO - Train: expid = 816594, Epoch = 19, avg_loss = 37.372228464005.
2024-01-02 09:21:48,428 - INFO - epoch complete!
2024-01-02 09:21:48,428 - INFO - evaluating now!
2024-01-02 09:21:48,458 - INFO - {'mode': 'Eval', 'epoch': 19, 'iter': 0, 'lr': 5.932633569241999e-05, 'loss': 54.67178726196289}
2024-01-02 09:22:01,178 - INFO - Eval: expid = 816594, Epoch = 19, avg_loss = 46.932848744196434.
2024-01-02 09:22:01,178 - INFO - Epoch [19/30] (26280)  train_loss: 37.3722, val_loss: 46.9328, lr: 0.000050, 144.48s
2024-01-02 09:22:01,278 - INFO - {'mode': 'Train', 'epoch': 20, 'iter': 0, 'lr': 5.000000000000002e-05, 'loss': 12.77503490447998}
2024-01-02 09:22:51,624 - INFO - {'mode': 'Train', 'epoch': 20, 'iter': 500, 'lr': 5.000000000000002e-05, 'loss': 32.701698303222656}
2024-01-02 09:23:41,784 - INFO - {'mode': 'Train', 'epoch': 20, 'iter': 1000, 'lr': 5.000000000000002e-05, 'loss': 25.984012603759766}
2024-01-02 09:24:13,284 - INFO - Train: expid = 816594, Epoch = 20, avg_loss = 36.112466325406004.
2024-01-02 09:24:13,284 - INFO - epoch complete!
2024-01-02 09:24:13,284 - INFO - evaluating now!
2024-01-02 09:24:13,314 - INFO - {'mode': 'Eval', 'epoch': 20, 'iter': 0, 'lr': 5.000000000000002e-05, 'loss': 20.840269088745117}
2024-01-02 09:24:26,064 - INFO - Eval: expid = 816594, Epoch = 20, avg_loss = 46.00275067515569.
2024-01-02 09:24:26,064 - INFO - Epoch [20/30] (27594)  train_loss: 36.1125, val_loss: 46.0028, lr: 0.000041, 144.89s
2024-01-02 09:24:26,174 - INFO - {'mode': 'Train', 'epoch': 21, 'iter': 0, 'lr': 4.12214747707527e-05, 'loss': 21.906801223754883}
2024-01-02 09:25:16,351 - INFO - {'mode': 'Train', 'epoch': 21, 'iter': 500, 'lr': 4.12214747707527e-05, 'loss': 14.411500930786133}
2024-01-02 09:26:06,461 - INFO - {'mode': 'Train', 'epoch': 21, 'iter': 1000, 'lr': 4.12214747707527e-05, 'loss': 25.918516159057617}
2024-01-02 09:26:37,798 - INFO - Train: expid = 816594, Epoch = 21, avg_loss = 35.10835895719809.
2024-01-02 09:26:37,798 - INFO - epoch complete!
2024-01-02 09:26:37,808 - INFO - evaluating now!
2024-01-02 09:26:37,838 - INFO - {'mode': 'Eval', 'epoch': 21, 'iter': 0, 'lr': 4.12214747707527e-05, 'loss': 38.046749114990234}
2024-01-02 09:26:50,648 - INFO - Eval: expid = 816594, Epoch = 21, avg_loss = 47.37462055955303.
2024-01-02 09:26:50,648 - INFO - Epoch [21/30] (28908)  train_loss: 35.1084, val_loss: 47.3746, lr: 0.000033, 144.58s
2024-01-02 09:26:50,758 - INFO - {'mode': 'Train', 'epoch': 22, 'iter': 0, 'lr': 3.308693936411421e-05, 'loss': 58.04945373535156}
2024-01-02 09:27:40,898 - INFO - {'mode': 'Train', 'epoch': 22, 'iter': 500, 'lr': 3.308693936411421e-05, 'loss': 16.84960174560547}
2024-01-02 09:28:30,898 - INFO - {'mode': 'Train', 'epoch': 22, 'iter': 1000, 'lr': 3.308693936411421e-05, 'loss': 11.928007125854492}
2024-01-02 09:29:02,254 - INFO - Train: expid = 816594, Epoch = 22, avg_loss = 34.2893852593216.
2024-01-02 09:29:02,254 - INFO - epoch complete!
2024-01-02 09:29:02,254 - INFO - evaluating now!
2024-01-02 09:29:02,284 - INFO - {'mode': 'Eval', 'epoch': 22, 'iter': 0, 'lr': 3.308693936411421e-05, 'loss': 48.510643005371094}
2024-01-02 09:29:15,044 - INFO - Eval: expid = 816594, Epoch = 22, avg_loss = 45.974437735396435.
2024-01-02 09:29:15,044 - INFO - Epoch [22/30] (30222)  train_loss: 34.2894, val_loss: 45.9744, lr: 0.000026, 144.40s
2024-01-02 09:29:15,154 - INFO - {'mode': 'Train', 'epoch': 23, 'iter': 0, 'lr': 2.5685517452260587e-05, 'loss': 14.456371307373047}
2024-01-02 09:30:05,244 - INFO - {'mode': 'Train', 'epoch': 23, 'iter': 500, 'lr': 2.5685517452260587e-05, 'loss': 67.64983367919922}
2024-01-02 09:30:55,431 - INFO - {'mode': 'Train', 'epoch': 23, 'iter': 1000, 'lr': 2.5685517452260587e-05, 'loss': 17.85283851623535}
2024-01-02 09:31:26,821 - INFO - Train: expid = 816594, Epoch = 23, avg_loss = 34.01186620220471.
2024-01-02 09:31:26,831 - INFO - epoch complete!
2024-01-02 09:31:26,831 - INFO - evaluating now!
2024-01-02 09:31:26,861 - INFO - {'mode': 'Eval', 'epoch': 23, 'iter': 0, 'lr': 2.5685517452260587e-05, 'loss': 21.133956909179688}
2024-01-02 09:31:39,631 - INFO - Eval: expid = 816594, Epoch = 23, avg_loss = 47.18627144158159.
2024-01-02 09:31:39,631 - INFO - Epoch [23/30] (31536)  train_loss: 34.0119, val_loss: 47.1863, lr: 0.000019, 144.59s
2024-01-02 09:31:39,741 - INFO - {'mode': 'Train', 'epoch': 24, 'iter': 0, 'lr': 1.9098300562505266e-05, 'loss': 28.865570068359375}
2024-01-02 09:32:29,891 - INFO - {'mode': 'Train', 'epoch': 24, 'iter': 500, 'lr': 1.9098300562505266e-05, 'loss': 61.23952865600586}
2024-01-02 09:33:19,947 - INFO - {'mode': 'Train', 'epoch': 24, 'iter': 1000, 'lr': 1.9098300562505266e-05, 'loss': 65.18443298339844}
2024-01-02 09:33:51,317 - INFO - Train: expid = 816594, Epoch = 24, avg_loss = 33.0303616110877.
2024-01-02 09:33:51,317 - INFO - epoch complete!
2024-01-02 09:33:51,317 - INFO - evaluating now!
2024-01-02 09:33:51,347 - INFO - {'mode': 'Eval', 'epoch': 24, 'iter': 0, 'lr': 1.9098300562505266e-05, 'loss': 56.74605178833008}
2024-01-02 09:34:04,127 - INFO - Eval: expid = 816594, Epoch = 24, avg_loss = 45.506046071988806.
2024-01-02 09:34:04,137 - INFO - Epoch [24/30] (32850)  train_loss: 33.0304, val_loss: 45.5060, lr: 0.000013, 144.51s
2024-01-02 09:34:04,237 - INFO - {'mode': 'Train', 'epoch': 25, 'iter': 0, 'lr': 1.339745962155613e-05, 'loss': 19.159879684448242}
2024-01-02 09:34:54,450 - INFO - {'mode': 'Train', 'epoch': 25, 'iter': 500, 'lr': 1.339745962155613e-05, 'loss': 10.511554718017578}
2024-01-02 09:35:44,480 - INFO - {'mode': 'Train', 'epoch': 25, 'iter': 1000, 'lr': 1.339745962155613e-05, 'loss': 82.40591430664062}
2024-01-02 09:36:15,900 - INFO - Train: expid = 816594, Epoch = 25, avg_loss = 33.020354932971955.
2024-01-02 09:36:15,900 - INFO - epoch complete!
2024-01-02 09:36:15,900 - INFO - evaluating now!
2024-01-02 09:36:15,930 - INFO - {'mode': 'Eval', 'epoch': 25, 'iter': 0, 'lr': 1.339745962155613e-05, 'loss': 38.85248947143555}
2024-01-02 09:36:28,720 - INFO - Eval: expid = 816594, Epoch = 25, avg_loss = 46.882360612965066.
2024-01-02 09:36:28,720 - INFO - Epoch [25/30] (34164)  train_loss: 33.0204, val_loss: 46.8824, lr: 0.000009, 144.58s
2024-01-02 09:36:28,820 - INFO - {'mode': 'Train', 'epoch': 26, 'iter': 0, 'lr': 8.645454235739903e-06, 'loss': 21.23167610168457}
2024-01-02 09:37:18,957 - INFO - {'mode': 'Train', 'epoch': 26, 'iter': 500, 'lr': 8.645454235739903e-06, 'loss': 22.20968246459961}
2024-01-02 09:38:09,057 - INFO - {'mode': 'Train', 'epoch': 26, 'iter': 1000, 'lr': 8.645454235739903e-06, 'loss': 14.812856674194336}
2024-01-02 09:38:40,374 - INFO - Train: expid = 816594, Epoch = 26, avg_loss = 31.95193116417621.
2024-01-02 09:38:40,374 - INFO - epoch complete!
2024-01-02 09:38:40,374 - INFO - evaluating now!
2024-01-02 09:38:40,404 - INFO - {'mode': 'Eval', 'epoch': 26, 'iter': 0, 'lr': 8.645454235739903e-06, 'loss': 14.870115280151367}
2024-01-02 09:38:53,204 - INFO - Eval: expid = 816594, Epoch = 26, avg_loss = 46.354884858545105.
2024-01-02 09:38:53,204 - INFO - Epoch [26/30] (35478)  train_loss: 31.9519, val_loss: 46.3549, lr: 0.000005, 144.48s
2024-01-02 09:38:53,204 - WARNING - Early stopping at epoch: 26
2024-01-02 09:38:53,204 - INFO - Trained totally 27 epochs, average train time is 131.241s, average eval time is 12.727s
2024-01-02 09:38:53,304 - INFO - Loaded model at 16
2024-01-02 09:38:53,384 - INFO - Save png at ./libcity/cache/816594/816594_loss.png
2024-01-02 09:38:53,444 - INFO - Save png at ./libcity/cache/816594/816594_lr.png
2024-01-02 09:38:53,584 - INFO - Saved model at ./libcity/cache/816594/model_cache/816594_LinearETA_bj.pt
2024-01-02 09:38:53,584 - INFO - Start evaluating ...
2024-01-02 09:38:53,624 - INFO - {'mode': 'Test', 'epoch': 0, 'iter': 0, 'lr': 7.920883091822407e-05, 'loss': 34.476707458496094}
2024-01-02 09:39:07,484 - INFO - Test: expid = 816594, Epoch = 0, avg_loss = 45.22853396798922.
2024-01-02 09:39:07,484 - INFO - Evaluate result is {"MAE": 4.779141377912809, "RMSE": 6.225992464583759, "MAPE": 0.39698263077431073, "R2": 0.33146325848957825, "EVAR": 0.4520219114000939}
2024-01-02 09:39:07,484 - INFO - Evaluate result is saved at ./libcity/cache/816594/evaluate_cache\816594_2024_01_02_09_39_07_LinearETA_bj.json
2024-01-02 09:39:07,484 - INFO - 
{
 "MAE": 4.779141377912809,
 "RMSE": 6.225992464583759,
 "MAPE": 0.39698263077431073,
 "R2": 0.33146325848957825,
 "EVAR": 0.4520219114000939
}
2024-01-02 09:39:07,494 - INFO - Evaluate result is saved at ./libcity/cache/816594/evaluate_cache\816594_2024_01_02_09_39_07_LinearETA_bj.csv
2024-01-02 09:39:07,494 - INFO - 
        MAE      RMSE      MAPE        R2      EVAR
1  4.779141  6.225992  0.396983  0.331463  0.452022
2024-01-02 09:39:07,494 - INFO - Test time 13.90999436378479s.
